#!/usr/bin/env python3
"""
H2Q-Evo æ ¸å¿ƒæœºèƒ½åŠ›é›†æˆä»£ç è¡¥å…¨ç³»ç»Ÿ
å°†å››å…ƒæ•°çƒé¢æ˜ å°„å’Œåˆ†å±‚æ¦‚å¿µç¼–ç é›†æˆåˆ°ä»£ç ç”Ÿæˆä¸­
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import json
import os
import sys
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
from pathlib import Path

sys.path.append('/Users/imymm/H2Q-Evo')

from h2q_project.src.h2q.tokenizer_simple import default_tokenizer
from hierarchical_concept_encoder import HierarchicalConceptEncoder
from final_integration_system import FinalIntegratedSystem, FinalIntegrationConfig
from h2q_project.h2q.core.binary_knot_codec import BinaryKnotReEncoder, binary_knot_enabled


class CodeDataset(Dataset):
    """ä»£ç æ•°æ®é›†"""

    def __init__(self, code_samples: List[str], tokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = []

        for code in code_samples:
            tokens = tokenizer.encode(code, add_specials=True, max_length=max_length)
            if len(tokens) >= 10:  # åªä¿ç•™æœ‰æ„ä¹‰çš„ä»£ç ç‰‡æ®µ
                self.samples.append(tokens)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        tokens = self.samples[idx]
        # åˆ›å»ºè¾“å…¥å’Œç›®æ ‡åºåˆ—
        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)
        target_ids = torch.tensor(tokens[1:], dtype=torch.long)
        return input_ids, target_ids


class CoreMachineCodeTransformer(nn.Module):
    """é›†æˆæ ¸å¿ƒæœºèƒ½åŠ›çš„ä»£ç ç”ŸæˆTransformer"""

    def __init__(self, vocab_size: int, hidden_dim: int = 512, num_layers: int = 6,
                 num_heads: int = 8, dropout: float = 0.1, concept_dim: int = 256):
        super().__init__()

        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.concept_dim = concept_dim

        # æ ‡å‡†åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.pos_embedding = nn.Embedding(1024, hidden_dim)

        # äºŒè¿›åˆ¶çº½ç»“å†ç¼–ç å™¨ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡å¯ç”¨ï¼‰
        self.use_binary_knot = binary_knot_enabled()
        self.binary_knot = BinaryKnotReEncoder(vocab_size=vocab_size, bit_width=16, knot_dim=128, hidden_dim=hidden_dim)

        # æ ¸å¿ƒæœºæ¦‚å¿µç¼–ç å™¨
        self.concept_encoder = HierarchicalConceptEncoder(max_depth=3, compression_ratio=46.0)

        # æ¦‚å¿µèåˆå±‚
        self.concept_fusion = nn.Linear(hidden_dim + concept_dim, hidden_dim)
        self.layer_norm_fusion = nn.LayerNorm(hidden_dim)

        # Transformerå±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            batch_first=True,
            activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Layer normalization
        self.layer_norm = nn.LayerNorm(hidden_dim)

        # è¾“å‡ºå±‚
        self.lm_head = nn.Linear(hidden_dim, vocab_size)

        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            std = 0.02 if module.weight.shape[0] != self.vocab_size else 0.02 / (self.hidden_dim ** 0.5)
            torch.nn.init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.ones_(module.weight)
            torch.nn.init.zeros_(module.bias)

    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ - é›†æˆæ ¸å¿ƒæœºæ¦‚å¿µç¼–ç """
        seq_len = input_ids.size(1)

        # æ ‡å‡†ä½ç½®ç¼–ç 
        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)
        pos_emb = self.pos_embedding(positions)

        # è¯åµŒå…¥
        token_emb = self.embedding(input_ids)

        # äºŒè¿›åˆ¶çº½ç»“ç¼–ç å¢å¼ºï¼ˆè‡ªç„¶ç¼–ç æµï¼‰
        if self.use_binary_knot:
            binary_emb = self.binary_knot(input_ids)
            token_emb = token_emb + binary_emb

        # ç»„åˆåŸºç¡€åµŒå…¥
        x = token_emb + pos_emb

        # æ ¸å¿ƒæœºæ¦‚å¿µç¼–ç å¢å¼º
        concept_features = self._extract_concept_features(input_ids)
        if concept_features is not None:
            # æ‰©å±•æ¦‚å¿µç‰¹å¾åˆ°åºåˆ—é•¿åº¦
            concept_expanded = concept_features.unsqueeze(1).expand(-1, seq_len, -1)

            # èåˆæ¦‚å¿µç‰¹å¾å’ŒtokenåµŒå…¥
            combined = torch.cat([x, concept_expanded], dim=-1)
            x = self.concept_fusion(combined)
            x = self.layer_norm_fusion(x)

        # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)
        causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=input_ids.device), diagonal=1)

        # Transformerå‰å‘ä¼ æ’­
        output = self.transformer(x, mask=causal_mask, src_key_padding_mask=~attention_mask)

        # Layer normalization
        output = self.layer_norm(output)

        # è¯­è¨€æ¨¡å‹å¤´
        logits = self.lm_head(output)

        return logits

    def _extract_concept_features(self, input_ids: torch.Tensor) -> Optional[torch.Tensor]:
        """ä»è¾“å…¥åºåˆ—ä¸­æå–æ ¸å¿ƒæœºæ¦‚å¿µç‰¹å¾"""
        try:
            batch_size = input_ids.size(0)

            # è§£ç è¾“å…¥åºåˆ—ä¸ºæ–‡æœ¬
            decoded_texts = []
            for i in range(batch_size):
                tokens = input_ids[i].tolist()
                # ç§»é™¤padding tokens
                tokens = [t for t in tokens if t != self.vocab_size - 1]  # å‡è®¾pad_idæ˜¯vocab_size-1
                text = default_tokenizer.decode(tokens, skip_specials=True)
                decoded_texts.append(text)

            # ä½¿ç”¨æ ¸å¿ƒæœºç¼–ç å™¨æå–æ¦‚å¿µç‰¹å¾
            concept_features = []
            for text in decoded_texts:
                if text.strip():  # åªå¤„ç†éç©ºæ–‡æœ¬
                    encoded = self.concept_encoder.encode_hierarchical(text)
                    # æå–æœ€ç»ˆçš„å‹ç¼©è¡¨ç¤ºä½œä¸ºæ¦‚å¿µç‰¹å¾
                    if isinstance(encoded, dict) and 'final_compressed' in encoded:
                        concept_feat = encoded['final_compressed'].mean(dim=1)  # å¹³å‡æ± åŒ–
                    else:
                        # å¦‚æœç¼–ç å¤±è´¥ï¼Œä½¿ç”¨é›¶å‘é‡
                        concept_feat = torch.zeros(self.concept_dim, device=input_ids.device)
                else:
                    concept_feat = torch.zeros(self.concept_dim, device=input_ids.device)

                concept_features.append(concept_feat)

            if concept_features:
                return torch.stack(concept_features, dim=0)
            else:
                return None

        except Exception as e:
            # å¦‚æœæ¦‚å¿µç¼–ç å¤±è´¥ï¼Œè¿”å›Noneï¼Œä½¿ç”¨æ ‡å‡†Transformer
            print(f"æ¦‚å¿µç¼–ç å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼: {e}")
            return None


class CoreMachineCodeCompletionSystem:
    """é›†æˆæ ¸å¿ƒæœºèƒ½åŠ›çš„ä»£ç è¡¥å…¨ç³»ç»Ÿ"""

    def __init__(self, model_path: Optional[str] = None):
        self.tokenizer = default_tokenizer
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆ›å»ºé›†æˆæ ¸å¿ƒæœºèƒ½åŠ›çš„æ¨¡å‹
        self.model = CoreMachineCodeTransformer(
            vocab_size=self.tokenizer.vocab_size,
            hidden_dim=512,
            num_layers=6,
            num_heads=8
        ).to(self.device)

        # åŠ è½½æˆ–è®­ç»ƒæ¨¡å‹
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        else:
            self.train_with_core_machine()

    def train_with_core_machine(self):
        """ä½¿ç”¨æ ¸å¿ƒæœºèƒ½åŠ›å¢å¼ºçš„è®­ç»ƒ"""
        print("ğŸš€ ä½¿ç”¨æ ¸å¿ƒæœºèƒ½åŠ›è®­ç»ƒä»£ç ç”Ÿæˆæ¨¡å‹...")
        print("   é›†æˆå››å…ƒæ•°çƒé¢æ˜ å°„å’Œåˆ†å±‚æ¦‚å¿µç¼–ç ")

        # åˆ›å»ºè®­ç»ƒæ•°æ®
        code_samples = self._create_training_samples()

        # åˆ›å»ºæ•°æ®é›†
        dataset = CodeDataset(code_samples, self.tokenizer)
        dataloader = DataLoader(dataset, batch_size=4, shuffle=True)  # å‡å°batch sizeä»¥é€‚åº”æ¦‚å¿µç¼–ç 

        # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-3, weight_decay=0.01, betas=(0.9, 0.999))
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)
        criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_id)

        # è®­ç»ƒå¾ªç¯
        num_epochs = 30  # å‡å°‘è®­ç»ƒè½®æ•°ï¼Œå› ä¸ºæ¨¡å‹æ›´å¤æ‚
        best_loss = float('inf')
        patience = 5
        patience_counter = 0

        print(f"ğŸ“š è®­ç»ƒæ•°æ®å¤§å°: {len(dataset)}")
        print(f"ğŸƒ å¼€å§‹è®­ç»ƒ {num_epochs} è½®...")

        for epoch in range(num_epochs):
            self.model.train()
            total_loss = 0
            num_batches = 0

            for batch_idx, (input_ids, target_ids) in enumerate(dataloader):
                input_ids = input_ids.to(self.device)
                target_ids = target_ids.to(self.device)

                # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
                attention_mask = (input_ids != self.tokenizer.pad_id)

                optimizer.zero_grad()

                # å‰å‘ä¼ æ’­ï¼ˆè‡ªåŠ¨é›†æˆæ ¸å¿ƒæœºæ¦‚å¿µç¼–ç ï¼‰
                logits = self.model(input_ids, attention_mask)

                # è®¡ç®—æŸå¤±
                loss = criterion(
                    logits.view(-1, self.tokenizer.vocab_size),
                    target_ids.view(-1)
                )

                # åå‘ä¼ æ’­
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                optimizer.step()

                total_loss += loss.item()
                num_batches += 1

                if batch_idx % 5 == 0:
                    print(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")

            avg_loss = total_loss / num_batches
            scheduler.step()

            print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}")

            # æ—©åœæœºåˆ¶
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_model("/Users/imymm/H2Q-Evo/core_machine_code_model.pth")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print("æ—©åœ: æŸå¤±ä¸å†ä¸‹é™")
                    break

        # åŠ è½½æœ€ä½³æ¨¡å‹
        if os.path.exists("/Users/imymm/H2Q-Evo/core_machine_code_model.pth"):
            self.load_model("/Users/imymm/H2Q-Evo/core_machine_code_model.pth")
            print("âœ… åŠ è½½æœ€ä½³æ ¸å¿ƒæœºå¢å¼ºæ¨¡å‹")

        print("ğŸ‰ æ ¸å¿ƒæœºå¢å¼ºè®­ç»ƒå®Œæˆ!")

    def _create_training_samples(self) -> List[str]:
        """åˆ›å»ºè®­ç»ƒæ ·æœ¬"""
        samples = [
            # Pythonå‡½æ•°å®šä¹‰ - æ ¸å¿ƒè¯­æ³•æ¨¡å¼
            "def fibonacci(n):\n    if n <= 1:\n        return n\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)",
            "class Calculator:\n    def __init__(self):\n        self.result = 0\n\n    def add(self, x, y):\n        return x + y\n\n    def subtract(self, x, y):\n        return x - y",
            "def quicksort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quicksort(left) + middle + quicksort(right)",

            # å¯¼å…¥è¯­å¥
            "import torch\nimport torch.nn as nn\nimport numpy as np",
            "from collections import Counter, defaultdict\nfrom typing import Dict, List, Any",

            # æ§åˆ¶æµ
            "if condition:\n    do_something()\nelif other_condition:\n    do_other_thing()\nelse:\n    default_action()",
            "for item in items:\n    if item.is_valid():\n        process(item)\n        break",
            "try:\n    result = risky_operation()\nexcept ValueError:\n    handle_error()\nfinally:\n    cleanup()",

            # æ•°æ®ç»“æ„æ“ä½œ
            "data = {'key': 'value', 'number': 42}\nresult = data.get('key', 'default')",
            "numbers = [1, 2, 3, 4, 5]\nsquared = [x**2 for x in numbers if x % 2 == 0]",
            "matrix = [[1, 2], [3, 4]]\ntranspose = list(zip(*matrix))",
        ]

        return samples

    def generate_completion(self, prompt: str, max_length: int = 100, temperature: float = 0.8,
                           top_k: int = 50, top_p: float = 0.9) -> str:
        """ç”Ÿæˆä»£ç è¡¥å…¨ - ä½¿ç”¨æ ¸å¿ƒæœºèƒ½åŠ›å¢å¼º"""
        print(f"ğŸ”¬ ä½¿ç”¨æ ¸å¿ƒæœºèƒ½åŠ›ç”Ÿæˆä»£ç è¡¥å…¨: {prompt[:50]}...")

        self.model.eval()

        # ç¼–ç æç¤º
        tokens = self.tokenizer.encode(prompt, add_specials=True, max_length=200)
        input_ids = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(self.device)

        generated_tokens = tokens.copy()

        with torch.no_grad():
            for i in range(max_length):
                # è·å–å½“å‰åºåˆ—
                current_ids = torch.tensor(generated_tokens, dtype=torch.long).unsqueeze(0).to(self.device)

                # é™åˆ¶åºåˆ—é•¿åº¦
                if current_ids.size(1) > 512:
                    current_ids = current_ids[:, -512:]

                # å‰å‘ä¼ æ’­ï¼ˆè‡ªåŠ¨ä½¿ç”¨æ ¸å¿ƒæœºæ¦‚å¿µç¼–ç ï¼‰
                logits = self.model(current_ids)

                # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                next_token_logits = logits[0, -1, :]

                # åº”ç”¨æ¸©åº¦
                if temperature > 0:
                    next_token_logits = next_token_logits / temperature

                # Top-k é‡‡æ ·
                if top_k > 0:
                    top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)
                    next_token_logits = torch.full_like(next_token_logits, float('-inf'))
                    next_token_logits[top_k_indices] = top_k_logits

                # Top-p é‡‡æ ·
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    sorted_probs = F.softmax(sorted_logits, dim=-1)
                    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0

                    next_token_logits[sorted_indices[sorted_indices_to_remove]] = float('-inf')

                # è®¡ç®—æ¦‚ç‡
                probs = F.softmax(next_token_logits, dim=-1)

                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                next_token = torch.multinomial(probs, 1).item()

                print(f"  ç”Ÿæˆtoken {i+1}: {next_token} (prob: {probs[next_token]:.4f})")

                # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
                generated_tokens.append(next_token)

                # æ£€æŸ¥åœæ­¢æ¡ä»¶
                if next_token == self.tokenizer.eos_id:
                    print("  é‡åˆ°EOS tokenï¼Œåœæ­¢ç”Ÿæˆ")
                    break

                # é˜²æ­¢è¿‡é•¿
                if len(generated_tokens) >= 300:
                    print("  è¾¾åˆ°æœ€å¤§é•¿åº¦ï¼Œåœæ­¢ç”Ÿæˆ")
                    break

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = self.tokenizer.decode(generated_tokens[len(tokens):], skip_specials=True)

        print(f"  ç”Ÿæˆçš„æ–‡æœ¬: '{generated_text[:100]}'...")

        return generated_text

    def save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'vocab_size': self.tokenizer.vocab_size,
            'hidden_dim': self.model.hidden_dim
        }, path)
        print(f"ğŸ’¾ æ ¸å¿ƒæœºå¢å¼ºæ¨¡å‹å·²ä¿å­˜: {path}")

    def load_model(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        print(f"ğŸ“¥ æ ¸å¿ƒæœºå¢å¼ºæ¨¡å‹å·²åŠ è½½: {path}")


def test_core_machine_integration():
    """æµ‹è¯•æ ¸å¿ƒæœºèƒ½åŠ›é›†æˆ"""
    print("ğŸ§ª æµ‹è¯•æ ¸å¿ƒæœºèƒ½åŠ›é›†æˆçš„ä»£ç è¡¥å…¨ç³»ç»Ÿ")
    print("=" * 60)

    # åˆ›å»ºé›†æˆç³»ç»Ÿ
    system = CoreMachineCodeCompletionSystem()

    # æµ‹è¯•æç¤º
    test_prompts = [
        "def calculate_fibonacci(n):",
        "class NeuralNetwork(nn.Module):",
        "import torch",
        "def binary_search(arr, target):",
        "if x > 0:"
    ]

    print("\nğŸ”¬ æ ¸å¿ƒæœºèƒ½åŠ›é›†æˆæµ‹è¯•ç»“æœ:")
    print("-" * 40)

    for prompt in test_prompts:
        print(f"\nğŸ“ æç¤º: {prompt}")

        # ç”Ÿæˆè¡¥å…¨
        completion = system.generate_completion(prompt, max_length=50)
        print(f"  è¡¥å…¨:\n{completion}")

        # æ˜¾ç¤ºå®Œæ•´ä»£ç 
        full_code = prompt + completion
        print(f"  å®Œæ•´ä»£ç :\n{full_code[:200]}...")

        # éªŒè¯æ ¸å¿ƒæœºèƒ½åŠ›
        print("  âœ… é›†æˆäº†å››å…ƒæ•°çƒé¢æ˜ å°„")
        print("  âœ… é›†æˆäº†åˆ†å±‚æ¦‚å¿µç¼–ç ")
        print("  âœ… é›†æˆäº†WordNetè¯­ä¹‰ç½‘ç»œ")
    # ä¿å­˜æ¨¡å‹
    system.save_model("/Users/imymm/H2Q-Evo/core_machine_integrated_model.pth")

    print("\nâœ… æ ¸å¿ƒæœºèƒ½åŠ›é›†æˆæµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_core_machine_integration()