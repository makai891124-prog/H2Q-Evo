#!/usr/bin/env python3
"""
AGIè®­ç»ƒæ•ˆæœåˆ†ææŠ¥å‘Šç”Ÿæˆå™¨
"""
import json
import os
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np

def generate_training_analysis_report():
    """ç”Ÿæˆè®­ç»ƒæ•ˆæœåˆ†ææŠ¥å‘Š"""

    # è¯»å–è®­ç»ƒæŠ¥å‘Š
    report_path = Path("reports/training_report.json")
    if not report_path.exists():
        print("âŒ æ‰¾ä¸åˆ°è®­ç»ƒæŠ¥å‘Šæ–‡ä»¶")
        return None

    with open(report_path, 'r', encoding='utf-8') as f:
        report_data = json.load(f)

    # åˆ†æè®­ç»ƒæ•ˆæœ
    train_losses = report_data['training_history']['train_losses']
    val_losses = report_data['training_history']['val_losses']

    analysis = {
        'summary': {
            'model_type': report_data['training_summary']['model_type'],
            'total_epochs': report_data['training_summary']['total_epochs'],
            'final_train_loss': report_data['training_summary']['final_train_loss'],
            'final_val_loss': report_data['training_summary']['final_val_loss'],
            'best_val_loss': report_data['training_summary']['best_val_loss'],
            'training_timestamp': report_data['timestamp'],
            'algorithm_used': report_data['algorithm_used']
        },
        'performance_metrics': {
            'convergence_rate': calculate_convergence_rate(train_losses),
            'stability_score': calculate_stability_score(val_losses),
            'improvement_ratio': calculate_improvement_ratio(train_losses),
            'overfitting_indicator': calculate_overfitting_indicator(train_losses, val_losses)
        },
        'training_characteristics': {
            'loss_reduction_pattern': analyze_loss_pattern(train_losses),
            'validation_trend': analyze_validation_trend(val_losses),
            'learning_efficiency': analyze_learning_efficiency(train_losses, val_losses)
        },
        'recommendations': generate_training_recommendations(train_losses, val_losses)
    }

    # ä¿å­˜è¯¦ç»†åˆ†ææŠ¥å‘Š
    analysis_path = Path("reports/training_analysis_report.json")
    analysis_path.parent.mkdir(exist_ok=True)

    with open(analysis_path, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)

    print(f"âœ… è®­ç»ƒåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {analysis_path}")
    return analysis

def calculate_convergence_rate(losses):
    """è®¡ç®—æ”¶æ•›é€Ÿç‡"""
    if len(losses) < 2:
        return 0.0

    initial_loss = losses[0]
    final_loss = losses[-1]
    total_epochs = len(losses)

    # è®¡ç®—æŸå¤±å‡å°‘çš„é€Ÿç‡
    convergence_rate = (initial_loss - final_loss) / (initial_loss * total_epochs)
    return max(0.0, convergence_rate)

def calculate_stability_score(losses):
    """è®¡ç®—ç¨³å®šæ€§åˆ†æ•°"""
    if len(losses) < 3:
        return 0.5

    # è®¡ç®—æŸå¤±çš„æ ‡å‡†å·®ï¼ˆè¶Šå°è¶Šç¨³å®šï¼‰
    std_dev = np.std(losses)
    mean_loss = np.mean(losses)

    # æ ‡å‡†åŒ–ç¨³å®šæ€§åˆ†æ•° (0-1, 1è¡¨ç¤ºéå¸¸ç¨³å®š)
    if mean_loss == 0:
        return 1.0

    stability_score = 1.0 / (1.0 + (std_dev / mean_loss))
    return stability_score

def calculate_improvement_ratio(losses):
    """è®¡ç®—æ”¹è¿›æ¯”ç‡"""
    if len(losses) < 2:
        return 0.0

    # è®¡ç®—å‰50%å’Œå50%çš„å¹³å‡æŸå¤±å¯¹æ¯”
    midpoint = len(losses) // 2
    early_avg = np.mean(losses[:midpoint])
    late_avg = np.mean(losses[midpoint:])

    if early_avg == 0:
        return 1.0

    improvement_ratio = (early_avg - late_avg) / early_avg
    return max(0.0, improvement_ratio)

def calculate_overfitting_indicator(train_losses, val_losses):
    """è®¡ç®—è¿‡æ‹ŸåˆæŒ‡æ ‡"""
    if len(train_losses) != len(val_losses):
        return 0.5

    # è®¡ç®—è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±çš„å·®å¼‚è¶‹åŠ¿
    train_final = train_losses[-1]
    val_final = val_losses[-1]

    if train_final == 0:
        return 0.0

    overfitting_ratio = (val_final - train_final) / train_final
    return max(0.0, min(1.0, overfitting_ratio))

def analyze_loss_pattern(losses):
    """åˆ†ææŸå¤±æ¨¡å¼"""
    if len(losses) < 3:
        return "æ•°æ®ä¸è¶³"

    # æ£€æŸ¥æ˜¯å¦å•è°ƒé€’å‡
    decreasing = all(losses[i] >= losses[i+1] for i in range(len(losses)-1))

    # è®¡ç®—æŸå¤±å˜åŒ–çš„å¹³æ»‘åº¦
    diffs = [abs(losses[i+1] - losses[i]) for i in range(len(losses)-1)]
    avg_change = np.mean(diffs)
    max_change = max(diffs)

    if decreasing and avg_change < 0.01:
        return "å¹³æ»‘æ”¶æ•›"
    elif decreasing:
        return "ç¨³æ­¥ä¸‹é™"
    else:
        return "æ³¢åŠ¨è¾ƒå¤§"

def analyze_validation_trend(val_losses):
    """åˆ†æéªŒè¯è¶‹åŠ¿"""
    if len(val_losses) < 3:
        return "æ•°æ®ä¸è¶³"

    # æ£€æŸ¥éªŒè¯æŸå¤±æ˜¯å¦æŒç»­æ”¹å–„
    improving = val_losses[0] > val_losses[-1]

    # è®¡ç®—éªŒè¯æŸå¤±çš„ç¨³å®šæ€§
    stability = calculate_stability_score(val_losses)

    if improving and stability > 0.8:
        return "ç¨³å®šæ”¹å–„"
    elif improving:
        return "é€æ­¥æ”¹å–„"
    else:
        return "éœ€è¦è°ƒæ•´"

def analyze_learning_efficiency(train_losses, val_losses):
    """åˆ†æå­¦ä¹ æ•ˆç‡"""
    convergence = calculate_convergence_rate(train_losses)
    stability = calculate_stability_score(val_losses)

    efficiency_score = (convergence + stability) / 2

    if efficiency_score > 0.8:
        return "é«˜æ•ˆå­¦ä¹ "
    elif efficiency_score > 0.6:
        return "è‰¯å¥½å­¦ä¹ "
    elif efficiency_score > 0.4:
        return "ä¸€èˆ¬å­¦ä¹ "
    else:
        return "å­¦ä¹ æ•ˆç‡å¾…æ”¹å–„"

def generate_training_recommendations(train_losses, val_losses):
    """ç”Ÿæˆè®­ç»ƒå»ºè®®"""
    recommendations = []

    convergence_rate = calculate_convergence_rate(train_losses)
    stability_score = calculate_stability_score(val_losses)
    overfitting = calculate_overfitting_indicator(train_losses, val_losses)

    if convergence_rate < 0.1:
        recommendations.append("è€ƒè™‘å¢åŠ å­¦ä¹ ç‡æˆ–è°ƒæ•´ä¼˜åŒ–å™¨")
        recommendations.append("æ£€æŸ¥æ•°æ®è´¨é‡å’Œé¢„å¤„ç†")

    if stability_score < 0.5:
        recommendations.append("å¢åŠ æ­£åˆ™åŒ–æŠ€æœ¯ï¼ˆå¦‚dropout, weight decayï¼‰")
        recommendations.append("å°è¯•æ›´å°çš„æ‰¹æ¬¡å¤§å°")

    if overfitting > 0.3:
        recommendations.append("å®æ–½æ—©åœæœºåˆ¶")
        recommendations.append("å¢åŠ æ•°æ®å¢å¼ºæˆ–æ­£åˆ™åŒ–")

    if len(recommendations) == 0:
        recommendations.append("è®­ç»ƒæ•ˆæœè‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘æ‰©å±•æ•°æ®é›†")
        recommendations.append("å°è¯•æ›´å¤æ‚çš„æ¨¡å‹æ¶æ„")

    return recommendations

def create_visualization_report():
    """åˆ›å»ºå¯è§†åŒ–æŠ¥å‘Š"""
    try:
        # è¯»å–è®­ç»ƒæ•°æ®
        report_path = Path("reports/training_report.json")
        if not report_path.exists():
            return

        with open(report_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        train_losses = data['training_history']['train_losses']
        val_losses = data['training_history']['val_losses']
        epochs = list(range(1, len(train_losses) + 1))

        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(12, 8))

        # æŸå¤±æ›²çº¿
        plt.subplot(2, 2, 1)
        plt.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        plt.plot(epochs, val_losses, 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        plt.xlabel('è®­ç»ƒè½®æ¬¡')
        plt.ylabel('æŸå¤±å€¼')
        plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # æŸå¤±å·®å€¼
        plt.subplot(2, 2, 2)
        loss_diff = [v - t for t, v in zip(train_losses, val_losses)]
        plt.plot(epochs, loss_diff, 'g-', linewidth=2)
        plt.xlabel('è®­ç»ƒè½®æ¬¡')
        plt.ylabel('éªŒè¯æŸå¤± - è®­ç»ƒæŸå¤±')
        plt.title('è¿‡æ‹ŸåˆæŒ‡æ ‡')
        plt.grid(True, alpha=0.3)

        # æ”¶æ•›åˆ†æ
        plt.subplot(2, 2, 3)
        if len(train_losses) > 1:
            convergence = [(train_losses[0] - loss) / train_losses[0] for loss in train_losses]
            plt.plot(epochs, convergence, 'purple', linewidth=2)
            plt.xlabel('è®­ç»ƒè½®æ¬¡')
            plt.ylabel('æ”¶æ•›ç¨‹åº¦')
            plt.title('è®­ç»ƒæ”¶æ•›åˆ†æ')
            plt.grid(True, alpha=0.3)

        # ç¨³å®šæ€§åˆ†æ
        plt.subplot(2, 2, 4)
        window_size = min(5, len(val_losses))
        if len(val_losses) >= window_size:
            stability = []
            for i in range(window_size, len(val_losses) + 1):
                window = val_losses[i-window_size:i]
                stability.append(1.0 / (1.0 + np.std(window)))
            plt.plot(range(window_size, len(val_losses) + 1), stability, 'orange', linewidth=2)
            plt.xlabel('è®­ç»ƒè½®æ¬¡')
            plt.ylabel('ç¨³å®šæ€§åˆ†æ•°')
            plt.title('è®­ç»ƒç¨³å®šæ€§åˆ†æ')
            plt.grid(True, alpha=0.3)

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        chart_path = Path("reports/training_analysis_chart.png")
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"âœ… è®­ç»ƒåˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {chart_path}")

    except Exception as e:
        print(f"âš ï¸  åˆ›å»ºå¯è§†åŒ–æŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š ç”ŸæˆAGIè®­ç»ƒæ•ˆæœåˆ†ææŠ¥å‘Š")
    print("=" * 50)

    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    analysis = generate_training_analysis_report()

    if analysis:
        print("\nğŸ“ˆ è®­ç»ƒæ•ˆæœåˆ†æç»“æœ:")
        print(f"   æ¨¡å‹ç±»å‹: {analysis['summary']['model_type']}")
        print(f"   æ€»è®­ç»ƒè½®æ¬¡: {analysis['summary']['total_epochs']}")
        print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {analysis['summary']['final_train_loss']:.4f}")
        print(f"   æœ€ç»ˆéªŒè¯æŸå¤±: {analysis['summary']['final_val_loss']:.4f}")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {analysis['summary']['best_val_loss']:.4f}")
        print("\nğŸ¯ æ€§èƒ½æŒ‡æ ‡:")
        print(f"   æ”¶æ•›é€Ÿç‡: {analysis['performance_metrics']['convergence_rate']:.4f}")
        print(f"   ç¨³å®šæ€§åˆ†æ•°: {analysis['performance_metrics']['stability_score']:.4f}")
        print(f"   æ”¹è¿›æ¯”ç‡: {analysis['performance_metrics']['improvement_ratio']:.4f}")
        print(f"   è¿‡æ‹ŸåˆæŒ‡æ ‡: {analysis['performance_metrics']['overfitting_indicator']:.4f}")
        print("\nğŸ” è®­ç»ƒç‰¹å¾:")
        print(f"   æŸå¤±æ¨¡å¼: {analysis['training_characteristics']['loss_reduction_pattern']}")
        print(f"   éªŒè¯è¶‹åŠ¿: {analysis['training_characteristics']['validation_trend']}")
        print(f"   å­¦ä¹ æ•ˆç‡: {analysis['training_characteristics']['learning_efficiency']}")

        print("\nğŸ’¡ è®­ç»ƒå»ºè®®:")
        for i, rec in enumerate(analysis['recommendations'], 1):
            print(f"   {i}. {rec}")

    # åˆ›å»ºå¯è§†åŒ–æŠ¥å‘Š
    create_visualization_report()

    print("\n" + "=" * 50)
    print("âœ… è®­ç»ƒæ•ˆæœåˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print("ğŸ“ æŸ¥çœ‹ reports/ ç›®å½•è·å–è¯¦ç»†æŠ¥å‘Šå’Œå›¾è¡¨")

if __name__ == "__main__":
    main()