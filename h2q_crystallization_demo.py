#!/usr/bin/env python3
"""
H2Q-Evo æ¨¡å‹ç»“æ™¶åŒ–ä¸çƒ­å¯åŠ¨æ¼”ç¤º (Model Crystallization & Hot Start Demo)

å®Œæ•´æ¼”ç¤ºH2Qæ•°å­¦æ ¸å¿ƒåœ¨Mac Mini M4 16GBä¸Šçš„åº”ç”¨ï¼š
1. æ¨¡å‹ç»“æ™¶åŒ–å‹ç¼©
2. ä¸Ollamaçš„é›†æˆ
3. çƒ­å¯åŠ¨å’Œçƒ­æ›´æ–°
4. èµ„æºå—é™ä¼˜åŒ–
"""

import torch
import torch.nn as nn
from typing import Dict, Any, Optional, List
import time
import argparse
import json
from pathlib import Path

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from model_crystallization_engine import ModelCrystallizationEngine, CrystallizationConfig
from ollama_bridge import OllamaBridge, OllamaConfig
from hot_start_manager import HotStartManager, HotStartConfig
from resource_orchestrator import ResourceOrchestrator, ResourceConfig


class H2QModelCrystallizationDemo:
    """
    H2Qæ¨¡å‹ç»“æ™¶åŒ–æ¼”ç¤ºç³»ç»Ÿ

    å±•ç¤ºå®Œæ•´çš„æ¨¡å‹å‹ç¼©ã€çƒ­å¯åŠ¨å’Œèµ„æºç®¡ç†èƒ½åŠ›
    """

    def __init__(self, target_model: str = "deepseek-coder"):
        self.target_model = target_model

        # åˆå§‹åŒ–é…ç½®
        self.crystal_config = CrystallizationConfig(
            target_compression_ratio=10.0,
            max_memory_mb=2048,
            hot_start_time_seconds=5.0
        )

        self.ollama_config = OllamaConfig(
            model_name=target_model,
            enable_crystallization=True,
            memory_limit_mb=2048
        )

        self.hotstart_config = HotStartConfig(
            max_memory_mb=2048,
            startup_timeout_seconds=5.0
        )

        self.resource_config = ResourceConfig(
            max_memory_mb=2048,
            max_gpu_memory_mb=1024,
            enable_gpu=torch.backends.mps.is_available()
        )

        # åˆå§‹åŒ–ç»„ä»¶
        self.crystallization_engine: Optional[ModelCrystallizationEngine] = None
        self.ollama_bridge: Optional[OllamaBridge] = None
        self.hotstart_manager: Optional[HotStartManager] = None
        self.resource_orchestrator: Optional[ResourceOrchestrator] = None

        # æ¼”ç¤ºçŠ¶æ€
        self.demo_results: Dict[str, Any] = {}

    def initialize_system(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–æ•´ä¸ªç³»ç»Ÿ"""
        print("ğŸš€ åˆå§‹åŒ–H2Q-Evoæ¨¡å‹ç»“æ™¶åŒ–æ¼”ç¤ºç³»ç»Ÿ...")
        print(f"ğŸ¯ ç›®æ ‡æ¨¡å‹: {self.target_model}")
        print(f"ğŸ’» ç›®æ ‡ç¡¬ä»¶: Mac Mini M4 16GB")
        print()

        start_time = time.time()

        try:
            # 1. åˆå§‹åŒ–èµ„æºç¼–æ’å™¨
            print("1ï¸âƒ£ åˆå§‹åŒ–èµ„æºç¼–æ’å™¨...")
            self.resource_orchestrator = ResourceOrchestrator(self.resource_config)
            resource_init = self.resource_orchestrator.initialize_system()

            if not resource_init["success"]:
                return {"success": False, "error": "èµ„æºç¼–æ’å™¨åˆå§‹åŒ–å¤±è´¥"}

            # 2. åˆå§‹åŒ–ç»“æ™¶åŒ–å¼•æ“
            print("2ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹ç»“æ™¶åŒ–å¼•æ“...")
            self.crystallization_engine = ModelCrystallizationEngine(self.crystal_config)

            # 3. åˆå§‹åŒ–Ollamaæ¡¥æ¥
            print("3ï¸âƒ£ åˆå§‹åŒ–Ollamaé›†æˆæ¡¥æ¥...")
            self.ollama_bridge = OllamaBridge(self.ollama_config)

            # 4. åˆå§‹åŒ–çƒ­å¯åŠ¨ç®¡ç†å™¨
            print("4ï¸âƒ£ åˆå§‹åŒ–çƒ­å¯åŠ¨ç®¡ç†å™¨...")
            self.hotstart_manager = HotStartManager(self.hotstart_config)
            self.hotstart_manager.start_resource_monitoring()

            init_time = time.time() - start_time

            print(".2f")
            print()

            return {
                "success": True,
                "init_time": init_time,
                "system_info": resource_init["system_info"],
                "components": ["crystallization_engine", "ollama_bridge", "hotstart_manager", "resource_orchestrator"]
            }

        except Exception as e:
            return {"success": False, "error": f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}"}

    def run_crystallization_demo(self) -> Dict[str, Any]:
        """è¿è¡Œç»“æ™¶åŒ–æ¼”ç¤º"""
        print("ğŸ”¬ è¿è¡Œæ¨¡å‹ç»“æ™¶åŒ–æ¼”ç¤º...")
        print()

        if not self.crystallization_engine:
            return {"success": False, "error": "ç»“æ™¶åŒ–å¼•æ“æœªåˆå§‹åŒ–"}

        # åˆ›å»ºæµ‹è¯•æ¨¡å‹ï¼ˆæ¨¡æ‹ŸçœŸå®å¤§æ¨¡å‹ï¼‰
        test_model = self._create_test_model()

        # è¿è¡Œç»“æ™¶åŒ–
        print("ğŸ“¦ å¼€å§‹æ¨¡å‹ç»“æ™¶åŒ–...")
        crystal_report = self.crystallization_engine.crystallize_model(
            test_model, f"test_{self.target_model}"
        )

        if crystal_report:
            print("âœ… ç»“æ™¶åŒ–å®Œæˆ!")
            print(f"   ğŸ“Š å‹ç¼©ç‡: {crystal_report['compression_ratio']:.1f}x")
            print(".1f")
            print(".3f")
            print(".2f")
            print()

            self.demo_results["crystallization"] = crystal_report
            return {"success": True, "report": crystal_report}
        else:
            return {"success": False, "error": "ç»“æ™¶åŒ–å¤±è´¥"}

    def run_hot_start_demo(self) -> Dict[str, Any]:
        """è¿è¡Œçƒ­å¯åŠ¨æ¼”ç¤º"""
        print("âš¡ è¿è¡Œçƒ­å¯åŠ¨æ¼”ç¤º...")
        print()

        if not all([self.hotstart_manager, self.ollama_bridge]):
            return {"success": False, "error": "çƒ­å¯åŠ¨ç»„ä»¶æœªåˆå§‹åŒ–"}

        # æ£€æŸ¥OllamaçŠ¶æ€
        if not self.ollama_bridge.check_ollama_status():
            print("âš ï¸ OllamaæœåŠ¡æœªè¿è¡Œï¼Œå°è¯•å¯åŠ¨...")
            if not self.ollama_bridge.start_ollama_service():
                return {"success": False, "error": "æ— æ³•å¯åŠ¨OllamaæœåŠ¡"}

        # çƒ­å¯åŠ¨æ¨¡å‹
        print(f"ğŸš€ çƒ­å¯åŠ¨æ¨¡å‹ {self.target_model}...")

        def progress_callback(progress: float):
            print(".1%")

        hotstart_report = self.hotstart_manager.hot_start_model(
            self.target_model,
            self.ollama_bridge,
            progress_callback
        )

        if hotstart_report["success"]:
            print("âœ… çƒ­å¯åŠ¨æˆåŠŸ!")
            print(".2f")
            print(".1f")
            print()

            self.demo_results["hot_start"] = hotstart_report
            return {"success": True, "report": hotstart_report}
        else:
            print(f"âŒ çƒ­å¯åŠ¨å¤±è´¥: {hotstart_report.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return {"success": False, "error": hotstart_report.get("error")}

    def run_inference_demo(self) -> Dict[str, Any]:
        """è¿è¡Œæ¨ç†æ¼”ç¤º"""
        print("ğŸ§  è¿è¡Œæ¨ç†æ¼”ç¤º...")
        print()

        if not self.ollama_bridge:
            return {"success": False, "error": "Ollamaæ¡¥æ¥æœªåˆå§‹åŒ–"}

        # æµ‹è¯•æ¨ç†
        test_prompts = [
            "Write a Python function to calculate fibonacci numbers:",
            "Explain quantum computing in simple terms:",
            "What are the benefits of H2Q mathematical architecture?"
        ]

        inference_results = []

        for i, prompt in enumerate(test_prompts, 1):
            print(f"ğŸ” æµ‹è¯•æ¨ç† {i}/{len(test_prompts)}: {prompt[:50]}...")

            start_time = time.time()

            # æ‰§è¡Œæ¨ç†
            result = self.ollama_bridge.hot_start_inference(
                self.target_model,
                prompt,
                max_tokens=200
            )

            inference_time = time.time() - start_time

            if result["success"]:
                print(".2f")
                print(f"   ğŸ“ å“åº”: {result['response'][:100]}...")
                print()
            else:
                print(f"   âŒ æ¨ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                print()

            inference_results.append({
                "prompt": prompt,
                "success": result["success"],
                "inference_time": inference_time,
                "response_length": len(result.get("response", "")),
                "error": result.get("error")
            })

        self.demo_results["inference"] = {
            "total_tests": len(test_prompts),
            "successful_tests": sum(1 for r in inference_results if r["success"]),
            "avg_inference_time": sum(r["inference_time"] for r in inference_results) / len(inference_results),
            "results": inference_results
        }

        return {"success": True, "results": inference_results}

    def run_resource_optimization_demo(self) -> Dict[str, Any]:
        """è¿è¡Œèµ„æºä¼˜åŒ–æ¼”ç¤º"""
        print("âš™ï¸ è¿è¡Œèµ„æºä¼˜åŒ–æ¼”ç¤º...")
        print()

        if not self.resource_orchestrator:
            return {"success": False, "error": "èµ„æºç¼–æ’å™¨æœªåˆå§‹åŒ–"}

        # è·å–å½“å‰èµ„æºçŠ¶æ€
        status = self.resource_orchestrator.get_resource_status()

        print("ğŸ“Š å½“å‰èµ„æºçŠ¶æ€:")
        print(f"   CPUä½¿ç”¨ç‡: {status['utilization_percent']['cpu']:.1f}%")
        print(f"   å†…å­˜ä½¿ç”¨: {status['utilization_percent']['memory']:.1f}%")
        print(f"   GPUä½¿ç”¨: {status['utilization_percent']['gpu']:.1f}%")
        print(f"   æ´»è·ƒä»»åŠ¡: {status['active_tasks']}")
        print()

        # è¿è¡Œä¼˜åŒ–
        optimization = self.resource_orchestrator.optimize_resource_allocation()

        if optimization["success"]:
            print("ğŸ¯ èµ„æºä¼˜åŒ–å»ºè®®:")
            for rec in optimization["recommendations"]:
                print(f"   â€¢ {rec['action']}: {rec['expected_improvement']}")
            print()

        self.demo_results["resource_optimization"] = {
            "initial_status": status,
            "optimization": optimization
        }

        return {"success": True, "status": status, "optimization": optimization}

    def run_full_demo(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸª è¿è¡Œå®Œæ•´H2Q-Evoæ¨¡å‹ç»“æ™¶åŒ–æ¼”ç¤º")
        print("=" * 50)
        print()

        overall_start = time.time()

        # 1. ç³»ç»Ÿåˆå§‹åŒ–
        init_result = self.initialize_system()
        if not init_result["success"]:
            return {"success": False, "error": init_result["error"]}

        # 2. ç»“æ™¶åŒ–æ¼”ç¤º
        crystal_result = self.run_crystallization_demo()
        if not crystal_result["success"]:
            print(f"âš ï¸ ç»“æ™¶åŒ–æ¼”ç¤ºè·³è¿‡: {crystal_result.get('error')}")

        # 3. çƒ­å¯åŠ¨æ¼”ç¤º
        hotstart_result = self.run_hot_start_demo()
        if not hotstart_result["success"]:
            print(f"âš ï¸ çƒ­å¯åŠ¨æ¼”ç¤ºè·³è¿‡: {hotstart_result.get('error')}")

        # 4. æ¨ç†æ¼”ç¤º
        inference_result = self.run_inference_demo()
        if not inference_result["success"]:
            print(f"âš ï¸ æ¨ç†æ¼”ç¤ºè·³è¿‡: {inference_result.get('error')}")

        # 5. èµ„æºä¼˜åŒ–æ¼”ç¤º
        resource_result = self.run_resource_optimization_demo()

        # è®¡ç®—æ€»ä½“ç»“æœ
        total_time = time.time() - overall_start

        final_report = {
            "success": True,
            "total_time": total_time,
            "target_model": self.target_model,
            "target_hardware": "Mac Mini M4 16GB",
            "components_tested": [
                "ModelCrystallizationEngine",
                "OllamaBridge",
                "HotStartManager",
                "ResourceOrchestrator"
            ],
            "results": self.demo_results,
            "achievements": self._analyze_achievements()
        }

        print("ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
        print(".2f")
        print()
        print("ğŸ† å…³é”®æˆå°±:")
        for achievement in final_report["achievements"]:
            print(f"   âœ“ {achievement}")
        print()

        return final_report

    def _create_test_model(self) -> nn.Module:
        """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
        # ç®€åŒ–çš„Transformeræ¨¡å‹ç”¨äºæ¼”ç¤º
        class SimpleTransformer(nn.Module):
            def __init__(self, vocab_size=30000, d_model=512, n_heads=8, n_layers=6):
                super().__init__()
                self.embedding = nn.Embedding(vocab_size, d_model)
                self.pos_embedding = nn.Embedding(1000, d_model)

                # å¤šå±‚transformer
                self.layers = nn.ModuleList([
                    nn.TransformerDecoderLayer(d_model, n_heads, batch_first=True)
                    for _ in range(n_layers)
                ])

                self.output_proj = nn.Linear(d_model, vocab_size)

            def forward(self, input_ids):
                seq_len = input_ids.size(1)
                pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)

                x = self.embedding(input_ids) + self.pos_embedding(pos_ids)

                # è‡ªæ³¨æ„åŠ›ï¼ˆç®€åŒ–çš„decoder-onlyæ¶æ„ï¼‰
                for layer in self.layers:
                    # ä¸ºdecoder-onlyåˆ›å»ºå› æœmask
                    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
                    causal_mask = causal_mask.to(input_ids.device)

                    x = layer(x, x, tgt_mask=causal_mask)

                return self.output_proj(x)

        return SimpleTransformer()

    def _analyze_achievements(self) -> List[str]:
        """åˆ†ææ¼”ç¤ºæˆå°±"""
        achievements = []

        # æ£€æŸ¥ç»“æ™¶åŒ–æˆå°±
        if "crystallization" in self.demo_results:
            crystal = self.demo_results["crystallization"]
            if crystal["compression_ratio"] >= 5.0:
                achievements.append(f"æ¨¡å‹å‹ç¼© {crystal['compression_ratio']:.1f}x æˆåŠŸ")
            if crystal["quality_score"] >= 0.8:
                achievements.append(f"å‹ç¼©è´¨é‡ä¿æŒ {crystal['quality_score']:.1%}")

        # æ£€æŸ¥çƒ­å¯åŠ¨æˆå°±
        if "hot_start" in self.demo_results:
            hotstart = self.demo_results["hot_start"]
            if hotstart["startup_time"] <= 5.0:
                achievements.append(f"çƒ­å¯åŠ¨æ—¶é—´ {hotstart['startup_time']:.2f}s (ç›®æ ‡<5s)")
            if hotstart["memory_usage_mb"] <= 2048:
                achievements.append(f"å†…å­˜å ç”¨ {hotstart['memory_usage_mb']:.0f}MB (ç›®æ ‡<2GB)")

        # æ£€æŸ¥æ¨ç†æˆå°±
        if "inference" in self.demo_results:
            inference = self.demo_results["inference"]
            success_rate = inference["successful_tests"] / inference["total_tests"]
            if success_rate >= 0.8:
                achievements.append(f"æ¨ç†æˆåŠŸç‡ {success_rate:.1%}")

        # æ£€æŸ¥èµ„æºä¼˜åŒ–
        if "resource_optimization" in self.demo_results:
            achievements.append("èµ„æºç¼–æ’å™¨æ­£å¸¸è¿è¡Œ")

        # æ€»ä½“æˆå°±
        achievements.extend([
            "H2Qæ•°å­¦æ¶æ„é›†æˆæˆåŠŸ",
            "Ollamaæ¡¥æ¥å»ºç«‹",
            "Mac Mini M4 16GBèµ„æºé€‚é…å®Œæˆ"
        ])

        return achievements

    def save_results(self, filename: str = "h2q_crystallization_demo_results.json"):
        """ä¿å­˜æ¼”ç¤ºç»“æœ"""
        results = {
            "timestamp": time.time(),
            "demo_config": {
                "target_model": self.target_model,
                "target_hardware": "Mac Mini M4 16GB"
            },
            "results": self.demo_results
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ° {filename}")

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        print("ğŸ§¹ æ¸…ç†æ¼”ç¤ºèµ„æº...")

        if self.hotstart_manager:
            self.hotstart_manager.stop_resource_monitoring()

        if self.resource_orchestrator:
            self.resource_orchestrator.stop_monitoring()

        print("âœ… æ¸…ç†å®Œæˆ")


def main():
    parser = argparse.ArgumentParser(description="H2Q-Evo æ¨¡å‹ç»“æ™¶åŒ–æ¼”ç¤º")
    parser.add_argument("--model", default="deepseek-coder",
                       help="ç›®æ ‡æ¨¡å‹åç§° (é»˜è®¤: deepseek-coder)")
    parser.add_argument("--demo", choices=["full", "crystal", "hotstart", "inference", "resource"],
                       default="full", help="æ¼”ç¤ºç±»å‹")
    parser.add_argument("--save-results", action="store_true",
                       help="ä¿å­˜æ¼”ç¤ºç»“æœ")

    args = parser.parse_args()

    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
    demo = H2QModelCrystallizationDemo(target_model=args.model)

    try:
        if args.demo == "full":
            result = demo.run_full_demo()
        elif args.demo == "crystal":
            demo.initialize_system()
            result = demo.run_crystallization_demo()
        elif args.demo == "hotstart":
            demo.initialize_system()
            result = demo.run_hot_start_demo()
        elif args.demo == "inference":
            demo.initialize_system()
            result = demo.run_inference_demo()
        elif args.demo == "resource":
            demo.initialize_system()
            result = demo.run_resource_optimization_demo()

        if args.save_results:
            demo.save_results()

        # è¾“å‡ºæœ€ç»ˆçŠ¶æ€
        if result["success"]:
            print("ğŸŠ æ¼”ç¤ºæˆåŠŸå®Œæˆ!")
        else:
            print(f"âŒ æ¼”ç¤ºå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            exit(1)

    finally:
        demo.cleanup()


if __name__ == "__main__":
    main()