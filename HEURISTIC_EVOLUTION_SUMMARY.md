# 启发式开发过程整理与实施对比

来源文件：Upload Project File For Analysis

## 一、早期启发式策略提炼
1. **输入约束优先**：缺少项目文件时先请求压缩包或仓库链接，避免假设。
2. **结构化输出模板**：固定四段式输出（概览、结构、核心逻辑、质量评价）。
3. **分步流水线**：
   - 递归读取 `.py` 文件
   - 结构化拼接（带路径标记）
   - LLM 统一分析
   - 输出报告并落地为文件
4. **资源/上下文控制**：考虑 token 上限与长文本分段或流式输出。
5. **安全执行隔离**：用 Docker 作为沙箱执行训练/测试/验证。
6. **状态驱动进化**：用 `evo_state.json` 持久化 todo 与历史。
7. **资源约束导向**：针对 mac mini 16GB/200GB，采用流式训练、梯度累积、及时释放内存。

## 二、与当前实现的对比（已落地 vs 待补齐）
**已落地**
- Docker 隔离与生命周期（evolution_system.py）。
- Gemini 接入与验证链路（h2q_project/h2q/agi/gemini_cli_integration.py）。
- 公开基准评测切换（standard_benchmarks.py + honest_capability_system.py）。
- 进化循环与状态记录（evolution_results、evo_state.json）。

**待补齐/需要强化**
- “代码分析插件”仍缺少与当前进化系统的正式对接与自动触发。
- 任务调度策略需从“手工/单次”升级为“可重复、可回滚、可审计”。
- 对资源约束的自动化守护（内存/磁盘/运行时间监控）仍较弱。
- 训练/评测强制公开基准的流程需要更强的“硬阈值拒绝逻辑”。

## 三、对齐实施（无作弊/无欺骗原则）
**原则**：所有能力评估必须为公开基准；禁止自编题库与硬编码；评测日志可追溯。

**近期实施清单（可直接执行）**
1. **公开基准硬门禁**
   - 评测仅走 `standard_benchmarks.run_standard_benchmarks`。
   - 若 `datasets` 不可用或题量不足，直接中止评测并标记失败。
2. **多选/排序评分**
   - 使用多选评分降低蒙对概率。
   - 每个基准至少 100 题。
3. **任务记录与可审计**
   - 运行日志与评测报告统一落盘。
   - 重要变更写入变更记录（commit + report）。

## 四、当前差距与工程修复方向（与主流大模型相比）
- **训练规模与数据覆盖不足**：需完善数据管线与数据治理。
- **模型结构与优化策略不足**：需要更成熟的训练架构与对齐流程。
- **工具化/检索增强不足**：引入 RAG 与工具调用闭环。
- **可控性与可解释性不足**：强化审计与可追溯性。

## 五、下一步建议（不作弊、过程优先）
- 以公开基准评测作为“门禁”而非“成绩展示”。
- 建立“失败即回滚”的进化策略，确保过程可信。
- 逐步拆解能力模块（语言、推理、检索、工具调用）并分别评估。
