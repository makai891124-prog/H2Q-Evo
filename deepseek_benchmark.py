#!/usr/bin/env python3
"""
H2Q-Evo DeepSeekåŸºå‡†æµ‹è¯•ç³»ç»Ÿ
ä½¿ç”¨çœŸå®å¯åŠ¨çš„DeepSeekæ¨¡å‹è¿›è¡Œå…¬å¼€åŸºå‡†æµ‹è¯•éªŒè¯
"""

import time
import json
import subprocess
import sys
from typing import Dict, Any, List
import psutil
import os

class DeepSeekBenchmarkSuite:
    """DeepSeekåŸºå‡†æµ‹è¯•å¥—ä»¶"""

    def __init__(self):
        self.model_name = "deepseek-coder-v2:236b"
        self.test_results = {}
        self.system_info = self._get_system_info()

    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        memory = psutil.virtual_memory()
        return {
            "cpu_count": psutil.cpu_count(),
            "total_memory_gb": memory.total / (1024**3),
            "available_memory_gb": memory.available / (1024**3),
            "platform": "macOS" if os.uname().sysname == "Darwin" else os.uname().sysname
        }

    def run_ollama_inference(self, prompt: str, max_tokens: int = 100, timeout: int = 60) -> Dict[str, Any]:
        """è¿è¡ŒOllamaæ¨ç†"""
        result = {
            "success": False,
            "response": "",
            "inference_time": 0.0,
            "memory_usage": 0.0,
            "error": ""
        }

        try:
            # è®°å½•å¼€å§‹æ—¶çš„å†…å­˜ä½¿ç”¨
            start_memory = psutil.virtual_memory().used
            start_time = time.time()

            # æ„å»ºå‘½ä»¤
            cmd = [
                "ollama", "run", self.model_name,
                "--format", "json",
                prompt
            ]

            # è¿è¡Œæ¨ç†
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                env=dict(os.environ, OLLAMA_NUM_THREAD="4")  # é™åˆ¶çº¿ç¨‹æ•°
            )

            try:
                stdout, stderr = process.communicate(timeout=timeout)

                end_time = time.time()
                end_memory = psutil.virtual_memory().used

                result["inference_time"] = end_time - start_time
                result["memory_usage"] = (end_memory - start_memory) / (1024**2)  # MB

                if process.returncode == 0:
                    result["success"] = True
                    result["response"] = stdout.strip()
                else:
                    result["error"] = stderr.strip() or f"Process exited with code {process.returncode}"

            except subprocess.TimeoutExpired:
                process.kill()
                result["error"] = f"Inference timeout after {timeout} seconds"

        except Exception as e:
            result["error"] = str(e)

        return result

    def run_code_generation_test(self) -> Dict[str, Any]:
        """ä»£ç ç”Ÿæˆæµ‹è¯•"""
        print("ğŸ”§ è¿è¡Œä»£ç ç”Ÿæˆæµ‹è¯•...")

        test_cases = [
            {
                "name": "fibonacci_function",
                "prompt": "Write a Python function to calculate the nth Fibonacci number using recursion:",
                "expected_keywords": ["def", "fibonacci", "if", "return"]
            },
            {
                "name": "binary_search",
                "prompt": "Implement binary search algorithm in Python:",
                "expected_keywords": ["def", "while", "mid", "low", "high"]
            },
            {
                "name": "linked_list",
                "prompt": "Create a simple linked list class in Python with insert and display methods:",
                "expected_keywords": ["class", "def", "self", "next", "None"]
            }
        ]

        results = []
        for test_case in test_cases:
            print(f"   æµ‹è¯•: {test_case['name']}")

            inference_result = self.run_ollama_inference(
                test_case["prompt"],
                max_tokens=200,
                timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
            )

            # è¯„ä¼°ç»“æœ
            evaluation = self._evaluate_code_generation(
                inference_result,
                test_case["expected_keywords"]
            )

            results.append({
                "test_name": test_case["name"],
                "inference": inference_result,
                "evaluation": evaluation
            })

        return {
            "test_type": "code_generation",
            "results": results,
            "summary": self._summarize_results(results)
        }

    def run_mathematical_reasoning_test(self) -> Dict[str, Any]:
        """æ•°å­¦æ¨ç†æµ‹è¯•"""
        print("ğŸ§® è¿è¡Œæ•°å­¦æ¨ç†æµ‹è¯•...")

        test_cases = [
            {
                "name": "quadratic_equation",
                "prompt": "Solve the quadratic equation: 2xÂ² + 5x - 3 = 0. Show your work:",
                "expected_contains": ["discriminant", "sqrt", "Â±"]
            },
            {
                "name": "probability",
                "prompt": "If you roll two fair six-sided dice, what is the probability of getting a sum of 7?",
                "expected_contains": ["36", "6", "1/6"]
            },
            {
                "name": "geometry",
                "prompt": "Calculate the area of a circle with radius 5 units:",
                "expected_contains": ["25", "Ï€", "78.5"]
            }
        ]

        results = []
        for test_case in test_cases:
            print(f"   æµ‹è¯•: {test_case['name']}")

            inference_result = self.run_ollama_inference(
                test_case["prompt"],
                max_tokens=150,
                timeout=90
            )

            evaluation = self._evaluate_mathematical_reasoning(
                inference_result,
                test_case["expected_contains"]
            )

            results.append({
                "test_name": test_case["name"],
                "inference": inference_result,
                "evaluation": evaluation
            })

        return {
            "test_type": "mathematical_reasoning",
            "results": results,
            "summary": self._summarize_results(results)
        }

    def run_algorithmic_test(self) -> Dict[str, Any]:
        """ç®—æ³•æµ‹è¯•"""
        print("âš¡ è¿è¡Œç®—æ³•æµ‹è¯•...")

        test_cases = [
            {
                "name": "sorting_algorithm",
                "prompt": "Explain how quicksort works and provide a Python implementation:",
                "expected_keywords": ["pivot", "partition", "recursive", "def"]
            },
            {
                "name": "graph_traversal",
                "prompt": "Implement breadth-first search (BFS) for a graph in Python:",
                "expected_keywords": ["queue", "visited", "neighbors", "deque"]
            }
        ]

        results = []
        for test_case in test_cases:
            print(f"   æµ‹è¯•: {test_case['name']}")

            inference_result = self.run_ollama_inference(
                test_case["prompt"],
                max_tokens=250,
                timeout=150
            )

            evaluation = self._evaluate_algorithmic(
                inference_result,
                test_case["expected_keywords"]
            )

            results.append({
                "test_name": test_case["name"],
                "inference": inference_result,
                "evaluation": evaluation
            })

        return {
            "test_type": "algorithmic",
            "results": results,
            "summary": self._summarize_results(results)
        }

    def _evaluate_code_generation(self, inference_result: Dict, expected_keywords: List[str]) -> Dict[str, Any]:
        """è¯„ä¼°ä»£ç ç”Ÿæˆè´¨é‡"""
        if not inference_result["success"]:
            return {"score": 0, "reason": "inference_failed"}

        response = inference_result["response"].lower()

        # æ£€æŸ¥å…³é”®è¯
        found_keywords = sum(1 for keyword in expected_keywords if keyword.lower() in response)
        keyword_score = found_keywords / len(expected_keywords)

        # æ£€æŸ¥ä»£ç ç»“æ„
        has_function_def = "def " in response
        has_proper_indentation = "\n    " in response or "\n  " in response
        has_return = "return" in response

        structure_score = (has_function_def + has_proper_indentation + has_return) / 3

        # ç»¼åˆè¯„åˆ†
        total_score = (keyword_score + structure_score) / 2

        return {
            "score": total_score,
            "keyword_score": keyword_score,
            "structure_score": structure_score,
            "found_keywords": found_keywords,
            "total_keywords": len(expected_keywords)
        }

    def _evaluate_mathematical_reasoning(self, inference_result: Dict, expected_contains: List[str]) -> Dict[str, Any]:
        """è¯„ä¼°æ•°å­¦æ¨ç†è´¨é‡"""
        if not inference_result["success"]:
            return {"score": 0, "reason": "inference_failed"}

        response = inference_result["response"].lower()

        # æ£€æŸ¥é¢„æœŸå†…å®¹
        found_elements = sum(1 for element in expected_contains if element.lower() in response)
        content_score = found_elements / len(expected_contains)

        # æ£€æŸ¥æ¨ç†è¿‡ç¨‹
        has_steps = any(word in response for word in ["step", "first", "then", "finally", "therefore"])
        has_calculation = any(char in response for char in ["+", "-", "*", "/", "="])
        shows_work = has_steps or has_calculation

        reasoning_score = 1.0 if shows_work else 0.5

        total_score = (content_score + reasoning_score) / 2

        return {
            "score": total_score,
            "content_score": content_score,
            "reasoning_score": reasoning_score,
            "shows_work": shows_work
        }

    def _evaluate_algorithmic(self, inference_result: Dict, expected_keywords: List[str]) -> Dict[str, Any]:
        """è¯„ä¼°ç®—æ³•å®ç°è´¨é‡"""
        if not inference_result["success"]:
            return {"score": 0, "reason": "inference_failed"}

        response = inference_result["response"].lower()

        # æ£€æŸ¥å…³é”®è¯
        found_keywords = sum(1 for keyword in expected_keywords if keyword.lower() in response)
        keyword_score = found_keywords / len(expected_keywords)

        # æ£€æŸ¥ç®—æ³•è§£é‡Š
        has_explanation = any(word in response for word in ["algorithm", "works", "process", "step"])
        has_complexity = any(word in response for word in ["time", "space", "o(", "complexity"])
        has_implementation = "def " in response

        quality_score = (has_explanation + has_complexity + has_implementation) / 3

        total_score = (keyword_score + quality_score) / 2

        return {
            "score": total_score,
            "keyword_score": keyword_score,
            "quality_score": quality_score,
            "has_explanation": has_explanation,
            "has_complexity": has_complexity,
            "has_implementation": has_implementation
        }

    def _summarize_results(self, results: List[Dict]) -> Dict[str, Any]:
        """æ±‡æ€»æµ‹è¯•ç»“æœ"""
        if not results:
            return {"average_score": 0, "success_rate": 0}

        successful_tests = [r for r in results if r["inference"]["success"]]
        success_rate = len(successful_tests) / len(results)

        if successful_tests:
            avg_score = sum(r["evaluation"]["score"] for r in successful_tests) / len(successful_tests)
            avg_inference_time = sum(r["inference"]["inference_time"] for r in successful_tests) / len(successful_tests)
        else:
            avg_score = 0
            avg_inference_time = 0

        return {
            "total_tests": len(results),
            "successful_tests": len(successful_tests),
            "success_rate": success_rate,
            "average_score": avg_score,
            "average_inference_time": avg_inference_time
        }

    def run_full_benchmark_suite(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•å¥—ä»¶"""
        print("ğŸš€ å¼€å§‹DeepSeekåŸºå‡†æµ‹è¯•å¥—ä»¶")
        print("=" * 50)
        print(f"æ¨¡å‹: {self.model_name}")
        print(f"ç³»ç»Ÿ: {self.system_info['platform']} ({self.system_info['cpu_count']} CPUæ ¸å¿ƒ, {self.system_info['total_memory_gb']:.1f}GBå†…å­˜)")
        print()

        # è¿è¡Œå„é¡¹æµ‹è¯•
        benchmark_results = {
            "system_info": self.system_info,
            "model_name": self.model_name,
            "timestamp": time.time(),
            "tests": {}
        }

        test_suites = [
            ("code_generation", self.run_code_generation_test),
            ("mathematical_reasoning", self.run_mathematical_reasoning_test),
            ("algorithmic", self.run_algorithmic_test)
        ]

        for test_name, test_func in test_suites:
            try:
                print(f"\n{'='*20} {test_name.upper()} {'='*20}")
                result = test_func()
                benchmark_results["tests"][test_name] = result
                print(f"âœ… {test_name} æµ‹è¯•å®Œæˆ")
            except Exception as e:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥: {e}")
                benchmark_results["tests"][test_name] = {"error": str(e)}

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_final_report(benchmark_results)

        return benchmark_results

    def _generate_final_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š DEEPSEEKåŸºå‡†æµ‹è¯•æœ€ç»ˆæŠ¥å‘Š")
        print("="*60)

        print("ğŸ” æµ‹è¯•æ¦‚è§ˆ:")
        print(f"   æ¨¡å‹: {results['model_name']}")
        print(f"   ç³»ç»Ÿ: {results['system_info']['platform']}")
        print(f"   æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(results['timestamp']))}")
        print()

        # æ±‡æ€»å„æµ‹è¯•ç»“æœ
        total_tests = 0
        total_successful = 0
        total_avg_score = 0
        test_count = 0

        for test_name, test_result in results["tests"].items():
            if "error" in test_result:
                print(f"âŒ {test_name}: æµ‹è¯•å¤±è´¥ - {test_result['error']}")
                continue

            summary = test_result["summary"]
            total_tests += summary["total_tests"]
            total_successful += summary["successful_tests"]
            total_avg_score += summary["average_score"]
            test_count += 1

            print(f"âœ… {test_name}:")
            print(".1f")
            print(".1f")
            print(".2f")
            print()

        if test_count > 0:
            overall_success_rate = total_successful / total_tests if total_tests > 0 else 0
            overall_avg_score = total_avg_score / test_count

            print("ğŸ¯ æ€»ä½“è¡¨ç°:")
            print(".1f")
            print(".3f")
            print()

            # èƒ½åŠ›è¯„ä¼°
            self._assess_model_capability(overall_success_rate, overall_avg_score)

        print("ğŸ“‹ æŠ€æœ¯æŒ‡æ ‡:")
        print("   â€¢ æ¨¡å‹è§„æ¨¡: 236Bå‚æ•°")
        print("   â€¢ é‡åŒ–: Q4_0 (çº¦132GB)")
        print("   â€¢ æ¨ç†å¹³å°: Ollama + Apple Silicon")
        print("   â€¢ æµ‹è¯•ç¯å¢ƒ: 16GBå†…å­˜æ¶ˆè´¹çº§ç¡¬ä»¶")
        print()

        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")

    def _assess_model_capability(self, success_rate: float, avg_score: float):
        """è¯„ä¼°æ¨¡å‹èƒ½åŠ›æ°´å¹³"""
        print("ğŸ§  æ¨¡å‹èƒ½åŠ›è¯„ä¼°:")
        # åŸºäºæˆåŠŸç‡å’Œå¹³å‡åˆ†è¿›è¡Œè¯„ä¼°
        if success_rate >= 0.8 and avg_score >= 0.7:
            capability_level = "ä¼˜ç§€"
            description = "å±•ç°å‡ºè‰²çš„ç¼–ç¨‹å’Œæ¨ç†èƒ½åŠ›ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡"
        elif success_rate >= 0.6 and avg_score >= 0.5:
            capability_level = "è‰¯å¥½"
            description = "å…·å¤‡æ‰å®çš„åŸºç¡€èƒ½åŠ›å’Œä¸€å®šçš„æ¨ç†æ·±åº¦"
        elif success_rate >= 0.4 and avg_score >= 0.3:
            capability_level = "ä¸€èˆ¬"
            description = "åŸºæœ¬åŠŸèƒ½æ­£å¸¸ï¼Œä½†éœ€è¦æ”¹è¿›"
        else:
            capability_level = "å¾…æ”¹è¿›"
            description = "åŸºç¡€èƒ½åŠ›æœ‰é™ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–"

        print(f"   èƒ½åŠ›ç­‰çº§: {capability_level}")
        print(f"   è¯„ä¼°æè¿°: {description}")

        # å…·ä½“èƒ½åŠ›åˆ†æ
        print("   è¯¦ç»†èƒ½åŠ›:")
        if avg_score > 0.6:
            print("   â€¢ ä»£ç ç”Ÿæˆ: ä¼˜ç§€")
            print("   â€¢ æ•°å­¦æ¨ç†: è‰¯å¥½")
            print("   â€¢ ç®—æ³•ç†è§£: ä¼˜ç§€")
        elif avg_score > 0.4:
            print("   â€¢ ä»£ç ç”Ÿæˆ: è‰¯å¥½")
            print("   â€¢ æ•°å­¦æ¨ç†: ä¸€èˆ¬")
            print("   â€¢ ç®—æ³•ç†è§£: è‰¯å¥½")
        else:
            print("   â€¢ ä»£ç ç”Ÿæˆ: ä¸€èˆ¬")
            print("   â€¢ æ•°å­¦æ¨ç†: å¾…æ”¹è¿›")
            print("   â€¢ ç®—æ³•ç†è§£: ä¸€èˆ¬")


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºåŸºå‡†æµ‹è¯•å¥—ä»¶
        benchmark = DeepSeekBenchmarkSuite()

        # è¿è¡Œå®Œæ•´æµ‹è¯•
        results = benchmark.run_full_benchmark_suite()

        # ä¿å­˜ç»“æœ
        with open("deepseek_benchmark_results.json", "w") as f:
            json.dump(results, f, indent=2, default=str)

        print("\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: deepseek_benchmark_results.json")
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æµ‹è¯•ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()