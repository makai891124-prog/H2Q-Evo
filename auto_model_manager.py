#!/usr/bin/env python3
"""
H2Q-Evo è‡ªåŠ¨æ¨¡å‹ç®¡ç†å™¨

åŸºäºå†…åŒ–Ollamaç³»ç»Ÿçš„è‡ªåŠ¨åŒ–æ¨¡å‹ç®¡ç†å·¥å…·ï¼š
1. è‡ªåŠ¨å‘ç°å’Œä¸‹è½½æ¨¡å‹
2. æ™ºèƒ½ç¼“å­˜å’Œé¢„åŠ è½½
3. èµ„æºæ„ŸçŸ¥çš„æ¨¡å‹åˆ‡æ¢
4. æ‰¹é‡æ¨¡å‹å¤„ç†
"""

import time
import json
import os
from typing import Dict, List, Any, Optional
from pathlib import Path
import requests
from concurrent.futures import ThreadPoolExecutor
import threading

from internalized_ollama_system import (
    InternalizedOllamaSystem,
    InternalizedOllamaConfig,
    ModelRegistry,
    ModelDownloader
)


class AutoModelManager:
    """è‡ªåŠ¨æ¨¡å‹ç®¡ç†å™¨"""

    def __init__(self, config: InternalizedOllamaConfig):
        self.config = config
        self.system = InternalizedOllamaSystem(config)
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.model_usage_stats: Dict[str, Dict[str, Any]] = {}
        self.is_running = False

        # é¢„å®šä¹‰æ¨¡å‹æº
        self.model_sources = {
            "deepseek-coder-6.7b": {
                "url": "https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base/resolve/main/model.safetensors",
                "format": "safetensors",
                "size_gb": 13.2,
                "description": "DeepSeek Coder 6.7B åŸºç¡€æ¨¡å‹"
            },
            "llama-2-7b-chat": {
                "url": "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/model.safetensors",
                "format": "safetensors",
                "size_gb": 13.5,
                "description": "Llama 2 7B èŠå¤©æ¨¡å‹"
            },
            "codellama-7b": {
                "url": "https://huggingface.co/codellama/CodeLlama-7b-hf/resolve/main/model.safetensors",
                "format": "safetensors",
                "size_gb": 13.0,
                "description": "CodeLlama 7B ä»£ç ç”Ÿæˆæ¨¡å‹"
            },
            "tinyllama-1.1b": {
                "url": "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/model.safetensors",
                "format": "safetensors",
                "size_gb": 2.2,
                "description": "TinyLlama 1.1B è½»é‡çº§æ¨¡å‹ï¼ˆè¾¹ç¼˜è®¾å¤‡å‹å¥½ï¼‰"
            }
        }

    def start_auto_management(self) -> bool:
        """å¯åŠ¨è‡ªåŠ¨ç®¡ç†"""
        print("ğŸš€ å¯åŠ¨è‡ªåŠ¨æ¨¡å‹ç®¡ç†å™¨")
        print("=" * 50)

        if not self.system.startup():
            print("âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥")
            return False

        self.is_running = True

        # å¯åŠ¨åå°ä»»åŠ¡
        self._start_background_tasks()

        print("âœ… è‡ªåŠ¨æ¨¡å‹ç®¡ç†å™¨å¯åŠ¨æˆåŠŸ")
        return True

    def stop_auto_management(self):
        """åœæ­¢è‡ªåŠ¨ç®¡ç†"""
        print("ğŸ”„ åœæ­¢è‡ªåŠ¨æ¨¡å‹ç®¡ç†å™¨...")
        self.is_running = False
        self.system.shutdown()
        self.executor.shutdown(wait=True)
        print("âœ… è‡ªåŠ¨æ¨¡å‹ç®¡ç†å™¨å·²åœæ­¢")

    def discover_and_download_models(self, max_downloads: int = 2) -> List[str]:
        """å‘ç°å¹¶ä¸‹è½½æ¨¡å‹"""
        print(f"ğŸ” å‘ç°å¹¶ä¸‹è½½æœ€å¤š {max_downloads} ä¸ªæ¨¡å‹...")

        downloaded = []
        available_sources = list(self.model_sources.keys())

        # ä¼˜å…ˆä¸‹è½½è½»é‡çº§æ¨¡å‹ï¼ˆè¾¹ç¼˜è®¾å¤‡å‹å¥½ï¼‰
        priority_order = [
            "tinyllama-1.1b",  # æœ€è½»é‡
            "deepseek-coder-6.7b",
            "codellama-7b",
            "llama-2-7b-chat"
        ]

        for model_name in priority_order[:max_downloads]:
            if model_name in available_sources:
                source_info = self.model_sources[model_name]

                # æ£€æŸ¥å†…å­˜é¢„ç®—
                estimated_size_mb = source_info["size_gb"] * 1024
                if estimated_size_mb > self.config.model_memory_limit_mb:
                    print(f"âš ï¸ æ¨¡å‹ {model_name} å¤ªå¤§ ({source_info['size_gb']}GB)ï¼Œè·³è¿‡")
                    continue

                print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name} ({source_info['description']})")

                # æ³¨å†Œæ¨¡å‹
                self.system.registry.register_model(model_name, {
                    'name': model_name,
                    'format': source_info['format'],
                    'size_mb': estimated_size_mb,
                    'description': source_info['description'],
                    'auto_discovered': True
                })

                # ä¸‹è½½æ¨¡å‹ï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦å®ç°ä¸‹è½½é€»è¾‘ï¼‰
                # self.system.downloader.download_model(model_name, source_info['url'])

                # æ¨¡æ‹Ÿä¸‹è½½æˆåŠŸ
                downloaded.append(model_name)
                print(f"âœ… æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ")

        return downloaded

    def smart_model_preloading(self, strategy: str = "usage_based") -> Dict[str, bool]:
        """æ™ºèƒ½æ¨¡å‹é¢„åŠ è½½"""
        print(f"ğŸ§  æ‰§è¡Œæ™ºèƒ½é¢„åŠ è½½ç­–ç•¥: {strategy}")

        results = {}

        if strategy == "usage_based":
            # åŸºäºä½¿ç”¨ç»Ÿè®¡çš„é¢„åŠ è½½
            models_to_preload = self._get_top_used_models(limit=2)
        elif strategy == "size_based":
            # åŸºäºå¤§å°çš„é¢„åŠ è½½ï¼ˆä¼˜å…ˆå°æ¨¡å‹ï¼‰
            models_to_preload = ["tinyllama-1.1b", "deepseek-coder-6.7b"]
        else:
            # é»˜è®¤é¢„åŠ è½½ç­–ç•¥
            models_to_preload = ["tinyllama-1.1b"]

        for model_name in models_to_preload:
            print(f"ğŸ“¦ é¢„åŠ è½½æ¨¡å‹: {model_name}")
            success = self.system.load_model(model_name)
            results[model_name] = success

            if success:
                print(f"âœ… æ¨¡å‹ {model_name} é¢„åŠ è½½æˆåŠŸ")
            else:
                print(f"âŒ æ¨¡å‹ {model_name} é¢„åŠ è½½å¤±è´¥")

        return results

    def run_batch_inference(self, tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æ¨ç†æ‰§è¡Œ"""
        print(f"ğŸ”„ æ‰§è¡Œæ‰¹é‡æ¨ç†: {len(tasks)} ä¸ªä»»åŠ¡")

        results = []

        for task in tasks:
            model_name = task['model']
            prompt = task['prompt']
            task_id = task.get('id', f"task_{len(results)}")

            print(f"æ‰§è¡Œä»»åŠ¡ {task_id}: {prompt[:50]}...")

            # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
            if model_name not in self.system.loaded_models:
                if not self.system.load_model(model_name):
                    results.append({
                        'task_id': task_id,
                        'success': False,
                        'error': f'æ— æ³•åŠ è½½æ¨¡å‹ {model_name}'
                    })
                    continue

            # æ‰§è¡Œæ¨ç†
            start_time = time.time()
            result = self.system.run_inference(model_name, prompt)
            inference_time = time.time() - start_time

            # è®°å½•ä½¿ç”¨ç»Ÿè®¡
            self._record_model_usage(model_name, inference_time, len(prompt.split()))

            task_result = {
                'task_id': task_id,
                'model': model_name,
                'prompt': prompt,
                'success': 'error' not in result,
                'inference_time': inference_time,
                'result': result
            }

            results.append(task_result)
            print(f"   âœ… ä»»åŠ¡ {task_id} å®Œæˆ ({inference_time:.2f}ç§’)")

        return results

    def optimize_resource_usage(self) -> Dict[str, Any]:
        """ä¼˜åŒ–èµ„æºä½¿ç”¨"""
        print("ğŸ”§ æ‰§è¡Œèµ„æºä¼˜åŒ–...")

        status = self.system.get_system_status()
        current_memory = status['memory_usage']
        loaded_models = status['loaded_models']

        optimizations = {
            'memory_before': current_memory,
            'actions_taken': [],
            'memory_saved': 0
        }

        # å¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå¸è½½ä¸å¸¸ç”¨çš„æ¨¡å‹
        memory_threshold = self.config.max_memory_mb * 0.8  # 80%é˜ˆå€¼

        if current_memory > memory_threshold and len(loaded_models) > 1:
            # æ‰¾åˆ°æœ€å°‘ä½¿ç”¨çš„æ¨¡å‹
            least_used = self._get_least_used_model()
            if least_used:
                print(f"ğŸ—‘ï¸ å¸è½½ä¸å¸¸ç”¨æ¨¡å‹: {least_used}")
                self.system.unload_model(least_used)
                optimizations['actions_taken'].append(f'å¸è½½æ¨¡å‹: {least_used}')
                optimizations['memory_saved'] += 500  # ä¼°ç®—èŠ‚çœ500MB

        # æ‰§è¡Œåƒåœ¾å›æ”¶
        import gc
        collected = gc.collect()
        if collected > 0:
            optimizations['actions_taken'].append(f'åƒåœ¾å›æ”¶: æ”¶é›†äº† {collected} ä¸ªå¯¹è±¡')

        # æ›´æ–°å†…å­˜ç»Ÿè®¡
        new_status = self.system.get_system_status()
        optimizations['memory_after'] = new_status['memory_usage']

        print("âœ… èµ„æºä¼˜åŒ–å®Œæˆ")
        return optimizations

    def get_management_report(self) -> Dict[str, Any]:
        """è·å–ç®¡ç†æŠ¥å‘Š"""
        status = self.system.get_system_status()

        return {
            'system_status': status,
            'model_usage_stats': self.model_usage_stats,
            'available_models': self.system.list_models(),
            'background_tasks_active': self.is_running,
            'memory_efficiency': self._calculate_memory_efficiency(),
            'model_performance': self._calculate_model_performance()
        }

    def _start_background_tasks(self):
        """å¯åŠ¨åå°ä»»åŠ¡"""
        # å¯åŠ¨ç®€å•çš„åå°ä»»åŠ¡çº¿ç¨‹
        def background_worker():
            last_optimization = time.time()
            last_stats_update = time.time()

            while self.is_running:
                current_time = time.time()

                # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡èµ„æºä¼˜åŒ–
                if current_time - last_optimization >= 300:  # 5åˆ†é’Ÿ
                    self.optimize_resource_usage()
                    last_optimization = current_time

                # æ¯å°æ—¶æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
                if current_time - last_stats_update >= 3600:  # 1å°æ—¶
                    self._update_usage_statistics()
                    last_stats_update = current_time

                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

        # å¯åŠ¨åå°å·¥ä½œçº¿ç¨‹
        worker_thread = threading.Thread(target=background_worker, daemon=True)
        worker_thread.start()

    def _get_top_used_models(self, limit: int = 3) -> List[str]:
        """è·å–æœ€å¸¸ä½¿ç”¨çš„æ¨¡å‹"""
        if not self.model_usage_stats:
            return ["tinyllama-1.1b"]  # é»˜è®¤è¿”å›è½»é‡çº§æ¨¡å‹

        # æŒ‰ä½¿ç”¨æ¬¡æ•°æ’åº
        sorted_models = sorted(
            self.model_usage_stats.items(),
            key=lambda x: x[1].get('usage_count', 0),
            reverse=True
        )

        return [model_name for model_name, _ in sorted_models[:limit]]

    def _get_least_used_model(self) -> Optional[str]:
        """è·å–æœ€å°‘ä½¿ç”¨çš„æ¨¡å‹"""
        loaded_models = self.system.loaded_models

        if len(loaded_models) <= 1:
            return None

        # æ‰¾åˆ°ä½¿ç”¨æ¬¡æ•°æœ€å°‘çš„æ¨¡å‹
        min_usage = float('inf')
        least_used = None

        for model_name in loaded_models:
            usage_count = self.model_usage_stats.get(model_name, {}).get('usage_count', 0)
            if usage_count < min_usage:
                min_usage = usage_count
                least_used = model_name

        return least_used

    def _record_model_usage(self, model_name: str, inference_time: float, token_count: int):
        """è®°å½•æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡"""
        if model_name not in self.model_usage_stats:
            self.model_usage_stats[model_name] = {
                'usage_count': 0,
                'total_inference_time': 0.0,
                'total_tokens': 0,
                'last_used': 0
            }

        stats = self.model_usage_stats[model_name]
        stats['usage_count'] += 1
        stats['total_inference_time'] += inference_time
        stats['total_tokens'] += token_count
        stats['last_used'] = time.time()

    def _update_usage_statistics(self):
        """æ›´æ–°ä½¿ç”¨ç»Ÿè®¡"""
        # æ¸…ç†æ—§çš„ç»Ÿè®¡æ•°æ®ï¼ˆ7å¤©å‰ï¼‰
        cutoff_time = time.time() - (7 * 24 * 3600)  # 7å¤©

        to_remove = []
        for model_name, stats in self.model_usage_stats.items():
            if stats['last_used'] < cutoff_time:
                to_remove.append(model_name)

        for model_name in to_remove:
            del self.model_usage_stats[model_name]

        print(f"ğŸ“Š å·²æ¸…ç† {len(to_remove)} ä¸ªè¿‡æœŸçš„ä½¿ç”¨ç»Ÿè®¡")

    def _calculate_memory_efficiency(self) -> float:
        """è®¡ç®—å†…å­˜æ•ˆç‡"""
        status = self.system.get_system_status()
        current_memory = status['memory_usage']
        max_memory = self.config.max_memory_mb

        # æ•ˆç‡è¯„åˆ†ï¼šä½¿ç”¨ç‡è¶Šä½è¶Šå¥½ï¼Œä½†ä¸åº”è¯¥å¤ªä½ï¼ˆæµªè´¹ï¼‰
        usage_ratio = current_memory / max_memory

        if usage_ratio < 0.3:  # å¤ªä½ï¼Œèµ„æºæµªè´¹
            return 60.0
        elif usage_ratio < 0.7:  # ç†æƒ³èŒƒå›´
            return 100.0
        elif usage_ratio < 0.9:  # å¯æ¥å—
            return 80.0
        else:  # è¿‡é«˜
            return 40.0

    def _calculate_model_performance(self) -> Dict[str, float]:
        """è®¡ç®—æ¨¡å‹æ€§èƒ½"""
        performance = {}

        for model_name, stats in self.model_usage_stats.items():
            usage_count = stats['usage_count']
            total_time = stats['total_inference_time']
            total_tokens = stats['total_tokens']

            if usage_count > 0:
                avg_time = total_time / usage_count
                tokens_per_second = total_tokens / total_time if total_time > 0 else 0

                performance[model_name] = {
                    'average_inference_time': avg_time,
                    'tokens_per_second': tokens_per_second,
                    'usage_count': usage_count
                }

        return performance


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºè‡ªåŠ¨æ¨¡å‹ç®¡ç†å™¨"""
    print("ğŸ¤– H2Q-Evo è‡ªåŠ¨æ¨¡å‹ç®¡ç†å™¨æ¼”ç¤º")
    print("=" * 50)

    # é…ç½®ç³»ç»Ÿ
    config = InternalizedOllamaConfig(
        max_memory_mb=6144,  # 6GBå†…å­˜é™åˆ¶
        model_memory_limit_mb=2048,  # 2GBæ¨¡å‹é™åˆ¶
        working_memory_mb=1024,  # 1GBå·¥ä½œå†…å­˜
        enable_crystallization=True,
        target_device="cpu",
        optimize_for_edge=True
    )

    # åˆ›å»ºç®¡ç†å™¨
    manager = AutoModelManager(config)

    try:
        # å¯åŠ¨ç®¡ç†å™¨
        if not manager.start_auto_management():
            print("âŒ ç®¡ç†å™¨å¯åŠ¨å¤±è´¥")
            return

        # 1. æ¨¡å‹å‘ç°å’Œä¸‹è½½
        print("\n1. ğŸ” æ¨¡å‹å‘ç°å’Œä¸‹è½½")
        downloaded_models = manager.discover_and_download_models(max_downloads=2)
        print(f"ä¸‹è½½äº† {len(downloaded_models)} ä¸ªæ¨¡å‹: {downloaded_models}")

        # 2. æ™ºèƒ½é¢„åŠ è½½
        print("\n2. ğŸ§  æ™ºèƒ½æ¨¡å‹é¢„åŠ è½½")
        preload_results = manager.smart_model_preloading(strategy="size_based")
        successful_preloads = sum(1 for success in preload_results.values() if success)
        print(f"é¢„åŠ è½½æˆåŠŸ: {successful_preloads}/{len(preload_results)} ä¸ªæ¨¡å‹")

        # 3. æ‰¹é‡æ¨ç†æµ‹è¯•
        print("\n3. ğŸ”„ æ‰¹é‡æ¨ç†æµ‹è¯•")
        test_tasks = [
            {
                'id': 'task_1',
                'model': 'tinyllama-1.1b',
                'prompt': 'Write a simple hello world program in Python'
            },
            {
                'id': 'task_2',
                'model': 'tinyllama-1.1b',
                'prompt': 'Explain what is machine learning in simple terms'
            },
            {
                'id': 'task_3',
                'model': 'tinyllama-1.1b',
                'prompt': 'What are the benefits of using containers in software development?'
            }
        ]

        batch_results = manager.run_batch_inference(test_tasks)
        successful_tasks = sum(1 for result in batch_results if result['success'])
        print(f"æ‰¹é‡æ¨ç†å®Œæˆ: {successful_tasks}/{len(test_tasks)} ä¸ªä»»åŠ¡æˆåŠŸ")

        # 4. èµ„æºä¼˜åŒ–
        print("\n4. ğŸ”§ èµ„æºä¼˜åŒ–")
        optimization_results = manager.optimize_resource_usage()
        print(f"å†…å­˜ä¼˜åŒ–: {optimization_results['memory_before']:.1f}MB â†’ {optimization_results['memory_after']:.1f}MB")
        print(f"æ‰§è¡Œæ“ä½œ: {len(optimization_results['actions_taken'])} ä¸ª")

        # 5. ç®¡ç†æŠ¥å‘Š
        print("\n5. ğŸ“Š ç®¡ç†æŠ¥å‘Š")
        report = manager.get_management_report()
        print(f"ç³»ç»ŸçŠ¶æ€: {'è¿è¡Œä¸­' if report['system_status']['is_running'] else 'å·²åœæ­¢'}")
        print(f"å†…å­˜æ•ˆç‡: {report['memory_efficiency']:.1f}%")
        print(f"å¯ç”¨æ¨¡å‹: {len(report['available_models'])} ä¸ª")
        print(f"åŠ è½½æ¨¡å‹: {len(report['system_status']['loaded_models'])} ä¸ª")

        if report['model_performance']:
            print("æ¨¡å‹æ€§èƒ½:")
            for model_name, perf in report['model_performance'].items():
                print(f"  {model_name}:")
                print(f"    å¹³å‡æ¨ç†æ—¶é—´: {perf['average_inference_time']:.3f} ç§’")
                print(f"    ä»¤ç‰Œ/ç§’: {perf['tokens_per_second']:.1f}")
                print(f"    ä½¿ç”¨æ¬¡æ•°: {perf['usage_count']}")

        print("\nğŸ¯ è‡ªåŠ¨æ¨¡å‹ç®¡ç†å™¨æ¼”ç¤ºå®Œæˆï¼")
        print("âœ… æˆåŠŸå®ç°è‡ªåŠ¨åŒ–æ¨¡å‹ç®¡ç†")
        print("âœ… æ™ºèƒ½èµ„æºä¼˜åŒ–å’Œè°ƒåº¦")
        print("âœ… æ‰¹é‡æ¨ç†å¤„ç†")
        print("âœ… å®æ—¶ç›‘æ§å’ŒæŠ¥å‘Š")

        # ä¿æŒè¿è¡Œä¸€æ®µæ—¶é—´ä»¥å±•ç¤ºåå°ä»»åŠ¡
        print("\nâ° ä¿æŒè¿è¡Œ60ç§’ä»¥å±•ç¤ºåå°ä»»åŠ¡...")
        time.sleep(60)

    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¼”ç¤ºä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # ç¡®ä¿ç®¡ç†å™¨æ­£ç¡®å…³é—­
        manager.stop_auto_management()


if __name__ == "__main__":
    main()