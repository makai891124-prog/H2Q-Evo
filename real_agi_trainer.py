#!/usr/bin/env python3
"""
çœŸå®AGIè®­ç»ƒç³»ç»Ÿ - é›†æˆæ ‡å‡†æ•°æ®é›†
æ”¯æŒæ ‡å‡†æœºå™¨å­¦ä¹ åŸºå‡†ã€æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œäº¤å‰éªŒè¯
"""

import os
import sys
import json
import time
import torch
import torch.nn as nn
import torch.optim as optim
import logging
import psutil
import gc
import atexit
import math
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from torch.utils.data import DataLoader, Dataset
import torchvision
import torchvision.transforms as transforms

# å¯¼å…¥é«˜çº§è°±ç¨³å®šæ€§æ§åˆ¶å™¨
try:
    from advanced_spectral_controller import AdvancedSpectralController, RiemannSpectralLoss
    ADVANCED_SPECTRAL_AVAILABLE = True
except ImportError:
    ADVANCED_SPECTRAL_AVAILABLE = False
    print("è­¦å‘Š: é«˜çº§è°±ç¨³å®šæ€§æ§åˆ¶å™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿè°±ç§»è·Ÿè¸ªå™¨")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("real_agi_training.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("Real-AGI-Training")

class StandardDatasetLoader:
    """æ ‡å‡†æ•°æ®é›†åŠ è½½å™¨"""

    def __init__(self, dataset_name: str, batch_size: int = 32, device: str = "cpu"):
        self.dataset_name = dataset_name
        self.batch_size = batch_size
        self.device = device
        self.train_loader = None
        self.val_loader = None
        self.test_loader = None

    def load_dataset(self):
        """åŠ è½½æ ‡å‡†æ•°æ®é›†"""
        if self.dataset_name.lower() == "mnist":
            return self._load_mnist()
        elif self.dataset_name.lower() == "cifar10":
            return self._load_cifar10()
        elif self.dataset_name.lower() == "cifar100":
            return self._load_cifar100()
        elif self.dataset_name.lower() == "fashion_mnist":
            return self._load_fashion_mnist()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {self.dataset_name}")

    def _load_mnist(self):
        """åŠ è½½MNISTæ•°æ®é›†"""
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])

        train_dataset = torchvision.datasets.MNIST(
            root='./data', train=True, download=True, transform=transform
        )
        test_dataset = torchvision.datasets.MNIST(
            root='./data', train=False, download=True, transform=transform
        )

        # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
        train_size = int(0.8 * len(train_dataset))
        val_size = len(train_dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            train_dataset, [train_size, val_size]
        )

        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)

        return self.train_loader, self.val_loader, self.test_loader

    def _load_cifar10(self):
        """åŠ è½½CIFAR-10æ•°æ®é›†"""
        transform_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ])

        transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ])

        train_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=True, download=True, transform=transform_train
        )
        test_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True, transform=transform_test
        )

        # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
        train_size = int(0.8 * len(train_dataset))
        val_size = len(train_dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            train_dataset, [train_size, val_size]
        )

        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)

        return self.train_loader, self.val_loader, self.test_loader

    def _load_cifar100(self):
        """åŠ è½½CIFAR-100æ•°æ®é›†"""
        transform_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),
        ])

        transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),
        ])

        train_dataset = torchvision.datasets.CIFAR100(
            root='./data', train=True, download=True, transform=transform_train
        )
        test_dataset = torchvision.datasets.CIFAR100(
            root='./data', train=False, download=True, transform=transform_test
        )

        # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
        train_size = int(0.8 * len(train_dataset))
        val_size = len(train_dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            train_dataset, [train_size, val_size]
        )

        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)

        return self.train_loader, self.val_loader, self.test_loader

    def _load_fashion_mnist(self):
        """åŠ è½½Fashion-MNISTæ•°æ®é›†"""
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.2860,), (0.3530,))
        ])

        train_dataset = torchvision.datasets.FashionMNIST(
            root='./data', train=True, download=True, transform=transform
        )
        test_dataset = torchvision.datasets.FashionMNIST(
            root='./data', train=False, download=True, transform=transform
        )

        # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
        train_size = int(0.8 * len(train_dataset))
        val_size = len(train_dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            train_dataset, [train_size, val_size]
        )

        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)

        return self.train_loader, self.val_loader, self.test_loader

class BenchmarkModel(nn.Module):
    """åŸºå‡†æµ‹è¯•æ¨¡å‹ - æ”¯æŒæ ‡å‡†æ•°æ®é›†"""

    def __init__(self, dataset_name: str, num_classes: int = 10):
        super(BenchmarkModel, self).__init__()
        self.dataset_name = dataset_name

        if dataset_name.lower() in ['mnist', 'fashion_mnist']:
            # MNISTé£æ ¼çš„ç½‘ç»œ
            self.features = nn.Sequential(
                nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
            )
            self.classifier = nn.Sequential(
                nn.Linear(64 * 7 * 7, 128),
                nn.ReLU(),
                nn.Linear(128, num_classes)
            )
        else:
            # CIFARé£æ ¼çš„ç½‘ç»œ
            self.features = nn.Sequential(
                nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
            )
            self.classifier = nn.Sequential(
                nn.Linear(256 * 4 * 4, 512),
                nn.ReLU(),
                nn.Linear(512, num_classes)
            )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

class SpectralStabilityTracker:
    """è°±ç¨³å®šæ€§è·Ÿè¸ªå™¨"""

    def __init__(self):
        self.stability_history = []
        self.max_history = 100

    def update_stability(self, spectral_shift: float, loss: float) -> float:
        """æ›´æ–°è°±ç¨³å®šæ€§æŒ‡æ ‡"""
        # è®¡ç®—è°±ç¨³å®šæ€§åˆ†æ•°ï¼ˆåŸºäºè°±ç§»å’ŒæŸå¤±çš„ç›¸å…³æ€§ï¼‰
        stability_score = -abs(spectral_shift) * (1.0 / (1.0 + loss))

        self.stability_history.append({
            'spectral_shift': spectral_shift,
            'loss': loss,
            'stability_score': stability_score,
            'timestamp': time.time()
        })

        if len(self.stability_history) > self.max_history:
            self.stability_history = self.stability_history[-self.max_history:]

        return stability_score

    def get_correlation(self) -> float:
        """è®¡ç®—è°±ç¨³å®šæ€§ä¸å­¦ä¹ æ•ˆæœçš„ç›¸å…³æ€§"""
        if len(self.stability_history) < 10:
            return 0.0

        recent_data = self.stability_history[-50:]  # ä½¿ç”¨æœ€è¿‘50ä¸ªæ•°æ®ç‚¹
        spectral_shifts = [d['spectral_shift'] for d in recent_data]
        losses = [d['loss'] for d in recent_data]

        # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
        if len(set(spectral_shifts)) <= 1 or len(set(losses)) <= 1:
            return 0.0

        spectral_shifts = np.array(spectral_shifts)
        losses = np.array(losses)

        correlation = np.corrcoef(spectral_shifts, losses)[0, 1]
        return correlation

class RealAGITrainer:
    """çœŸå®AGIè®­ç»ƒå™¨ - é›†æˆæ ‡å‡†æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•"""

    def __init__(self, dataset_name: str = "cifar10", device: str = "cpu"):
        self.dataset_name = dataset_name
        self.device = device

        # æ•°æ®é›†é…ç½®
        self.dataset_config = {
            'mnist': {'num_classes': 10, 'input_shape': (1, 28, 28)},
            'fashion_mnist': {'num_classes': 10, 'input_shape': (1, 28, 28)},
            'cifar10': {'num_classes': 10, 'input_shape': (3, 32, 32)},
            'cifar100': {'num_classes': 100, 'input_shape': (3, 32, 32)}
        }

        # åˆå§‹åŒ–ç»„ä»¶
        self.dataset_loader = StandardDatasetLoader(dataset_name, device=device)
        self.train_loader, self.val_loader, self.test_loader = self.dataset_loader.load_dataset()

        # åˆ›å»ºæ¨¡å‹
        config = self.dataset_config[dataset_name.lower()]
        self.model = BenchmarkModel(dataset_name, config['num_classes']).to(device)

        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        self.criterion = nn.CrossEntropyLoss()

        # è°±ç¨³å®šæ€§æ§åˆ¶å™¨ - ç°åœ¨å¯ç”¨å¹¶é€‚é…æ ‡å‡†æ•°æ®é›†
        self.spectral_controller = None
        self.riemann_loss = None
        if ADVANCED_SPECTRAL_AVAILABLE:
            try:
                # ä¸ºä¸åŒæ•°æ®é›†åˆ›å»ºåˆé€‚ç»´åº¦çš„æ§åˆ¶å™¨
                if dataset_name.lower() in ['mnist', 'fashion_mnist']:
                    # MNISTç±»æ•°æ®é›†è¾“å‡ºç»´åº¦ä¸º10
                    self.spectral_controller = AdvancedSpectralController(dim=10)
                elif dataset_name.lower() == 'cifar10':
                    # CIFAR-10è¾“å‡ºç»´åº¦ä¸º10
                    self.spectral_controller = AdvancedSpectralController(dim=10)
                elif dataset_name.lower() == 'cifar100':
                    # CIFAR-100è¾“å‡ºç»´åº¦ä¸º100
                    self.spectral_controller = AdvancedSpectralController(dim=100)
                else:
                    # é»˜è®¤ç»´åº¦
                    self.spectral_controller = AdvancedSpectralController(dim=64)

                self.riemann_loss = RiemannSpectralLoss()
                logger.info(f"âœ… è°±ç¨³å®šæ€§æ§åˆ¶å™¨å·²å¯ç”¨ - ç»´åº¦: {self.spectral_controller.dim}")
            except Exception as e:
                logger.warning(f"è°±ç¨³å®šæ€§æ§åˆ¶å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.spectral_controller = None
        else:
            logger.info("â„¹ï¸ é«˜çº§è°±ç¨³å®šæ€§æ§åˆ¶å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿç¨³å®šæ€§è·Ÿè¸ª")

        # ç¨³å®šæ€§è·Ÿè¸ªå™¨
        self.stability_tracker = SpectralStabilityTracker()

        # è®­ç»ƒçŠ¶æ€
        self.step = 0
        self.best_accuracy = 0.0
        self.best_loss = float('inf')

        # åŸºå‡†æµ‹è¯•ç»“æœ
        self.benchmark_results = []

        logger.info(f"ğŸ¯ åˆå§‹åŒ–çœŸå®AGIè®­ç»ƒå™¨ - æ•°æ®é›†: {dataset_name}")

    def train_step(self) -> Dict[str, float]:
        """æ‰§è¡Œè®­ç»ƒæ­¥éª¤"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (inputs, targets) in enumerate(self.train_loader):
            inputs, targets = inputs.to(self.device), targets.to(self.device)

            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)

            # æ·»åŠ è°±ç¨³å®šæ€§æŸå¤±
            if self.spectral_controller is not None:
                try:
                    # è®¡ç®—è°±ç¨³å®šæ€§æŒ‡æ ‡
                    stability_score, stability_metrics = self.spectral_controller.compute_stability(outputs)
                    spectral_loss = self.riemann_loss(stability_metrics)
                    loss = loss + 0.1 * spectral_loss
                except Exception as e:
                    # å¦‚æœè°±ç¨³å®šæ€§è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ­£åˆ™åŒ–
                    logger.warning(f"è°±ç¨³å®šæ€§è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ: {e}")
                    spectral_loss = 0.01 * torch.norm(outputs, p=2)
                    loss = loss + spectral_loss

            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            if batch_idx >= 10:  # é™åˆ¶æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•°é‡
                break

        accuracy = 100. * correct / total
        avg_loss = total_loss / (batch_idx + 1)

        # è®¡ç®—è°±ç¨³å®šæ€§æŒ‡æ ‡
        spectral_shift = 0.0
        if self.spectral_controller is not None:
            try:
                stability_score, _ = self.spectral_controller.compute_stability(outputs)
                spectral_shift = stability_score
            except Exception as e:
                logger.warning(f"è°±ç¨³å®šæ€§æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
                spectral_shift = 0.0

        # æ›´æ–°ç¨³å®šæ€§è·Ÿè¸ª
        stability_score = self.stability_tracker.update_stability(spectral_shift, avg_loss)

        self.step += 1

        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            'spectral_shift': spectral_shift,
            'stability_score': stability_score
        }

    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, targets in self.val_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)

                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets)

                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

        accuracy = 100. * correct / total
        avg_loss = total_loss / len(self.val_loader)

        return {
            'val_loss': avg_loss,
            'val_accuracy': accuracy
        }

    def benchmark_test(self) -> Dict[str, float]:
        """åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•"""
        self.model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, targets in self.test_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)

                outputs = self.model(inputs)
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

        test_accuracy = 100. * correct / total

        # è®°å½•åŸºå‡†æµ‹è¯•ç»“æœ
        result = {
            'dataset': self.dataset_name,
            'test_accuracy': test_accuracy,
            'step': self.step,
            'timestamp': datetime.now().isoformat()
        }
        self.benchmark_results.append(result)

        return result

    def cross_validate_stability(self) -> Dict[str, float]:
        """äº¤å‰éªŒè¯è°±ç¨³å®šæ€§æŒ‡æ ‡ä¸å­¦ä¹ æ•ˆæœçš„ç›¸å…³æ€§"""
        correlation = self.stability_tracker.get_correlation()

        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡çš„é¢„æµ‹èƒ½åŠ›
        if len(self.stability_tracker.stability_history) >= 20:
            recent_data = self.stability_tracker.stability_history[-20:]
            stability_scores = [d['stability_score'] for d in recent_data]
            losses = [d['loss'] for d in recent_data]

            # è®¡ç®—ç¨³å®šæ€§åˆ†æ•°å¯¹æŸå¤±çš„é¢„æµ‹ç›¸å…³æ€§
            stability_correlation = np.corrcoef(stability_scores, losses)[0, 1]

            # è®¡ç®—ç¨³å®šæ€§æ”¹å–„è¶‹åŠ¿
            stability_trend = np.polyfit(range(len(stability_scores)), stability_scores, 1)[0]

            return {
                'spectral_loss_correlation': correlation,
                'stability_prediction_correlation': stability_correlation,
                'stability_trend': stability_trend,
                'validation_samples': len(recent_data)
            }

        return {
            'spectral_loss_correlation': correlation,
            'stability_prediction_correlation': 0.0,
            'stability_trend': 0.0,
            'validation_samples': len(self.stability_tracker.stability_history)
        }

    def save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = f"real_agi_checkpoint_{self.dataset_name}_{self.step}.pth"
        torch.save({
            'step': self.step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_accuracy': self.best_accuracy,
            'best_loss': self.best_loss,
            'benchmark_results': self.benchmark_results
        }, checkpoint_path)
        logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path)
            self.step = checkpoint['step']
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.best_accuracy = checkpoint.get('best_accuracy', 0.0)
            self.best_loss = checkpoint.get('best_loss', float('inf'))
            self.benchmark_results = checkpoint.get('benchmark_results', [])
            logger.info(f"âœ… æ£€æŸ¥ç‚¹å·²åŠ è½½: {checkpoint_path}")
            return True
        return False

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")

    # æ”¯æŒçš„æ•°æ®é›†
    supported_datasets = ['mnist', 'fashion_mnist', 'cifar10', 'cifar100']

    # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºè®­ç»ƒå™¨å¹¶è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_summary = {}

    for dataset_name in supported_datasets:
        try:
            logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒæ•°æ®é›†: {dataset_name}")

            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = RealAGITrainer(dataset_name=dataset_name, device=device)

            # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint_path = f"real_agi_checkpoint_{dataset_name}_latest.pth"
            trainer.load_checkpoint(checkpoint_path)

            # è®­ç»ƒå¾ªç¯
            for epoch in range(5):  # æ¯ä¸ªæ•°æ®é›†è®­ç»ƒ5ä¸ªepoch
                logger.info(f"ğŸ“ˆ Epoch {epoch + 1}/5 - æ•°æ®é›†: {dataset_name}")

                # è®­ç»ƒæ­¥éª¤
                train_metrics = trainer.train_step()
                val_metrics = trainer.validate()

                logger.info(f"   è®­ç»ƒæŸå¤±: {train_metrics['loss']:.4f}")
                logger.info(f"   è®­ç»ƒå‡†ç¡®ç‡: {train_metrics['accuracy']:.2f}%")
                logger.info(f"   éªŒè¯æŸå¤±: {val_metrics['val_loss']:.4f}")
                logger.info(f"   éªŒè¯å‡†ç¡®ç‡: {val_metrics['val_accuracy']:.2f}%")
                logger.info(f"   è°±ç§»Î·å®éƒ¨: {train_metrics['spectral_shift']:.4f}")

                # æ›´æ–°æœ€ä½³æ€§èƒ½
                if val_metrics['val_accuracy'] > trainer.best_accuracy:
                    trainer.best_accuracy = val_metrics['val_accuracy']
                    trainer.best_loss = val_metrics['val_loss']

                # æ¯10æ­¥è¿›è¡Œä¸€æ¬¡äº¤å‰éªŒè¯
                if trainer.step % 10 == 0:
                    stability_metrics = trainer.cross_validate_stability()
                    logger.info(f"   è°±ç¨³å®šæ€§ç›¸å…³æ€§: {stability_metrics['spectral_loss_correlation']:.4f}")
                    logger.info(f"   ç¨³å®šæ€§é¢„æµ‹ç›¸å…³æ€§: {stability_metrics['stability_prediction_correlation']:.4f}")

            # æœ€ç»ˆåŸºå‡†æµ‹è¯•
            benchmark_result = trainer.benchmark_test()
            logger.info(f"ğŸ¯ {dataset_name} æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {benchmark_result['test_accuracy']:.2f}%")

            # ä¿å­˜ç»“æœ
            trainer.save_checkpoint()
            benchmark_summary[dataset_name] = benchmark_result

            # å†…å­˜æ¸…ç†
            del trainer
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒæ•°æ®é›† {dataset_name} å¤±è´¥: {e}")
            continue

    # ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š
    generate_benchmark_report(benchmark_summary)

def generate_benchmark_report(results: Dict[str, Dict]):
    """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
    report_path = "real_agi_benchmark_report.json"

    # è®¡ç®—å¹³å‡æ€§èƒ½
    accuracies = [result['test_accuracy'] for result in results.values() if 'test_accuracy' in result]
    avg_accuracy = np.mean(accuracies) if accuracies else 0.0

    # æ‰¾åˆ°æœ€ä½³è¡¨ç°çš„æ•°æ®é›†
    best_dataset = None
    if results:
        best_dataset = max(results.keys(), key=lambda k: results[k].get('test_accuracy', 0))
    else:
        best_dataset = "none"

    report = {
        'timestamp': datetime.now().isoformat(),
        'summary': {
            'datasets_tested': len(results),
            'average_accuracy': avg_accuracy,
            'best_performing_dataset': best_dataset
        },
        'detailed_results': results,
        'comparison_with_baselines': {
            'mnist': {
                'h2q_evo_accuracy': results.get('mnist', {}).get('test_accuracy', 0),
                'baseline_cnn': 99.2,  # å…¸å‹CNNåŸºå‡†
                'baseline_resnet': 99.6,  # ResNetåŸºå‡†
                'improvement_over_cnn': results.get('mnist', {}).get('test_accuracy', 0) - 99.2
            },
            'cifar10': {
                'h2q_evo_accuracy': results.get('cifar10', {}).get('test_accuracy', 0),
                'baseline_cnn': 78.5,  # å…¸å‹CNNåŸºå‡†
                'baseline_resnet': 92.1,  # ResNetåŸºå‡†
                'improvement_over_cnn': results.get('cifar10', {}).get('test_accuracy', 0) - 78.5
            }
        }
    }

    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)

    logger.info(f"ğŸ“Š åŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    logger.info(f"ğŸ“ˆ å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.2f}%")

if __name__ == "__main__":
    main()