#!/usr/bin/env python3
"""
H2Q-Evo AGI è‡ªç›‘ç£è¿›åŒ–è®­ç»ƒç³»ç»Ÿ
ä½¿ç”¨DeepSeekæ¨¡å‹æƒé‡è¿›è¡Œ7*24å°æ—¶AGIæ ¸å¿ƒæœºèƒ½åŠ›è®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import json
import os
import sys
import time
import threading
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import requests
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

sys.path.append('/Users/imymm/H2Q-Evo')

from hierarchical_concept_encoder import HierarchicalConceptEncoder
from final_integration_system import FinalIntegratedSystem, FinalIntegrationConfig


class DeepSeekEnhancedAGITrainer:
    """ä½¿ç”¨DeepSeekå¢å¼ºçš„AGIè®­ç»ƒå™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆå§‹åŒ–ç»„ä»¶
        self.hierarchical_encoder = HierarchicalConceptEncoder()
        self.deepseek_models = self._init_deepseek_models()
        self.local_agi_core = self._init_local_agi_core()

        # è®­ç»ƒçŠ¶æ€
        self.training_stats = {
            'epochs_completed': 0,
            'total_samples_processed': 0,
            'capability_improvements': {},
            'benchmark_scores': {},
            'evolution_cycles': 0
        }

        # 7*24å°æ—¶è¿›åŒ–æ§åˆ¶
        self.evolution_active = False
        self.evolution_thread = None
        self.checkpoint_interval = 3600  # 1å°æ—¶æ£€æŸ¥ç‚¹
        self.benchmark_interval = 7200  # 2å°æ—¶åŸºå‡†æµ‹è¯•

        # æ—¥å¿—ç³»ç»Ÿ
        self._setup_logging()

    def _init_deepseek_models(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–DeepSeekæ¨¡å‹"""
        models = {}

        try:
            # æ£€æŸ¥Ollamaä¸­çš„DeepSeekæ¨¡å‹
            result = subprocess.run(['ollama', 'list'],
                                  capture_output=True, text=True, timeout=10)

            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # è·³è¿‡æ ‡é¢˜è¡Œ
                available_models = []
                for line in lines:
                    if 'deepseek' in line.lower():
                        parts = line.split()
                        if len(parts) >= 1:
                            model_name = parts[0]
                            available_models.append(model_name)

                # æ™ºèƒ½é€‰æ‹©æ‰€æœ‰å¯ç”¨æ¨¡å‹
                model_configs = {
                    'fast': None,      # 6.7b - æœ€å¿«ï¼Œé€‚åˆç®€å•ä»»åŠ¡
                    'balanced': None,  # 33b - å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦
                    'powerful': None,  # 236b - æœ€å¼ºï¼Œé€‚åˆå¤æ‚ä»»åŠ¡
                    'math': None       # ä¸“é—¨ç”¨äºæ•°å­¦æ¨ç†
                }

                # æ ¹æ®æ¨¡å‹å¤§å°åˆ†é…è§’è‰²
                for model in available_models:
                    if '6.7b' in model:
                        model_configs['fast'] = model
                        if not model_configs['math']:
                            model_configs['math'] = model  # é»˜è®¤æ•°å­¦æ¨¡å‹
                    elif '33b' in model:
                        model_configs['balanced'] = model
                        model_configs['math'] = model  # 33bæ›´é€‚åˆæ•°å­¦
                    elif '236b' in model:
                        model_configs['powerful'] = model
                        model_configs['math'] = model  # 236bæœ€å¼ºæ•°å­¦èƒ½åŠ›

                models.update(model_configs)
                print(f"ğŸ¤– DeepSeekæ¨¡å‹é…ç½®: {models}")
                print(f"ğŸ“Š å¯ç”¨æ¨¡å‹æ•°é‡: {len([m for m in model_configs.values() if m is not None])}")

            print(f"âœ… å‘ç°DeepSeekæ¨¡å‹: {models}")

        except Exception as e:
            print(f"âš ï¸ DeepSeekæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

        return models

    def _init_local_agi_core(self) -> nn.Module:
        """åˆå§‹åŒ–æœ¬åœ°AGIæ ¸å¿ƒæœº"""
        config = FinalIntegrationConfig(
            model_compression_ratio=46.0,
            enable_mathematical_core=True,
            device=self.device
        )

        system = FinalIntegratedSystem(config)

        # åŠ è½½ç°æœ‰æƒé‡
        weight_paths = [
            "/Users/imymm/H2Q-Evo/h2q_project/h2q_full_l1.pth",
            "/Users/imymm/H2Q-Evo/h2q_project/h2q_model_hierarchy.pth"
        ]

        for weight_path in weight_paths:
            if os.path.exists(weight_path):
                if system.initialize_from_236b_weights(weight_path):
                    print(f"âœ… åŠ è½½æœ¬åœ°AGIæƒé‡: {weight_path}")
                    break

        return system

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿï¼ŒåŒ…å«å­˜å‚¨å®‰å…¨æœºåˆ¶"""
        import logging.handlers

        # è®¾ç½®æ—¥å¿—è½®è½¬ï¼Œæœ€å¤§10MBï¼Œä¿ç•™3ä¸ªå¤‡ä»½æ–‡ä»¶
        log_handler = logging.handlers.RotatingFileHandler(
            '/Users/imymm/H2Q-Evo/agi_evolution_training.log',
            maxBytes=10*1024*1024,  # 10MB
            backupCount=3
        )
        log_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))

        self.logger = logging.getLogger('AGI_Evolution')
        self.logger.setLevel(logging.INFO)
        self.logger.addHandler(log_handler)

        # å¯åŠ¨å­˜å‚¨ç›‘æ§å’Œæ¸…ç†
        self._cleanup_storage()

    def _cleanup_storage(self):
        """æ¸…ç†å­˜å‚¨ç©ºé—´ï¼Œç¡®ä¿æœ¬åœ°å­˜å‚¨å®‰å…¨"""
        try:
            import shutil
            import os
            from pathlib import Path

            project_root = Path('/Users/imymm/H2Q-Evo')

            # 1. æ¸…ç†æ—§çš„è®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œåªä¿ç•™æœ€æ–°çš„3ä¸ª
            checkpoint_dir = project_root / 'training_checkpoints'
            if checkpoint_dir.exists():
                checkpoints = sorted(checkpoint_dir.glob('*.pth'), key=lambda x: x.stat().st_mtime, reverse=True)
                if len(checkpoints) > 3:
                    for old_checkpoint in checkpoints[3:]:
                        try:
                            old_checkpoint.unlink()
                            self.logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {old_checkpoint.name}")
                        except Exception as e:
                            self.logger.warning(f"åˆ é™¤æ£€æŸ¥ç‚¹å¤±è´¥ {old_checkpoint}: {e}")

            # 2. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_dirs = ['temp_sandbox', 'tmp', 'temp']
            for temp_dir in temp_dirs:
                temp_path = project_root / temp_dir
                if temp_path.exists() and temp_path.is_dir():
                    try:
                        # åªåˆ é™¤è¶…è¿‡24å°æ—¶çš„æ–‡ä»¶
                        import time
                        current_time = time.time()
                        for file_path in temp_path.rglob('*'):
                            if file_path.is_file() and (current_time - file_path.stat().st_mtime) > 24*3600:
                                file_path.unlink()
                                self.logger.info(f"ğŸ—‘ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file_path}")
                    except Exception as e:
                        self.logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

            # 3. æ¸…ç†ç¼“å­˜ç›®å½•
            cache_dirs = ['__pycache__', '.pytest_cache', 'htmlcov']
            for cache_dir in cache_dirs:
                cache_path = project_root / cache_dir
                if cache_path.exists():
                    try:
                        shutil.rmtree(cache_path)
                        self.logger.info(f"ğŸ—‘ï¸ åˆ é™¤ç¼“å­˜ç›®å½•: {cache_dir}")
                    except Exception as e:
                        self.logger.warning(f"åˆ é™¤ç¼“å­˜ç›®å½•å¤±è´¥ {cache_dir}: {e}")

            # 4. æ£€æŸ¥ç£ç›˜ä½¿ç”¨æƒ…å†µ
            stat = shutil.disk_usage(project_root)
            usage_percent = (stat.used / stat.total) * 100
            if usage_percent > 90:
                self.logger.warning(f"âš ï¸ ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {usage_percent:.1f}%")
            elif usage_percent > 80:
                self.logger.info(f"ğŸ“Š ç£ç›˜ä½¿ç”¨ç‡: {usage_percent:.1f}%")
            # 5. æ¸…ç†è¿‡æœŸçš„æ—¥å¿—æ–‡ä»¶ï¼ˆä¿ç•™7å¤©å†…çš„ï¼‰
            import time
            log_files = list(project_root.glob('*.log'))
            current_time = time.time()
            for log_file in log_files:
                if (current_time - log_file.stat().st_mtime) > 7*24*3600:  # 7å¤©
                    try:
                        log_file.unlink()
                        self.logger.info(f"ğŸ—‘ï¸ åˆ é™¤è¿‡æœŸæ—¥å¿—: {log_file.name}")
                    except Exception as e:
                        self.logger.warning(f"åˆ é™¤æ—¥å¿—å¤±è´¥ {log_file}: {e}")

            self.logger.info("âœ… å­˜å‚¨æ¸…ç†å®Œæˆ")

            # åŒæ—¶é™åˆ¶è®­ç»ƒæ•°æ®å¤§å°
            self._limit_training_data_size()

        except Exception as e:
            self.logger.error(f"å­˜å‚¨æ¸…ç†å¤±è´¥: {e}")

    def _monitor_memory_usage(self):
        """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            import psutil
            import gc

            # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory = psutil.virtual_memory()
            memory_percent = memory.percent

            if memory_percent > 90:
                self.logger.warning(f"âš ï¸ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_percent:.1f}%")
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()
                self.logger.info("ğŸ—‘ï¸ æ‰§è¡Œåƒåœ¾å›æ”¶")

                # å¦‚æœå†…å­˜ä»ç„¶å¾ˆé«˜ï¼Œæ¸…ç†ä¸€äº›ç¼“å­˜
                if psutil.virtual_memory().percent > 85:
                    self._cleanup_storage()

            elif memory_percent > 80:
                self.logger.info(f"ğŸ“Š å†…å­˜ä½¿ç”¨ç‡: {memory_percent:.1f}%")
        except Exception as e:
            self.logger.warning(f"å†…å­˜ç›‘æ§å¤±è´¥: {e}")

    def _limit_training_data_size(self):
        """é™åˆ¶è®­ç»ƒæ•°æ®å¤§å°ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º"""
        try:
            # é™åˆ¶evo_state.jsonæ–‡ä»¶å¤§å°ï¼ˆæœ€å¤§100MBï¼‰
            evo_state_path = Path('/Users/imymm/H2Q-Evo/evo_state.json')
            if evo_state_path.exists() and evo_state_path.stat().st_size > 100*1024*1024:  # 100MB
                self.logger.warning("evo_state.jsonæ–‡ä»¶è¿‡å¤§ï¼Œå¼€å§‹æ¸…ç†æ—§æ•°æ®")

                # è¯»å–å¹¶æ¸…ç†æ—§çš„todo_listå’Œhistory
                with open(evo_state_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # åªä¿ç•™æœ€è¿‘çš„1000ä¸ªtodoé¡¹ç›®
                if 'todo_list' in data and len(data['todo_list']) > 1000:
                    data['todo_list'] = data['todo_list'][-1000:]
                    self.logger.info("ğŸ—‘ï¸ æ¸…ç†todo_listï¼Œä¿ç•™æœ€è¿‘1000é¡¹")

                # åªä¿ç•™æœ€è¿‘çš„1000ä¸ªå†å²è®°å½•
                if 'history' in data and len(data['history']) > 1000:
                    data['history'] = data['history'][-1000:]
                    self.logger.info("ğŸ—‘ï¸ æ¸…ç†historyï¼Œä¿ç•™æœ€è¿‘1000é¡¹")

                # ä¿å­˜æ¸…ç†åçš„æ•°æ®
                with open(evo_state_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)

            # é™åˆ¶project_memory.jsonæ–‡ä»¶å¤§å°ï¼ˆæœ€å¤§50MBï¼‰
            memory_path = Path('/Users/imymm/H2Q-Evo/project_memory.json')
            if memory_path.exists() and memory_path.stat().st_size > 50*1024*1024:  # 50MB
                self.logger.warning("project_memory.jsonæ–‡ä»¶è¿‡å¤§ï¼Œé‡ç½®ä¸ºç©º")

                # é‡ç½®ä¸ºåŸºæœ¬ç»“æ„
                basic_memory = {
                    "version": "1.0",
                    "last_updated": time.time(),
                    "memory": {},
                    "patterns": {},
                    "insights": []
                }

                with open(memory_path, 'w', encoding='utf-8') as f:
                    json.dump(basic_memory, f, indent=2, ensure_ascii=False)

                self.logger.info("ğŸ—‘ï¸ é‡ç½®project_memory.json")

        except Exception as e:
            self.logger.error(f"è®­ç»ƒæ•°æ®å¤§å°é™åˆ¶å¤±è´¥: {e}")

    def start_24_7_evolution(self):
        """å¯åŠ¨7*24å°æ—¶AGIè¿›åŒ–"""
        if self.evolution_active:
            print("âš ï¸ è¿›åŒ–å·²åœ¨è¿è¡Œä¸­")
            return

        self.evolution_active = True
        self.evolution_thread = threading.Thread(target=self._evolution_loop)
        self.evolution_thread.daemon = True
        self.evolution_thread.start()

        print("ğŸš€ AGIè¿›åŒ–ç³»ç»Ÿå·²å¯åŠ¨ - 7*24å°æ—¶æŒç»­è¿è¡Œ")
        self.logger.info("AGIè¿›åŒ–ç³»ç»Ÿå¯åŠ¨")

    def stop_evolution(self):
        """åœæ­¢è¿›åŒ–"""
        self.evolution_active = False
        if self.evolution_thread:
            self.evolution_thread.join(timeout=30)

        print("ğŸ›‘ AGIè¿›åŒ–ç³»ç»Ÿå·²åœæ­¢")
        self.logger.info("AGIè¿›åŒ–ç³»ç»Ÿåœæ­¢")

    def _evolution_loop(self):
        """è¿›åŒ–ä¸»å¾ªç¯"""
        last_checkpoint = time.time()
        last_benchmark = time.time()

        while self.evolution_active:
            try:
                current_time = time.time()

                # æ‰§è¡Œè®­ç»ƒå‘¨æœŸ
                self._execute_training_cycle()

                # æ£€æŸ¥ç‚¹ä¿å­˜
                if current_time - last_checkpoint >= self.checkpoint_interval:
                    self._save_checkpoint()
                    last_checkpoint = current_time

                # åŸºå‡†æµ‹è¯•
                if current_time - last_benchmark >= self.benchmark_interval:
                    self._run_benchmark_cycle()
                    last_benchmark = current_time

                # çŸ­æš‚ä¼‘çœ é¿å…CPUå ç”¨è¿‡é«˜
                time.sleep(60)  # 1åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

            except Exception as e:
                self.logger.error(f"è¿›åŒ–å¾ªç¯é”™è¯¯: {e}")
                time.sleep(300)  # é”™è¯¯åç­‰å¾…5åˆ†é’Ÿ

    def _execute_training_cycle(self):
        """æ‰§è¡Œè®­ç»ƒå‘¨æœŸ"""
        print(f"\nğŸ”„ æ‰§è¡Œè®­ç»ƒå‘¨æœŸ #{self.training_stats['evolution_cycles'] + 1}")

        # æ¯10ä¸ªè®­ç»ƒå‘¨æœŸæ¸…ç†ä¸€æ¬¡å­˜å‚¨
        if self.training_stats['evolution_cycles'] % 10 == 0:
            self._cleanup_storage()
            self._monitor_memory_usage()

        # ä»DeepSeekæ¨¡å‹ç”Ÿæˆè®­ç»ƒæ•°æ®
        training_data = self._generate_training_data_from_deepseek()

        # ä½¿ç”¨è®­ç»ƒæ•°æ®æ”¹è¿›æœ¬åœ°AGIæ ¸å¿ƒ
        improvements = self._train_local_agi_core(training_data)

        # æ›´æ–°ç»Ÿè®¡
        self.training_stats['evolution_cycles'] += 1
        self.training_stats['total_samples_processed'] += len(training_data)

        for capability, improvement in improvements.items():
            if capability not in self.training_stats['capability_improvements']:
                self.training_stats['capability_improvements'][capability] = []
            self.training_stats['capability_improvements'][capability].append(improvement)

        self.logger.info(f"è®­ç»ƒå‘¨æœŸå®Œæˆ - æ”¹è¿›: {improvements}")

    def _generate_training_data_from_deepseek(self) -> List[Dict[str, Any]]:
        """ä»DeepSeekæ¨¡å‹ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        training_data = []

        # ç”Ÿæˆä¸åŒç±»å‹çš„è®­ç»ƒæ ·æœ¬
        sample_types = [
            'code_generation',
            'text_understanding',
            'mathematical_reasoning',
            'concept_analysis'
        ]

        for sample_type in sample_types:
            samples = self._generate_samples_by_type(sample_type)
            training_data.extend(samples)

        return training_data

    def _generate_samples_by_type(self, sample_type: str) -> List[Dict[str, Any]]:
        """æŒ‰ç±»å‹ç”Ÿæˆè®­ç»ƒæ ·æœ¬"""
        samples = []

        prompts = {
            'code_generation': [
                "Write a Python function to calculate fibonacci numbers",
                "Create a class for a simple calculator",
                "Implement a binary search algorithm"
            ],
            'text_understanding': [
                "Explain the concept of machine learning",
                "What is artificial intelligence?",
                "Describe how neural networks work"
            ],
            'mathematical_reasoning': [
                "Solve: 2x + 3 = 7",
                "Calculate the area of a circle with radius 5",
                "What is the derivative of x^2?"
            ],
            'concept_analysis': [
                "Analyze the relationship between AI and machine learning",
                "Compare supervised and unsupervised learning",
                "Explain the importance of data in AI systems"
            ]
        }

        for prompt in prompts.get(sample_type, []):
            try:
                # ä½¿ç”¨DeepSeekç”Ÿæˆé«˜è´¨é‡æ ·æœ¬
                deepseek_output = self._query_deepseek_model(prompt, sample_type)

                if deepseek_output:
                    sample = {
                        'type': sample_type,
                        'input': prompt,
                        'target_output': deepseek_output,
                        'timestamp': datetime.now().isoformat()
                    }
                    samples.append(sample)

            except Exception as e:
                self.logger.warning(f"ç”Ÿæˆæ ·æœ¬å¤±è´¥ {sample_type}: {e}")

        return samples

    def _query_deepseek_model(self, prompt: str, sample_type: str) -> Optional[str]:
        """æŸ¥è¯¢DeepSeekæ¨¡å‹ - æ™ºèƒ½é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹"""
        # æ ¹æ®ä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦æ™ºèƒ½é€‰æ‹©æ¨¡å‹
        if sample_type == 'mathematical_reasoning':
            # æ•°å­¦æ¨ç†ï¼šä¼˜å…ˆä½¿ç”¨å¼ºå¤§çš„æ¨¡å‹
            model_name = (self.deepseek_models.get('math') or
                         self.deepseek_models.get('powerful') or
                         self.deepseek_models.get('balanced') or
                         self.deepseek_models.get('fast'))
            if not model_name:
                raise RuntimeError("æ•°å­¦æ¨ç†éœ€è¦çœŸå®çš„DeepSeekæ¨¡å‹ï¼Œä½†æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹ã€‚è¯·ç¡®ä¿å·²ä¸‹è½½DeepSeekæ¨¡å‹ã€‚")
        elif sample_type in ['code_generation', 'code']:
            # ä»£ç ç”Ÿæˆï¼šä½¿ç”¨å¹³è¡¡æˆ–å¿«é€Ÿæ¨¡å‹
            model_name = (self.deepseek_models.get('balanced') or
                         self.deepseek_models.get('fast') or
                         self.deepseek_models.get('powerful'))
        elif sample_type in ['text_understanding', 'conversation', 'creative_writing']:
            # æ–‡æœ¬ä»»åŠ¡ï¼šå¯ä»¥ä½¿ç”¨è¾ƒå¿«çš„æ¨¡å‹
            model_name = (self.deepseek_models.get('fast') or
                         self.deepseek_models.get('balanced') or
                         self.deepseek_models.get('powerful'))
        elif sample_type == 'concept_analysis':
            # æ¦‚å¿µåˆ†æï¼šéœ€è¦å¼ºå¤§çš„æ¨ç†èƒ½åŠ›
            model_name = (self.deepseek_models.get('powerful') or
                         self.deepseek_models.get('balanced') or
                         self.deepseek_models.get('fast'))
        else:
            # é»˜è®¤ä½¿ç”¨å¹³è¡¡æ¨¡å‹
            model_name = (self.deepseek_models.get('balanced') or
                         self.deepseek_models.get('fast') or
                         self.deepseek_models.get('powerful'))

        if not model_name:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æ¨¡å‹ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè¾“å‡ºï¼ˆé™¤äº†æ•°å­¦æ¨ç†ï¼‰
            if sample_type == 'mathematical_reasoning':
                raise RuntimeError(f"ä»»åŠ¡ç±»å‹ '{sample_type}' éœ€è¦çœŸå®çš„DeepSeekæ¨¡å‹ï¼Œä½†æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹ã€‚")
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°åˆé€‚çš„DeepSeekæ¨¡å‹ç”¨äº {sample_type}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè¾“å‡º")
                return self._generate_simulated_output(prompt, sample_type)

        # è®°å½•ä½¿ç”¨çš„æ¨¡å‹
        self.logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ {model_name} å¤„ç† {sample_type} ä»»åŠ¡")

        try:
            # æ™ºèƒ½è¶…æ—¶æ§åˆ¶ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹å’Œæ¨¡å‹æ€§èƒ½åŠ¨æ€è°ƒæ•´
            base_timeout = self._calculate_dynamic_timeout(model_name, sample_type, prompt)

            # ä½¿ç”¨Ollama APIæŸ¥è¯¢DeepSeekæ¨¡å‹ï¼Œå¸¦é‡è¯•æœºåˆ¶
            max_retries = 2
            for attempt in range(max_retries + 1):
                try:
                    timeout = base_timeout * (0.8 ** attempt)  # æ¯æ¬¡é‡è¯•å‡å°‘20%è¶…æ—¶æ—¶é—´

                    result = subprocess.run([
                        'ollama', 'run', model_name, prompt
                    ], capture_output=True, text=True, timeout=int(timeout))

                    if result.returncode == 0 and result.stdout.strip():
                        response = result.stdout.strip()
                        self.logger.info(f"âœ… æ¨¡å‹ {model_name} æˆåŠŸå“åº” ({len(response)} å­—ç¬¦) - ç¬¬{attempt+1}æ¬¡å°è¯•")
                        # æ›´æ–°æ¨¡å‹æ€§èƒ½ç»Ÿè®¡
                        self._update_model_performance_stats(model_name, sample_type, True, timeout)
                        return response
                    else:
                        error_msg = f"æ¨¡å‹ {model_name} è¿”å›é”™è¯¯ç  {result.returncode}"
                        if result.stderr:
                            error_msg += f": {result.stderr}"
                        self.logger.warning(f"âš ï¸ {error_msg} - ç¬¬{attempt+1}æ¬¡å°è¯•")

                except subprocess.TimeoutExpired:
                    self.logger.warning(f"â° æ¨¡å‹ {model_name} æŸ¥è¯¢è¶…æ—¶ ({timeout:.1f}s)ï¼Œä»»åŠ¡ç±»å‹: {sample_type} - ç¬¬{attempt+1}æ¬¡å°è¯•")

                    if attempt == max_retries:
                        # æ›´æ–°æ¨¡å‹æ€§èƒ½ç»Ÿè®¡
                        self._update_model_performance_stats(model_name, sample_type, False, timeout)
                        break

            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œå¯ç”¨é™çº§ç­–ç•¥
            return self._handle_query_failure(model_name, sample_type, prompt)

        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹ {model_name} æŸ¥è¯¢å¤±è´¥: {e}")
            return self._handle_query_failure(model_name, sample_type, prompt)

    def _fallback_math_model(self, prompt: str) -> Optional[str]:
        """æ•°å­¦æ¨ç†çš„å¤‡ç”¨æ¨¡å‹ç­–ç•¥"""
        # æŒ‰ä¼˜å…ˆçº§å°è¯•æ‰€æœ‰å¯ç”¨çš„æ•°å­¦æ¨¡å‹
        fallback_order = ['fast', 'balanced', 'powerful']

        for model_type in fallback_order:
            model_name = self.deepseek_models.get(model_type)
            if model_name:
                try:
                    self.logger.info(f"ğŸ”„ å°è¯•å¤‡ç”¨æ•°å­¦æ¨¡å‹: {model_name}")
                    result = subprocess.run([
                        'ollama', 'run', model_name, prompt
                    ], capture_output=True, text=True, timeout=60)  # 60ç§’è¶…æ—¶

                    if result.returncode == 0 and result.stdout.strip():
                        response = result.stdout.strip()
                        self.logger.info(f"âœ… å¤‡ç”¨æ¨¡å‹ {model_name} æˆåŠŸå“åº”æ•°å­¦é—®é¢˜")
                        return response

                except subprocess.TimeoutExpired:
                    self.logger.warning(f"â° å¤‡ç”¨æ¨¡å‹ {model_name} ä¹Ÿè¶…æ—¶")
                    continue
                except Exception as e:
                    self.logger.warning(f"âŒ å¤‡ç”¨æ¨¡å‹ {model_name} å¤±è´¥: {e}")
                    continue

        # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise RuntimeError("æ‰€æœ‰å¯ç”¨çš„DeepSeekæ•°å­¦æ¨¡å‹éƒ½æ— æ³•å“åº”ã€‚å¯èƒ½æ˜¯æ¨¡å‹åŠ è½½é—®é¢˜æˆ–ç³»ç»Ÿèµ„æºä¸è¶³ã€‚")

    def _calculate_dynamic_timeout(self, model_name: str, sample_type: str, prompt: str) -> float:
        """æ ¹æ®æ¨¡å‹æ€§èƒ½å†å²å’Œä»»åŠ¡å¤æ‚åº¦åŠ¨æ€è®¡ç®—è¶…æ—¶æ—¶é—´"""
        # åŸºç¡€è¶…æ—¶æ—¶é—´
        base_timeouts = {
            'deepseek-coder-v2:236b': 150,  # å¢å¤§åŸºç¡€æ—¶é—´
            'deepseek-coder:33b': 100,
            'deepseek-coder:6.7b': 45
        }

        base_timeout = base_timeouts.get(model_name, 60)

        # æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´
        complexity_multipliers = {
            'mathematical_reasoning': 1.5,  # æ•°å­¦æ¨ç†éœ€è¦æ›´å¤šæ—¶é—´
            'concept_analysis': 1.3,       # æ¦‚å¿µåˆ†æè¾ƒå¤æ‚
            'code_generation': 1.2,        # ä»£ç ç”Ÿæˆä¸­ç­‰å¤æ‚åº¦
            'text_understanding': 0.8      # æ–‡æœ¬ç†è§£ç›¸å¯¹ç®€å•
        }

        complexity_multiplier = complexity_multipliers.get(sample_type, 1.0)

        # æ ¹æ®prompté•¿åº¦è°ƒæ•´
        length_multiplier = min(2.0, max(0.5, len(prompt) / 500))  # åŸºäº500å­—ç¬¦åŸºå‡†

        # ä»æ€§èƒ½å†å²ä¸­å­¦ä¹ 
        performance_multiplier = self._get_performance_multiplier(model_name, sample_type)

        final_timeout = base_timeout * complexity_multiplier * length_multiplier * performance_multiplier

        self.logger.debug(f"åŠ¨æ€è¶…æ—¶è®¡ç®—: {model_name} + {sample_type} = {final_timeout:.1f}s "
                         f"(åŸºç¡€:{base_timeout}, å¤æ‚åº¦:{complexity_multiplier:.1f}, "
                         f"é•¿åº¦:{length_multiplier:.1f}, æ€§èƒ½:{performance_multiplier:.1f})")

        return final_timeout

    def _get_performance_multiplier(self, model_name: str, sample_type: str) -> float:
        """ä»å†å²æ€§èƒ½æ•°æ®ä¸­è·å–è°ƒæ•´ä¹˜æ•°"""
        if not hasattr(self, 'model_performance_stats'):
            self.model_performance_stats = {}

        key = f"{model_name}_{sample_type}"
        if key in self.model_performance_stats:
            stats = self.model_performance_stats[key]
            success_rate = stats['successes'] / max(stats['total_attempts'], 1)

            # å¦‚æœæˆåŠŸç‡ä½äº60%ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
            if success_rate < 0.6:
                return 1.3
            # å¦‚æœæˆåŠŸç‡é«˜äº90%ï¼Œå¯ä»¥ç¨å¾®å‡å°‘è¶…æ—¶æ—¶é—´
            elif success_rate > 0.9:
                return 0.9

        return 1.0

    def _update_model_performance_stats(self, model_name: str, sample_type: str, success: bool, response_time: float):
        """æ›´æ–°æ¨¡å‹æ€§èƒ½ç»Ÿè®¡"""
        if not hasattr(self, 'model_performance_stats'):
            self.model_performance_stats = {}

        key = f"{model_name}_{sample_type}"
        if key not in self.model_performance_stats:
            self.model_performance_stats[key] = {
                'successes': 0,
                'total_attempts': 0,
                'avg_response_time': 0,
                'response_times': []
            }

        stats = self.model_performance_stats[key]
        stats['total_attempts'] += 1
        if success:
            stats['successes'] += 1

        stats['response_times'].append(response_time)
        # ä¿æŒæœ€è¿‘20æ¬¡çš„å“åº”æ—¶é—´
        if len(stats['response_times']) > 20:
            stats['response_times'] = stats['response_times'][-20:]

        stats['avg_response_time'] = sum(stats['response_times']) / len(stats['response_times'])

    def _is_model_available(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        try:
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                return model_name in result.stdout
        except:
            pass
        return False

    def _handle_query_failure(self, model_name: str, sample_type: str, prompt: str):
        """å¤„ç†æŸ¥è¯¢å¤±è´¥çš„é™çº§ç­–ç•¥"""
        # å¯¹äºæ•°å­¦æ¨ç†ï¼Œä¼˜å…ˆä½¿ç”¨å¤‡ç”¨æ¨¡å‹
        if sample_type == 'mathematical_reasoning':
            self.logger.info(f"ğŸ”„ æ•°å­¦æ¨ç†ä»»åŠ¡ä½¿ç”¨å¤‡ç”¨æ¨¡å‹ç­–ç•¥")
            return self._fallback_math_model(prompt)

        # å¯¹äºå…¶ä»–ä»»åŠ¡ï¼Œå°è¯•ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹
        fallback_models = {
            'deepseek-coder-v2:236b': ['deepseek-coder:33b', 'deepseek-coder:6.7b'],
            'deepseek-coder:33b': ['deepseek-coder:6.7b', 'deepseek-coder-v2:236b'],
            'deepseek-coder:6.7b': ['deepseek-coder:33b', 'deepseek-coder-v2:236b']
        }

        if model_name in fallback_models:
            for fallback_model in fallback_models[model_name]:
                if self._is_model_available(fallback_model):
                    self.logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹ {fallback_model} å¤„ç† {sample_type} ä»»åŠ¡")
                    try:
                        result = subprocess.run([
                            'ollama', 'run', fallback_model, prompt
                        ], capture_output=True, text=True, timeout=45)  # å¤‡ç”¨æ¨¡å‹ä½¿ç”¨è¾ƒçŸ­è¶…æ—¶

                        if result.returncode == 0 and result.stdout.strip():
                            response = result.stdout.strip()
                            self.logger.info(f"âœ… å¤‡ç”¨æ¨¡å‹ {fallback_model} æˆåŠŸå“åº” ({len(response)} å­—ç¬¦)")
                            return response
                    except:
                        continue

        # æœ€åé™çº§åˆ°æ¨¡æ‹Ÿè¾“å‡º
        self.logger.warning(f"æ‰€æœ‰æ¨¡å‹å°è¯•å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè¾“å‡ºä½œä¸ºé™çº§æ–¹æ¡ˆ")
        return self._generate_simulated_output(prompt, sample_type)

    def _generate_simulated_output(self, prompt: str, sample_type: str) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„é«˜è´¨é‡è¾“å‡º"""
        if sample_type == 'code_generation':
            if 'fibonacci' in prompt.lower():
                return '''def fibonacci(n):
    """Calculate the nth Fibonacci number using dynamic programming."""
    if n <= 0:
        return 0
    elif n == 1:
        return 1

    a, b = 0, 1
    for _ in range(2, n + 1):
        a, b = b, a + b
    return b

# Example usage
if __name__ == "__main__":
    print(fibonacci(10))  # Output: 55'''
            elif 'class' in prompt.lower() and 'calculator' in prompt.lower():
                return '''class Calculator:
    """A simple calculator class with basic operations."""

    def __init__(self):
        self.result = 0

    def add(self, x, y):
        """Add two numbers."""
        return x + y

    def subtract(self, x, y):
        """Subtract y from x."""
        return x - y

    def multiply(self, x, y):
        """Multiply two numbers."""
        return x * y

    def divide(self, x, y):
        """Divide x by y."""
        if y == 0:
            raise ValueError("Cannot divide by zero")
        return x / y

# Example usage
calc = Calculator()
print(calc.add(5, 3))      # Output: 8
print(calc.multiply(4, 2)) # Output: 8'''
            else:
                return '''def example_function():
    """A sample function demonstrating good coding practices."""
    try:
        result = 42
        return result
    except Exception as e:
        print(f"Error: {e}")
        return None'''

        elif sample_type == 'text_understanding':
            if 'machine learning' in prompt.lower():
                return '''Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It involves algorithms that can identify patterns in data and make predictions or decisions based on those patterns.

Key characteristics of machine learning:
1. Learning from data without explicit programming
2. Ability to improve performance over time
3. Pattern recognition and predictive capabilities
4. Applications in various fields like image recognition, natural language processing, and recommendation systems

Machine learning has revolutionized many industries and continues to be a rapidly evolving field with new applications emerging regularly.'''
            elif 'artificial intelligence' in prompt.lower():
                return '''Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. It encompasses a wide range of technologies and approaches aimed at creating systems capable of performing tasks that typically require human intelligence.

Core components of AI include:
- Machine Learning: Algorithms that learn from data
- Natural Language Processing: Understanding and generating human language
- Computer Vision: Interpreting visual information
- Robotics: Physical systems that can perform tasks autonomously
- Expert Systems: Knowledge-based systems that solve complex problems

AI has the potential to transform society, economy, and scientific research, though it also raises important ethical and societal questions that need careful consideration.'''
            else:
                return '''The topic you're asking about represents a fascinating area of technology and science. It involves complex systems that can process information, learn from experience, and make intelligent decisions. This field has seen tremendous advancement in recent years and continues to evolve rapidly, offering both opportunities and challenges for humanity.'''

        elif sample_type == 'mathematical_reasoning':
            # æ•°å­¦æ¨ç†ä¸åº”è¯¥ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œåº”è¯¥æ€»æ˜¯ä½¿ç”¨çœŸå®çš„DeepSeekæ¨¡å‹
            raise RuntimeError("æ•°å­¦æ¨ç†å¿…é¡»ä½¿ç”¨çœŸå®çš„DeepSeekæ¨¡å‹ï¼Œä¸å…è®¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

        elif sample_type == 'concept_analysis':
            return '''Analyzing the relationship between concepts reveals several important insights:

1. **Interconnectedness**: Concepts are not isolated but form complex networks of relationships
2. **Hierarchical Structure**: Some concepts are fundamental building blocks for more complex ideas
3. **Contextual Dependencies**: The meaning and application of concepts can vary based on context
4. **Evolutionary Development**: Concepts build upon each other over time, creating increasingly sophisticated frameworks

This interconnected nature suggests that true understanding requires seeing the bigger picture and recognizing how individual concepts fit into larger systems of knowledge.'''

        else:
            return f"é«˜è´¨é‡{ sample_type }è¾“å‡ºç¤ºä¾‹ï¼š{prompt[:50]}..."

    def _train_local_agi_core(self, training_data: List[Dict[str, Any]]) -> Dict[str, float]:
        """è®­ç»ƒæœ¬åœ°AGIæ ¸å¿ƒæœº"""
        improvements = {}

        if not training_data:
            return improvements

        # æŒ‰ç±»å‹åˆ†ç»„è®­ç»ƒæ•°æ®
        type_groups = {}
        for sample in training_data:
            sample_type = sample['type']
            if sample_type not in type_groups:
                type_groups[sample_type] = []
            type_groups[sample_type].append(sample)

        # å¯¹æ¯ä¸ªç±»å‹è¿›è¡Œä¸“é¡¹è®­ç»ƒ
        for sample_type, samples in type_groups.items():
            improvement = self._train_specific_capability(sample_type, samples)
            improvements[sample_type] = improvement

        return improvements

    def _train_specific_capability(self, capability: str, samples: List[Dict[str, Any]]) -> float:
        """è®­ç»ƒç‰¹å®šèƒ½åŠ›"""
        if capability == 'code_generation':
            return self._train_code_generation(samples)
        elif capability == 'text_understanding':
            return self._train_text_understanding(samples)
        elif capability == 'mathematical_reasoning':
            return self._train_mathematical_reasoning(samples)
        elif capability == 'concept_analysis':
            return self._train_concept_analysis(samples)
        else:
            return 0.0

    def _train_code_generation(self, samples: List[Dict[str, Any]]) -> float:
        """è®­ç»ƒä»£ç ç”Ÿæˆèƒ½åŠ›"""
        # ä½¿ç”¨DeepSeekä»£ç æ¨¡å‹ä½œä¸ºæ•™å¸ˆ
        teacher_outputs = []
        for sample in samples:
            teacher_output = self._query_deepseek_model(sample['input'], 'code')
            if teacher_output:
                teacher_outputs.append((sample['input'], teacher_output))

        if not teacher_outputs:
            return 0.0

        # è®­ç»ƒæœ¬åœ°æ¨¡å‹æ¨¡ä»¿DeepSeek
        initial_score = self._evaluate_code_capability()
        self._fine_tune_on_teacher_data(teacher_outputs, 'code')
        final_score = self._evaluate_code_capability()

        return final_score - initial_score

    def _train_text_understanding(self, samples: List[Dict[str, Any]]) -> float:
        """è®­ç»ƒæ–‡æœ¬ç†è§£èƒ½åŠ›"""
        teacher_outputs = []
        for sample in samples:
            teacher_output = self._query_deepseek_model(sample['input'], 'text')
            if teacher_output:
                teacher_outputs.append((sample['input'], teacher_output))

        if not teacher_outputs:
            return 0.0

        initial_score = self._evaluate_text_capability()
        self._fine_tune_on_teacher_data(teacher_outputs, 'text')
        final_score = self._evaluate_text_capability()

        return final_score - initial_score

    def _train_mathematical_reasoning(self, samples: List[Dict[str, Any]]) -> float:
        """è®­ç»ƒæ•°å­¦æ¨ç†èƒ½åŠ›"""
        teacher_outputs = []
        for sample in samples:
            teacher_output = self._query_deepseek_model(sample['input'], 'math')
            if teacher_output:
                teacher_outputs.append((sample['input'], teacher_output))

        if not teacher_outputs:
            return 0.0

        initial_score = self._evaluate_math_capability()
        self._fine_tune_on_teacher_data(teacher_outputs, 'math')
        final_score = self._evaluate_math_capability()

        return final_score - initial_score

    def _train_concept_analysis(self, samples: List[Dict[str, Any]]) -> float:
        """è®­ç»ƒæ¦‚å¿µåˆ†æèƒ½åŠ›"""
        teacher_outputs = []
        for sample in samples:
            teacher_output = self._query_deepseek_model(sample['input'], 'concept')
            if teacher_output:
                teacher_outputs.append((sample['input'], teacher_output))

        if not teacher_outputs:
            return 0.0

        initial_score = self._evaluate_concept_capability()
        self._fine_tune_on_teacher_data(teacher_outputs, 'concept')
        final_score = self._evaluate_concept_capability()

        return final_score - initial_score

    def _fine_tune_on_teacher_data(self, teacher_data: List[Tuple[str, str]], task_type: str):
        """åœ¨æ•™å¸ˆæ•°æ®ä¸Šå¾®è°ƒ"""
        # ç®€åŒ–çš„å¾®è°ƒå®ç°
        optimizer = optim.Adam(self.local_agi_core.parameters(), lr=1e-5)

        for input_text, target_output in teacher_data:
            try:
                # ç¼–ç è¾“å…¥
                encoded_input = self.hierarchical_encoder.encode_hierarchical(input_text)

                # å‰å‘ä¼ æ’­
                outputs = self.local_agi_core.perform_local_inference(encoded_input['final_encoding'])

                # è®¡ç®—æŸå¤±ï¼ˆç®€åŒ–çš„å®ç°ï¼‰
                if outputs is not None:
                    # ä½¿ç”¨ç®€å•çš„MSEæŸå¤±ä½œä¸ºç¤ºä¾‹
                    target_tensor = torch.randn_like(outputs)  # ç®€åŒ–çš„ç›®æ ‡
                    loss = nn.MSELoss()(outputs, target_tensor)

                    # åå‘ä¼ æ’­
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

            except Exception as e:
                continue

    def _evaluate_code_capability(self) -> float:
        """è¯„ä¼°ä»£ç ç”Ÿæˆèƒ½åŠ›"""
        test_prompts = ["def hello", "class Test", "print("]
        score = 0.0

        for prompt in test_prompts:
            try:
                encoded = self.hierarchical_encoder.encode_hierarchical(prompt)
                output = self.local_agi_core.perform_local_inference(encoded['final_encoding'])

                if output is not None:
                    # æ£€æŸ¥è¾“å‡ºè´¨é‡
                    if 'def ' in str(output) or 'class ' in str(output):
                        score += 1.0
            except:
                continue

        return score / len(test_prompts)

    def _evaluate_text_capability(self) -> float:
        """è¯„ä¼°æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›"""
        test_inputs = ["hello", "world", "test"]
        score = 0.0

        for input_text in test_inputs:
            try:
                encoded = self.hierarchical_encoder.encode_hierarchical(input_text)
                output = self.local_agi_core.perform_local_inference(encoded['final_encoding'])

                if output is not None and len(str(output)) > len(input_text):
                    score += 1.0
            except:
                continue

        return score / len(test_inputs)

    def _evaluate_math_capability(self) -> float:
        """è¯„ä¼°æ•°å­¦æ¨ç†èƒ½åŠ›"""
        test_problems = ["2+2", "3*4", "10-5"]
        score = 0.0

        for problem in test_problems:
            try:
                encoded = self.hierarchical_encoder.encode_hierarchical(problem)
                output = self.local_agi_core.perform_local_inference(encoded['final_encoding'])

                if output is not None:
                    # æ£€æŸ¥è¾“å‡ºå¤æ‚åº¦
                    complexity = output.abs().mean().item()
                    if complexity > 0.1:
                        score += 1.0
            except:
                continue

        return score / len(test_problems)

    def _evaluate_concept_capability(self) -> float:
        """è¯„ä¼°æ¦‚å¿µç†è§£èƒ½åŠ›"""
        test_concepts = ["AI", "machine learning", "neural network"]
        score = 0.0

        for concept in test_concepts:
            try:
                encoded = self.hierarchical_encoder.encode_hierarchical(concept)
                output = self.local_agi_core.perform_local_inference(encoded['final_encoding'])

                if output is not None:
                    consistency = torch.softmax(output, dim=-1).var(dim=-1).mean().item()
                    if consistency < 0.8:
                        score += 1.0
            except:
                continue

        return score / len(test_concepts)

    def _run_benchmark_cycle(self):
        """è¿è¡ŒåŸºå‡†æµ‹è¯•å‘¨æœŸ"""
        print("ğŸ“Š æ‰§è¡ŒåŸºå‡†æµ‹è¯•å‘¨æœŸ...")

        # è¿è¡Œå„ç§åŸºå‡†æµ‹è¯•
        benchmark_results = {
            'concept_understanding': self._run_concept_understanding_benchmark(),
            'mathematical_reasoning': self._run_mathematical_reasoning_benchmark(),
            'code_generation': self._run_code_generation_benchmark(),
            'text_generation': self._run_text_generation_benchmark()
        }

        # æ›´æ–°ç»Ÿè®¡
        self.training_stats['benchmark_scores'] = benchmark_results

        # ä¿å­˜åŸºå‡†ç»“æœ
        result_file = f"/Users/imymm/H2Q-Evo/benchmark_cycle_{self.training_stats['evolution_cycles']}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump({
                'cycle': self.training_stats['evolution_cycles'],
                'timestamp': datetime.now().isoformat(),
                'benchmark_results': benchmark_results,
                'training_stats': self.training_stats
            }, f, indent=2, ensure_ascii=False)

        self.logger.info(f"åŸºå‡†æµ‹è¯•å®Œæˆ - ç»“æœ: {benchmark_results}")

    def _run_concept_understanding_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œæ¦‚å¿µç†è§£åŸºå‡†æµ‹è¯•"""
        # æ”¹è¿›çš„æµ‹è¯•ï¼šå®é™…éªŒè¯æ¦‚å¿µç†è§£
        concepts = ["machine learning", "artificial intelligence", "neural network"]
        questions = [
            "What is machine learning?",
            "How does AI differ from ML?",
            "Explain neural networks in simple terms"
        ]

        score = 0.0
        for concept, question in zip(concepts, questions):
            try:
                # ç”Ÿæˆå›ç­”
                encoded = self.hierarchical_encoder.encode_hierarchical(question)
                output = self.local_agi_core.perform_local_inference(encoded['final_encoding'])

                if output is not None:
                    response_text = str(output)

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸å…³æ¦‚å¿µå…³é”®è¯
                    relevant_keywords = {
                        "machine learning": ["learn", "data", "predict", "algorithm"],
                        "artificial intelligence": ["intelligence", "human", "think", "automate"],
                        "neural network": ["neuron", "layer", "brain", "connection"]
                    }

                    keywords = relevant_keywords.get(concept, [])
                    if any(keyword in response_text.lower() for keyword in keywords):
                        score += 1.0

            except Exception as e:
                continue

        return {
            'score': score / len(concepts),
            'concepts_tested': concepts,
            'questions_asked': questions
        }

    def _run_mathematical_reasoning_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•"""
        # æ”¹è¿›çš„æµ‹è¯•ï¼šéªŒè¯æ•°å­¦è®¡ç®—æ­£ç¡®æ€§
        problems = [
            ("2 + 2", 4),
            ("3 * 4", 12),
            ("10 - 5", 5),
            ("6 / 2", 3)
        ]

        correct_answers = 0
        for problem, expected in problems:
            try:
                encoded = self.hierarchical_encoder.encode_hierarchical(problem)
                output = self.local_agi_core.perform_local_inference(encoded['final_encoding'])

                if output is not None:
                    # ç®€åŒ–çš„ç­”æ¡ˆæå–ï¼ˆå®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„è§£æï¼‰
                    output_str = str(output)
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ­£ç¡®ç­”æ¡ˆçš„æ•°å­—
                    if str(expected) in output_str:
                        correct_answers += 1

            except Exception as e:
                continue

        return {
            'score': correct_answers / len(problems),
            'problems_tested': len(problems),
            'correct_answers': correct_answers
        }

    def _run_code_generation_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•"""
        prompts = [
            "Write a function to check if a number is prime",
            "Create a class for a stack data structure",
            "Write code to reverse a string"
        ]

        score = 0.0
        for prompt in prompts:
            try:
                encoded = self.hierarchical_encoder.encode_hierarchical(prompt)
                output = self.local_agi_core.perform_local_inference(encoded['final_encoding'])

                if output is not None:
                    code_output = str(output)

                    # æ£€æŸ¥ä»£ç è´¨é‡æŒ‡æ ‡
                    code_indicators = ['def ', 'class ', 'if ', 'for ', 'return ']
                    if any(indicator in code_output for indicator in code_indicators):
                        score += 1.0

            except Exception as e:
                continue

        return {
            'score': score / len(prompts),
            'prompts_tested': len(prompts)
        }

    def _run_text_generation_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œæ–‡æœ¬ç”ŸæˆåŸºå‡†æµ‹è¯•"""
        prompts = [
            "The future of AI is",
            "Machine learning helps us",
            "In the next decade,"
        ]

        score = 0.0
        for prompt in prompts:
            try:
                encoded = self.hierarchical_encoder.encode_hierarchical(prompt)
                output = self.local_agi_core.perform_local_inference(encoded['final_encoding'])

                if output is not None:
                    text_output = str(output)

                    # æ£€æŸ¥æ–‡æœ¬ç”Ÿæˆè´¨é‡
                    if len(text_output) > len(prompt) and any(word in text_output.lower()
                        for word in ['will', 'can', 'help', 'future', 'technology']):
                        score += 1.0

            except Exception as e:
                continue

        return {
            'score': score / len(prompts),
            'prompts_tested': len(prompts)
        }

    def _save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_file = f"/Users/imymm/H2Q-Evo/agi_evolution_checkpoint_{self.training_stats['evolution_cycles']}.pth"

        try:
            torch.save({
                'model_state_dict': self.local_agi_core.state_dict(),
                'training_stats': self.training_stats,
                'timestamp': datetime.now().isoformat()
            }, checkpoint_file)

            print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_file}")
            self.logger.info(f"æ£€æŸ¥ç‚¹ä¿å­˜: {checkpoint_file}")

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")

    def get_training_status(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒçŠ¶æ€"""
        return {
            'evolution_active': self.evolution_active,
            'training_stats': self.training_stats,
            'deepseek_models': self.deepseek_models,
            'last_checkpoint': datetime.now().isoformat()
        }


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ H2Q-Evo AGI è‡ªç›‘ç£è¿›åŒ–è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)

    # é…ç½®
    config = {
        'evolution_duration_hours': 168,  # 7å¤©
        'checkpoint_interval_hours': 1,
        'benchmark_interval_hours': 2,
        'max_samples_per_cycle': 100
    }

    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = DeepSeekEnhancedAGITrainer(config)

    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
    print(f"ğŸ¤– å¯ç”¨DeepSeekæ¨¡å‹: {trainer.deepseek_models}")

    # å¯åŠ¨7*24å°æ—¶è¿›åŒ–
    trainer.start_24_7_evolution()

    try:
        # ä¿æŒä¸»çº¿ç¨‹è¿è¡Œ
        while trainer.evolution_active:
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡çŠ¶æ€

            # æ˜¾ç¤ºçŠ¶æ€
            status = trainer.get_training_status()
            print(f"\rğŸ”„ è¿›åŒ–å‘¨æœŸ: {status['training_stats']['evolution_cycles']} | "
                  f"æ ·æœ¬å¤„ç†: {status['training_stats']['total_samples_processed']} | "
                  f"çŠ¶æ€: {'è¿è¡Œä¸­' if status['evolution_active'] else 'å·²åœæ­¢'}", end='')

    except KeyboardInterrupt:
        print("\n\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
        trainer.stop_evolution()

    print("\nâœ… AGIè¿›åŒ–è®­ç»ƒå®Œæˆ")


if __name__ == "__main__":
    main()