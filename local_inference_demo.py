#!/usr/bin/env python3
"""
H2Q-Evo æœ¬åœ°æ¨¡å‹æ¨ç†æ¼”ç¤º
å±•ç¤ºè®­ç»ƒåçš„æ¨¡å‹èƒ½åŠ›
"""

import sys
import torch
import torch.nn as nn
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
H2Q_PROJECT = PROJECT_ROOT / "h2q_project"
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(H2Q_PROJECT))


class LocalInferenceModel:
    """æœ¬åœ°æ¨ç†æ¨¡å‹"""

    def __init__(self, model_path: Path = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.vocab_size = 256  # ASCIIå­—ç¬¦

        # å°è¯•åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        if model_path and model_path.exists():
            self.load_model(model_path)
        else:
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹
            self._init_default_model()

        print(f"ğŸ§  æœ¬åœ°æ¨ç†æ¨¡å‹å·²åŠ è½½ | è®¾å¤‡: {self.device}")

    def _init_default_model(self):
        """åˆå§‹åŒ–é»˜è®¤æ¨¡å‹"""
        embed_dim = 256
        n_heads = 8
        n_layers = 6

        self.model = nn.Sequential(
            nn.Embedding(self.vocab_size, embed_dim),
            nn.TransformerEncoder(
                nn.TransformerEncoderLayer(
                    embed_dim, n_heads, batch_first=True, dropout=0.1
                ),
                num_layers=n_layers
            ),
            nn.Linear(embed_dim, self.vocab_size)
        ).to(self.device)

    def load_model(self, model_path: Path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            # åˆå§‹åŒ–æ¨¡å‹ç»“æ„
            self._init_default_model()

            # åŠ è½½æƒé‡
            state_dict = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(state_dict)
            print(f"âœ“ æ¨¡å‹æƒé‡å·²åŠ è½½: {model_path}")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: {e}")
            self._init_default_model()

    def generate_text(self, prompt: str, max_length: int = 100, temperature: float = 1.0) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        self.model.eval()

        # ç¼–ç æç¤º
        tokens = [ord(c) % self.vocab_size for c in prompt]
        input_ids = torch.tensor([tokens], dtype=torch.long).to(self.device)

        generated = prompt

        with torch.no_grad():
            for _ in range(max_length):
                # è·å–é¢„æµ‹
                outputs = self.model(input_ids)
                next_token_logits = outputs[0, -1, :] / temperature

                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, 1).item()

                # æ·»åŠ åˆ°åºåˆ—
                next_char = chr(next_token % 128)  # é™åˆ¶åˆ°ASCIIèŒƒå›´
                generated += next_char

                # æ›´æ–°è¾“å…¥
                next_token_tensor = torch.tensor([[next_token]], dtype=torch.long).to(self.device)
                input_ids = torch.cat([input_ids, next_token_tensor], dim=1)

                # é™åˆ¶é•¿åº¦
                if len(input_ids[0]) >= 512:
                    break

        return generated


def demonstrate_capabilities():
    """æ¼”ç¤ºæ¨¡å‹èƒ½åŠ›"""
    print("\n" + "="*60)
    print("ğŸ§  H2Q-Evo æœ¬åœ°æ¨¡å‹æ¨ç†æ¼”ç¤º")
    print("="*60)
    print("ğŸ›¡ï¸ å®‰å…¨ä¿è¯ï¼šå®Œå…¨ç¦»çº¿ï¼Œæ— è”ç½‘")
    print("="*60 + "\n")

    # åŠ è½½æ¨¡å‹
    model_path = PROJECT_ROOT / "h2q_project" / "h2q_trained_model.pt"
    model = LocalInferenceModel(model_path)

    # æµ‹è¯•æç¤º
    test_prompts = [
        "äººå·¥æ™ºèƒ½æ˜¯",
        "æœºå™¨å­¦ä¹ ",
        "é‡å­è®¡ç®—å¯ä»¥",
        "æ·±åº¦å­¦ä¹ "
    ]

    print("ğŸ“ ç”Ÿæˆæ–‡æœ¬æ¼”ç¤º:")
    print("-" * 40)

    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nğŸ¯ æç¤º {i}: {prompt}")
        generated = model.generate_text(prompt, max_length=50, temperature=0.8)
        print(f"ğŸ¤– ç”Ÿæˆ: {generated}")
        print("-" * 40)

    # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡
    try:
        with open(PROJECT_ROOT / "training_log.json", 'r', encoding='utf-8') as f:
            logs = json.load(f)

        if logs:
            latest_log = logs[-1]
            print("\nğŸ“Š æœ€æ–°è®­ç»ƒç»Ÿè®¡:")
            print(f"  ğŸ“… è½®æ¬¡: {latest_log['epoch']}")
            print(f"  ğŸ“‰ æŸå¤±: {latest_log['train_loss']:.4f}")
            if latest_log.get('perplexity'):
                print(f"  ğŸ¯ å›°æƒ‘åº¦: {latest_log['perplexity']:.2f}")
            print(f"  â±ï¸ è½®æ¬¡æ—¶é—´: {latest_log['epoch_time']:.2f} ç§’")
            print(f"  ğŸ“Š æ€»æ—¶é—´: {latest_log['total_time']:.2f} ç§’")
    except FileNotFoundError:
        print("\nâš ï¸ æœªæ‰¾åˆ°è®­ç»ƒæ—¥å¿—")

    # æ˜¾ç¤ºè¿›åŒ–ç»Ÿè®¡
    try:
        with open(PROJECT_ROOT / "evolution_stats.json", 'r', encoding='utf-8') as f:
            stats = json.load(f)

        print("\nğŸ§¬ è¿›åŒ–ç»Ÿè®¡:")
        print(f"  ğŸ”¬ æ€»è¿›åŒ–æ¬¡æ•°: {stats['total_evolutions']}")
        print(f"  âœ… æˆåŠŸè¿›åŒ–: {stats['successful_evolutions']}")
        print(f"  âŒ å¤±è´¥è¿›åŒ–: {stats['failed_evolutions']}")
        print(f"  ğŸ“Š å¹³å‡æ”¹è¿›: {stats['average_improvement']:.4f}")
    except FileNotFoundError:
        print("\nâš ï¸ æœªæ‰¾åˆ°è¿›åŒ–ç»Ÿè®¡")

    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ’¡ æç¤ºï¼šæ¨¡å‹å·²é€šè¿‡æœ¬åœ°è®­ç»ƒè¿›åŒ–ï¼Œå¯ä»¥å®‰å…¨ç¦»çº¿ä½¿ç”¨")


def interactive_mode():
    """äº¤äº’æ¨¡å¼"""
    print("\n" + "="*60)
    print("ğŸ’¬ H2Q-Evo äº¤äº’å¼å¯¹è¯")
    print("="*60)
    print("ğŸ›¡ï¸ å®‰å…¨ä¿è¯ï¼šå®Œå…¨ç¦»çº¿ï¼Œæ— è”ç½‘")
    print("è¾“å…¥ 'quit' é€€å‡º")
    print("="*60 + "\n")

    # åŠ è½½æ¨¡å‹
    model_path = PROJECT_ROOT / "h2q_project" / "h2q_trained_model.pt"
    model = LocalInferenceModel(model_path)

    while True:
        try:
            user_input = input("ğŸ‘¤ æ‚¨: ").strip()
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break

            if user_input:
                print("ğŸ¤– AI: ", end="", flush=True)
                response = model.generate_text(user_input, max_length=100, temperature=0.7)
                print(response)
                print()

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="H2Q-Evo æœ¬åœ°æ¨¡å‹æ¨ç†æ¼”ç¤º")
    parser.add_argument("--mode", choices=["demo", "interactive"], default="demo",
                       help="è¿è¡Œæ¨¡å¼ï¼šdemo(æ¼”ç¤º) æˆ– interactive(äº¤äº’)")
    parser.add_argument("--model", type=str,
                       help="æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä¸ºè‡ªåŠ¨æŸ¥æ‰¾ï¼‰")

    args = parser.parse_args()

    if args.mode == "demo":
        demonstrate_capabilities()
    elif args.mode == "interactive":
        interactive_mode()


if __name__ == "__main__":
    main()