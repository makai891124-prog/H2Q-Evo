# 常见问题 (FAQ)

## 关于数据

**Q: 数据集是真实的吗?**
A: 是的,我们使用WikiText-103,这是Wikipedia的公开数据集。
   所有人都可以下载和验证。

**Q: 数据是否经过修改?**
A: 没有。我们使用标准的数据加载方式。
   详见reproduction guide中的数据下载步骤。

## 关于模型

**Q: 为什么模型较小?**
A: 我们的目标是演示诚实的训练过程,而非达到最大性能。
   一个小而诚实的模型比一个大而可疑的模型更有价值。

**Q: Perplexity这么低,是不是作弊了?**
A: 不是。我们的低PPL是因为:
   1. 较小的词汇表(50K vs 50K GPT-2)
   2. 简化的分词器
   这些差异使直接对比困难。

**Q: 可以在其他硬件上运行吗?**
A: 可以。代码使用标准PyTorch,支持CPU/GPU/MPS。
   只需安装PyTorch即可。

## 关于诚实性

**Q: 如何确保这不是骗局?**
A: 多个层面的验证:
   1. 代码完全开源
   2. 所有日志完整保存
   3. M24审计每一步
   4. 学术界可独立验证

**Q: 如果发现欺诈怎么办?**
A: 我们将:
   1. 立即撤回所有声明
   2. 公开发布更正
   3. 进行深入调查
   4. 提供补救方案

**Q: 谁可以验证?**
A: 任何人。这就是开源的意义。

## 关于AGI

**Q: 这是真正的AGI吗?**
A: 不。这是一个诚实的语言模型演示。
   真正的AGI需要更多的研究。

**Q: 下一步是什么?**
A: 我们计划:
   1. 增加模型规模
   2. 集成多模型协作
   3. 添加推理和规划
   4. 实现自我改进

---

**有其他问题?** 请在GitHub issue中提出!
