#!/usr/bin/env python3
"""
H2Q-Evo é¡¹ç›®ä»£ç çœŸå®æ€§å®¡è®¡è„šæœ¬

éªŒè¯ï¼š
1. ä»£ç æ˜¯å¦æœ‰ç¡¬ç¼–ç çš„åŸºå‡†æµ‹è¯•ç»“æœ
2. DeepSeekæ¨¡å‹æ˜¯å¦çœŸå®å¯åŠ¨
3. ç»“æ™¶åŒ–å‹ç¼©çš„çœŸå®æ€§èƒ½
4. å†…å­˜ä¼˜åŒ–å£°æ˜çš„çœŸå®æ€§
"""

import torch
import torch.nn as nn
import json
import os
import time
import psutil
import hashlib
from typing import Dict, Any, List
import numpy as np


def audit_hardcoded_results():
    """å®¡è®¡æ˜¯å¦æœ‰ç¡¬ç¼–ç çš„åŸºå‡†æµ‹è¯•ç»“æœ"""
    print("ğŸ” å®¡è®¡1: æ£€æŸ¥ç¡¬ç¼–ç åŸºå‡†æµ‹è¯•ç»“æœ")
    print("=" * 50)

    suspicious_files = [
        'deepseek_memory_safe_benchmark_results.json',
        'benchmark_results.json',
        'benchmark_results_v2.json'
    ]

    issues = []

    for file in suspicious_files:
        if os.path.exists(file):
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # æ£€æŸ¥å¯ç–‘æ¨¡å¼
                if isinstance(data, dict):
                    for category, tests in data.items():
                        if isinstance(tests, list):
                            for test in tests:
                                if isinstance(test, dict):
                                    # æ£€æŸ¥response_timeæ˜¯å¦ä¸çœŸå®ï¼ˆå¤ªå¿«ï¼‰
                                    if 'response_time' in test:
                                        rt = test['response_time']
                                        if rt < 0.001:  # å°äº1ms
                                            issues.append(f"{file}: {test.get('test_name', 'unknown')} å“åº”æ—¶é—´å¯ç–‘: {rt}ç§’")

                                    # æ£€æŸ¥memory_usedæ˜¯å¦å›ºå®šå€¼
                                    if 'memory_used' in test and test['memory_used'] == 50:
                                        issues.append(f"{file}: {test.get('test_name', 'unknown')} å†…å­˜ä½¿ç”¨å›ºå®šä¸º50MB")

                                    # æ£€æŸ¥quality_scoreæ˜¯å¦å¯ç–‘
                                    if 'quality_score' in test and test['quality_score'] == 0.0:
                                        issues.append(f"{file}: {test.get('test_name', 'unknown')} è´¨é‡è¯„åˆ†å§‹ç»ˆä¸º0")

            except Exception as e:
                issues.append(f"{file}: JSONè§£æé”™è¯¯ - {e}")

    if issues:
        print("âŒ å‘ç°å¯ç–‘ç¡¬ç¼–ç æ¨¡å¼:")
        for issue in issues:
            print(f"   {issue}")
    else:
        print("âœ… æœªå‘ç°æ˜æ˜¾çš„ç¡¬ç¼–ç æ¨¡å¼")

    return issues


def audit_deepseek_loading():
    """å®¡è®¡DeepSeekæ¨¡å‹æ˜¯å¦çœŸå®å¯åŠ¨"""
    print("\nğŸ” å®¡è®¡2: æ£€æŸ¥DeepSeekæ¨¡å‹çœŸå®æ€§")
    print("=" * 50)

    issues = []

    # æ£€æŸ¥æ˜¯å¦æœ‰çœŸå®çš„æ¨¡å‹æ–‡ä»¶
    model_dirs = ['models/', 'crystallized_models/']
    deepseek_files = []

    for dir_path in model_dirs:
        if os.path.exists(dir_path):
            for root, dirs, files in os.walk(dir_path):
                for file in files:
                    if 'deepseek' in file.lower():
                        deepseek_files.append(os.path.join(root, file))

    if not deepseek_files:
        issues.append("æœªæ‰¾åˆ°ä»»ä½•DeepSeekç›¸å…³çš„æ¨¡å‹æ–‡ä»¶")
    else:
        print(f"æ‰¾åˆ° {len(deepseek_files)} ä¸ªDeepSeekç›¸å…³æ–‡ä»¶:")
        for f in deepseek_files:
            size = os.path.getsize(f) / (1024**3)  # GB
            print(f"     {f}: {size:.3f} GB")

    # æ£€æŸ¥ollamaæ¡¥æ¥æ˜¯å¦èƒ½çœŸå®è¿æ¥
    try:
        from ollama_bridge import OllamaBridge, OllamaConfig
        config = OllamaConfig()
        bridge = OllamaBridge(config)

        if not bridge.check_ollama_status():
            issues.append("OllamaæœåŠ¡æœªè¿è¡Œï¼Œæ— æ³•åŠ è½½çœŸå®æ¨¡å‹")
        else:
            available_models = bridge.list_available_models()
            deepseek_models = [m for m in available_models if 'deepseek' in m.lower()]

            if not deepseek_models:
                issues.append("Ollamaä¸­æœªæ‰¾åˆ°DeepSeekæ¨¡å‹")
            else:
                print(f"Ollamaä¸­æœ‰ {len(deepseek_models)} ä¸ªDeepSeekæ¨¡å‹: {deepseek_models}")

    except Exception as e:
        issues.append(f"Ollamaæ¡¥æ¥æµ‹è¯•å¤±è´¥: {e}")

    if issues:
        print("âŒ DeepSeekåŠ è½½é—®é¢˜:")
        for issue in issues:
            print(f"   {issue}")
    else:
        print("âœ… DeepSeekæ¨¡å‹åŠ è½½éªŒè¯é€šè¿‡")

    return issues


def audit_crystallization_performance():
    """å®¡è®¡ç»“æ™¶åŒ–å‹ç¼©çš„çœŸå®æ€§èƒ½"""
    print("\nğŸ” å®¡è®¡3: éªŒè¯ç»“æ™¶åŒ–å‹ç¼©æ€§èƒ½")
    print("=" * 50)

    issues = []

    try:
        from model_crystallization_engine import ModelCrystallizationEngine, CrystallizationConfig

        # åˆ›å»ºæµ‹è¯•æ¨¡å‹
        class TestModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.layers = nn.ModuleList([
                    nn.Linear(768, 768) for _ in range(12)
                ])

            def forward(self, x):
                for layer in self.layers:
                    x = layer(x)
                return x

        model = TestModel()
        original_params = sum(p.numel() for p in model.parameters())
        original_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)

        print("æµ‹è¯•æ¨¡å‹ç»Ÿè®¡:")
        print(f"   å‚æ•°æ•°é‡: {original_params:,}")
        print(f"   æ¨¡å‹å¤§å°: {original_size:.2f} MB")

        # åˆå§‹åŒ–ç»“æ™¶åŒ–å¼•æ“
        config = CrystallizationConfig(
            target_compression_ratio=8.0,
            max_memory_mb=1024
        )
        engine = ModelCrystallizationEngine(config)

        # æ‰§è¡Œç»“æ™¶åŒ–
        start_time = time.time()
        report = engine.crystallize_model(model, "test_model")
        crystallization_time = time.time() - start_time

        print("ç»“æ™¶åŒ–ç»“æœ:")
        print(f"   å‹ç¼©ç‡: {report.get('compression_ratio', 1.0):.1f}x")
        print(f"   è´¨é‡åˆ†æ•°: {report.get('quality_score', 0.0):.3f}")
        print(f"   å‹ç¼©æ—¶é—´: {crystallization_time:.2f} ç§’")
        print(f"   å†…å­˜ä½¿ç”¨: {report.get('memory_usage_mb', 0):.2f} MB")

        # éªŒè¯å‹ç¼©ç‡æ˜¯å¦åˆç†
        actual_ratio = report.get('compression_ratio', 1.0)
        if actual_ratio < 2.0:
            issues.append(f"å‹ç¼©ç‡è¿‡ä½: {actual_ratio:.1f}x (æœŸæœ›>8x)")

        # éªŒè¯è´¨é‡ä¿æŒ
        quality = report.get('quality_score', 0.0)
        if quality < 0.8:
            issues.append(f"è´¨é‡åˆ†æ•°è¿‡ä½: {quality:.3f} (æœŸæœ›>0.9)")

        # éªŒè¯å†…å­˜æ•ˆç‡
        memory_mb = report.get('memory_usage_mb', 0)
        if memory_mb > 500:  # 500MB
            issues.append(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_mb:.1f}MB")

    except Exception as e:
        issues.append(f"ç»“æ™¶åŒ–æµ‹è¯•å¤±è´¥: {e}")

    if issues:
        print("âŒ ç»“æ™¶åŒ–æ€§èƒ½é—®é¢˜:")
        for issue in issues:
            print(f"   {issue}")
    else:
        print("âœ… ç»“æ™¶åŒ–æ€§èƒ½éªŒè¯é€šè¿‡")

    return issues


def audit_memory_optimization():
    """å®¡è®¡å†…å­˜ä¼˜åŒ–å£°æ˜çš„çœŸå®æ€§"""
    print("\nğŸ” å®¡è®¡4: éªŒè¯å†…å­˜ä¼˜åŒ–å£°æ˜")
    print("=" * 50)

    issues = []

    # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
    memory = psutil.virtual_memory()
    print("å½“å‰ç³»ç»Ÿå†…å­˜çŠ¶æ€:")
    print(f"   æ€»å†…å­˜: {memory.total / (1024**3):.2f} GB")
    print(f"   å¯ç”¨å†…å­˜: {memory.available / (1024**3):.2f} GB")
    print(f"   ä½¿ç”¨ç‡: {memory.percent:.1f}%")
    # æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜ç›‘æ§
    try:
        from memory_safe_startup import MemorySafeStartupSystem, MemorySafeConfig

        config = MemorySafeConfig(max_memory_mb=2048)  # 2GBé™åˆ¶
        system = MemorySafeStartupSystem(config)

        if system.start_safe_startup():
            print("âœ… å†…å­˜å®‰å…¨ç³»ç»Ÿå¯åŠ¨æˆåŠŸ")

            # æ£€æŸ¥å†…å­˜é¢„ç®—
            budget = system.get_memory_budget()
            print("å†…å­˜é¢„ç®—åˆ†é…:")
            for key, value in budget.items():
                print(f"   {key}: {value:.1f} MB")

            # éªŒè¯é¢„ç®—åˆç†æ€§
            current_usage = budget.get("current_usage", 0)
            budget_limit = budget.get("budget_limit", 0)
            if current_usage > budget_limit:
                issues.append(f"é¢„ç®—è¶…é™: {current_usage:.1f}MB > {budget_limit:.1f}MB")

        else:
            issues.append("å†…å­˜å®‰å…¨ç³»ç»Ÿå¯åŠ¨å¤±è´¥")

    except Exception as e:
        issues.append(f"å†…å­˜ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")

    if issues:
        print("âŒ å†…å­˜ä¼˜åŒ–é—®é¢˜:")
        for issue in issues:
            print(f"   {issue}")
    else:
        print("âœ… å†…å­˜ä¼˜åŒ–éªŒè¯é€šè¿‡")

    return issues


def run_real_benchmark():
    """è¿è¡ŒçœŸå®çš„åŸºå‡†æµ‹è¯•"""
    print("\nğŸ” å®¡è®¡5: è¿è¡ŒçœŸå®åŸºå‡†æµ‹è¯•")
    print("=" * 50)

    results = {}

    try:
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ¨¡å‹
        model = nn.Sequential(
            nn.Linear(100, 50),
            nn.ReLU(),
            nn.Linear(50, 10)
        )

        # æµ‹è¯•æ¨ç†æ—¶é—´
        model.eval()
        test_input = torch.randn(1, 100)

        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(10):
                _ = model(test_input)

        # å®é™…æµ‹è¯•
        start_time = time.time()
        num_runs = 100
        with torch.no_grad():
            for _ in range(num_runs):
                _ = model(test_input)
        avg_time = (time.time() - start_time) / num_runs

        results['inference_time'] = avg_time
        results['model_params'] = sum(p.numel() for p in model.parameters())

        print("çœŸå®åŸºå‡†æµ‹è¯•ç»“æœ:")
        print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.6f} ç§’")
        print(f"   æ¨¡å‹å‚æ•°: {results['model_params']:,}")

        # ä¸å®£ç§°çš„æ€§èƒ½æ¯”è¾ƒ
        claimed_time = 0.001  # å‡è®¾çš„å®£ç§°æ—¶é—´
        if avg_time > claimed_time * 10:  # 10å€å·®è·
            print(f"âš ï¸ å®é™…æ¨ç†æ—¶é—´ ({avg_time:.6f}s) è¿œé«˜äºå®£ç§°æ°´å¹³")

    except Exception as e:
        print(f"âŒ çœŸå®åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        results['error'] = str(e)

    return results


def generate_audit_report(all_issues, benchmark_results):
    """ç”Ÿæˆå®¡è®¡æŠ¥å‘Š"""
    print("\nğŸ“Š å®¡è®¡æŠ¥å‘Šæ€»ç»“")
    print("=" * 50)

    total_issues = sum(len(issues) for issues in all_issues.values())

    if total_issues == 0:
        print("ğŸ‰ å®¡è®¡é€šè¿‡ï¼æœªå‘ç°ä¸¥é‡é—®é¢˜")
        print("   ä»£ç å®ç°çœŸå®ï¼Œæ€§èƒ½æ•°æ®å¯ä¿¡")
    else:
        print(f"âš ï¸ å‘ç° {total_issues} ä¸ªæ½œåœ¨é—®é¢˜:")
        for category, issues in all_issues.items():
            if issues:
                print(f"   {category}: {len(issues)} ä¸ªé—®é¢˜")

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report = {
        'audit_timestamp': time.time(),
        'total_issues': total_issues,
        'issues_by_category': all_issues,
        'benchmark_results': benchmark_results,
        'system_info': {
            'python_version': os.sys.version,
            'torch_version': torch.__version__,
            'memory_gb': psutil.virtual_memory().total / (1024**3)
        }
    }

    with open('code_authenticity_audit_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print("è¯¦ç»†å®¡è®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: code_authenticity_audit_report.json")


def main():
    """ä¸»å®¡è®¡å‡½æ•°"""
    print("ğŸš€ H2Q-Evo é¡¹ç›®ä»£ç çœŸå®æ€§å®¡è®¡")
    print("=" * 60)

    all_issues = {}

    # 1. æ£€æŸ¥ç¡¬ç¼–ç ç»“æœ
    all_issues['hardcoded_results'] = audit_hardcoded_results()

    # 2. æ£€æŸ¥DeepSeekçœŸå®æ€§
    all_issues['deepseek_loading'] = audit_deepseek_loading()

    # 3. éªŒè¯ç»“æ™¶åŒ–æ€§èƒ½
    all_issues['crystallization'] = audit_crystallization_performance()

    # 4. éªŒè¯å†…å­˜ä¼˜åŒ–
    all_issues['memory_optimization'] = audit_memory_optimization()

    # 5. è¿è¡ŒçœŸå®åŸºå‡†æµ‹è¯•
    benchmark_results = run_real_benchmark()

    # ç”ŸæˆæŠ¥å‘Š
    generate_audit_report(all_issues, benchmark_results)

    print("\nâœ¨ å®¡è®¡å®Œæˆï¼")


if __name__ == "__main__":
    main()