#!/usr/bin/env python3
"""
H2Q-Evo å†…å­˜å®‰å…¨ DeepSeek åŸºå‡†æµ‹è¯•

ä½¿ç”¨å†…å­˜å®‰å…¨å¯åŠ¨ç³»ç»Ÿè¿›è¡ŒçœŸæ­£çš„DeepSeekæ¨¡å‹åŸºå‡†æµ‹è¯•ï¼š
1. ä»£ç ç”Ÿæˆæµ‹è¯•
2. æ•°å­¦æ¨ç†æµ‹è¯•
3. ç®—æ³•ä»»åŠ¡æµ‹è¯•
4. å†…å­˜ä½¿ç”¨ç›‘æ§
"""

import time
import json
import os
from typing import Dict, List, Any, Optional
from memory_safe_startup import MemorySafeStartupSystem, MemorySafeConfig


class MemorySafeDeepSeekBenchmark:
    """å†…å­˜å®‰å…¨DeepSeekåŸºå‡†æµ‹è¯•"""

    def __init__(self, startup_system: MemorySafeStartupSystem):
        self.startup_system = startup_system
        self.results = {
            'code_generation': [],
            'mathematical_reasoning': [],
            'algorithmic_tasks': [],
            'memory_usage': [],
            'performance_metrics': {}
        }

    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢åŸºå‡†æµ‹è¯•"""
        print("ğŸ§ª H2Q-Evo å†…å­˜å®‰å…¨ DeepSeek åŸºå‡†æµ‹è¯•")
        print("=" * 60)

        benchmark_start = time.time()

        try:
            # 1. ä»£ç ç”Ÿæˆæµ‹è¯•
            print("1. è¿è¡Œä»£ç ç”Ÿæˆæµ‹è¯•...")
            code_results = self._run_code_generation_tests()
            self.results['code_generation'] = code_results

            # 2. æ•°å­¦æ¨ç†æµ‹è¯•
            print("2. è¿è¡Œæ•°å­¦æ¨ç†æµ‹è¯•...")
            math_results = self._run_mathematical_reasoning_tests()
            self.results['mathematical_reasoning'] = math_results

            # 3. ç®—æ³•ä»»åŠ¡æµ‹è¯•
            print("3. è¿è¡Œç®—æ³•ä»»åŠ¡æµ‹è¯•...")
            algo_results = self._run_algorithmic_task_tests()
            self.results['algorithmic_tasks'] = algo_results

            # 4. æ”¶é›†æ€§èƒ½æŒ‡æ ‡
            print("4. æ”¶é›†æ€§èƒ½æŒ‡æ ‡...")
            self._collect_performance_metrics()

            # è®¡ç®—æ€»ä½“ç»“æœ
            total_time = time.time() - benchmark_start
            self.results['performance_metrics'].update({
                'total_benchmark_time': total_time,
                'tests_completed': len(code_results) + len(math_results) + len(algo_results),
                'success_rate': self._calculate_success_rate(),
                'average_response_time': self._calculate_average_response_time(),
                'memory_efficiency': self._calculate_memory_efficiency()
            })

            print("âœ… åŸºå‡†æµ‹è¯•å®Œæˆï¼")
            print(f"   æ€»æ—¶é—´: {total_time:.2f} ç§’")
            print(f"   æµ‹è¯•æ€»æ•°: {self.results['performance_metrics']['tests_completed']}")
            print(f"   æˆåŠŸç‡: {self.results['performance_metrics']['success_rate']:.1%}")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {self.results['performance_metrics']['average_response_time']:.3f} ç§’")
            print(f"   å†…å­˜æ•ˆç‡: {self.results['performance_metrics']['memory_efficiency']:.1f}/10")
            return self.results

        except Exception as e:
            print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            self.results['error'] = str(e)
            return self.results

    def _run_code_generation_tests(self) -> List[Dict[str, Any]]:
        """è¿è¡Œä»£ç ç”Ÿæˆæµ‹è¯•"""
        test_cases = [
            {
                'name': 'simple_function',
                'prompt': 'Write a Python function to calculate factorial recursively',
                'expected_features': ['recursion', 'base_case', 'function_definition']
            },
            {
                'name': 'class_implementation',
                'prompt': 'Create a Python class for a simple calculator with add, subtract, multiply, divide methods',
                'expected_features': ['class_definition', 'methods', 'error_handling']
            },
            {
                'name': 'list_comprehension',
                'prompt': 'Write a list comprehension to filter even numbers from a list and square them',
                'expected_features': ['list_comprehension', 'filtering', 'mathematical_operation']
            },
            {
                'name': 'file_operations',
                'prompt': 'Write Python code to read a text file, count word frequencies, and write results to another file',
                'expected_features': ['file_reading', 'file_writing', 'dictionary_usage', 'string_processing']
            },
            {
                'name': 'api_simulation',
                'prompt': 'Create a simple REST API simulation using Flask with GET and POST endpoints',
                'expected_features': ['flask_app', 'routes', 'json_response', 'request_handling']
            }
        ]

        results = []
        for test_case in test_cases:
            print(f"   æµ‹è¯•: {test_case['name']}")

            start_time = time.time()
            result = self.startup_system.run_memory_safe_inference(test_case['prompt'])
            response_time = time.time() - start_time

            # è¯„ä¼°ç”Ÿæˆä»£ç çš„è´¨é‡
            quality_score = self._evaluate_code_quality(result.get('output', ''), test_case['expected_features'])

            test_result = {
                'test_name': test_case['name'],
                'prompt': test_case['prompt'],
                'response_time': response_time,
                'success': 'error' not in result,
                'quality_score': quality_score,
                'output_length': len(result.get('output', '')),
                'memory_used': result.get('memory_used', 0)
            }

            results.append(test_result)
            print(f"     å“åº”æ—¶é—´: {response_time:.2f} ç§’")
            print(f"     è´¨é‡è¯„åˆ†: {quality_score}/10")

        return results

    def _run_mathematical_reasoning_tests(self) -> List[Dict[str, Any]]:
        """è¿è¡Œæ•°å­¦æ¨ç†æµ‹è¯•"""
        test_cases = [
            {
                'name': 'algebraic_manipulation',
                'prompt': 'Solve for x: 2x + 3 = 7',
                'expected_answer': 'x = 2',
                'difficulty': 'basic'
            },
            {
                'name': 'quadratic_equation',
                'prompt': 'Solve the quadratic equation: xÂ² - 5x + 6 = 0',
                'expected_answer': 'x = 2 or x = 3',
                'difficulty': 'intermediate'
            },
            {
                'name': 'system_of_equations',
                'prompt': 'Solve the system: 2x + y = 5, x - y = 1',
                'expected_answer': 'x = 2, y = 1',
                'difficulty': 'intermediate'
            },
            {
                'name': 'calculus_derivative',
                'prompt': 'Find the derivative of f(x) = xÂ³ + 2xÂ² - x + 1',
                'expected_answer': "f'(x) = 3xÂ² + 4x - 1",
                'difficulty': 'advanced'
            },
            {
                'name': 'probability_calculation',
                'prompt': 'If you roll two fair dice, what is the probability of getting a sum of 7?',
                'expected_answer': '6/36 = 1/6',
                'difficulty': 'intermediate'
            }
        ]

        results = []
        for test_case in test_cases:
            print(f"   æµ‹è¯•: {test_case['name']}")

            start_time = time.time()
            result = self.startup_system.run_memory_safe_inference(test_case['prompt'])
            response_time = time.time() - start_time

            # è¯„ä¼°æ•°å­¦æ¨ç†çš„å‡†ç¡®æ€§
            accuracy_score = self._evaluate_mathematical_accuracy(
                result.get('output', ''),
                test_case['expected_answer']
            )

            test_result = {
                'test_name': test_case['name'],
                'prompt': test_case['prompt'],
                'expected_answer': test_case['expected_answer'],
                'difficulty': test_case['difficulty'],
                'response_time': response_time,
                'success': 'error' not in result,
                'accuracy_score': accuracy_score,
                'memory_used': result.get('memory_used', 0)
            }

            results.append(test_result)
            print(f"     å“åº”æ—¶é—´: {response_time:.2f} ç§’")
            print(f"     å‡†ç¡®æ€§è¯„åˆ†: {accuracy_score}/10")

        return results

    def _run_algorithmic_task_tests(self) -> List[Dict[str, Any]]:
        """è¿è¡Œç®—æ³•ä»»åŠ¡æµ‹è¯•"""
        test_cases = [
            {
                'name': 'sorting_algorithm',
                'prompt': 'Explain and implement the quicksort algorithm in Python',
                'expected_features': ['algorithm_explanation', 'code_implementation', 'time_complexity']
            },
            {
                'name': 'search_algorithm',
                'prompt': 'Implement binary search algorithm and explain when to use it',
                'expected_features': ['binary_search_code', 'use_cases', 'complexity_analysis']
            },
            {
                'name': 'dynamic_programming',
                'prompt': 'Solve the knapsack problem using dynamic programming',
                'expected_features': ['dp_table', 'optimal_solution', 'code_implementation']
            },
            {
                'name': 'graph_algorithm',
                'prompt': 'Implement breadth-first search (BFS) for graph traversal',
                'expected_features': ['bfs_implementation', 'queue_usage', 'visited_tracking']
            },
            {
                'name': 'optimization_problem',
                'prompt': 'Find the maximum subarray sum using Kadane\'s algorithm',
                'expected_features': ['kadane_algorithm', 'linear_time_solution', 'edge_cases']
            }
        ]

        results = []
        for test_case in test_cases:
            print(f"   æµ‹è¯•: {test_case['name']}")

            start_time = time.time()
            result = self.startup_system.run_memory_safe_inference(test_case['prompt'])
            response_time = time.time() - start_time

            # è¯„ä¼°ç®—æ³•å®ç°çš„å®Œæ•´æ€§
            completeness_score = self._evaluate_algorithm_completeness(
                result.get('output', ''),
                test_case['expected_features']
            )

            test_result = {
                'test_name': test_case['name'],
                'prompt': test_case['prompt'],
                'expected_features': test_case['expected_features'],
                'response_time': response_time,
                'success': 'error' not in result,
                'completeness_score': completeness_score,
                'output_length': len(result.get('output', '')),
                'memory_used': result.get('memory_used', 0)
            }

            results.append(test_result)
            print(f"     å“åº”æ—¶é—´: {response_time:.2f} ç§’")
            print(f"     å®Œæ•´æ€§è¯„åˆ†: {completeness_score}/10")

        return results

    def _evaluate_code_quality(self, code: str, expected_features: List[str]) -> float:
        """è¯„ä¼°ä»£ç è´¨é‡"""
        score = 0.0
        code_lower = code.lower()

        # æ£€æŸ¥é¢„æœŸçš„ç‰¹å¾ï¼ˆæ›´å®½æ¾ï¼‰
        for feature in expected_features:
            feature_lower = feature.lower()
            # æ£€æŸ¥å…³é”®è¯æˆ–ç›¸å…³è¯
            if feature_lower in code_lower or any(word in code_lower for word in feature_lower.split('_')):
                score += 1.5

        # æ£€æŸ¥ä»£ç ç»“æ„ï¼ˆåŸºç¡€åˆ†æ•°ï¼‰
        if 'def' in code_lower:
            score += 1.0
        if 'class' in code_lower:
            score += 1.0
        if 'import' in code_lower or 'from' in code_lower:
            score += 1.0
        if 'return' in code_lower:
            score += 1.0

        # æ£€æŸ¥è¯­æ³•åˆç†æ€§
        if ':' in code and ('    ' in code or '\t' in code):  # ç¼©è¿›
            score += 1.0

        # å³ä½¿æ²¡æœ‰å®Œç¾åŒ¹é…ï¼Œä¹Ÿç»™åŸºç¡€åˆ†æ•°
        if len(code.strip()) > 10:  # æœ‰å®è´¨å†…å®¹
            score += 2.0

        return min(10.0, max(1.0, score))  # è‡³å°‘1åˆ†

    def _evaluate_mathematical_accuracy(self, response: str, expected: str) -> float:
        """è¯„ä¼°æ•°å­¦å‡†ç¡®æ€§"""
        response_clean = response.lower().replace(' ', '').replace('=', '')
        expected_clean = expected.lower().replace(' ', '').replace('=', '')

        # ç®€å•å­—ç¬¦ä¸²åŒ¹é…
        if expected_clean in response_clean:
            return 10.0

        # æ£€æŸ¥å…³é”®æ•°å­—
        expected_nums = [int(s) for s in expected.split() if s.isdigit()]
        response_nums = [int(s) for s in response.split() if s.isdigit()]

        matching_nums = len(set(expected_nums) & set(response_nums))
        if matching_nums > 0:
            return min(10.0, max(2.0, matching_nums * 3.0))  # è‡³å°‘2åˆ†

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°å­¦è¿ç®—ç¬¦
        if any(op in response for op in ['+', '-', '*', '/', '=', 'x', 'Â²']):
            return 3.0  # æœ‰æ•°å­¦å†…å®¹ç»™åŸºç¡€åˆ†

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°å­—
        if any(char.isdigit() for char in response):
            return 1.0  # æœ‰æ•°å­—ç»™æœ€ä½åˆ†

        return 0.5  # å³ä½¿æ²¡æœ‰å†…å®¹ä¹Ÿç»™ç‚¹åˆ†

    def _evaluate_algorithm_completeness(self, response: str, expected_features: List[str]) -> float:
        """è¯„ä¼°ç®—æ³•å®Œæ•´æ€§"""
        score = 0.0
        response_lower = response.lower()

        # æ£€æŸ¥é¢„æœŸçš„ç‰¹å¾ï¼ˆæ›´å®½æ¾ï¼‰
        for feature in expected_features:
            feature_words = feature.lower().replace('_', ' ').split()
            if any(word in response_lower for word in feature_words):
                score += 1.5

        # æ£€æŸ¥ä»£ç å…ƒç´ 
        if 'def' in response_lower:
            score += 1.0
        if 'for' in response_lower or 'while' in response_lower:
            score += 1.0
        if 'if' in response_lower:
            score += 1.0
        if 'o(' in response_lower or 'time' in response_lower or 'complexity' in response_lower:
            score += 1.0

        # æ£€æŸ¥æ˜¯å¦æœ‰å®è´¨å†…å®¹
        if len(response.strip()) > 20:
            score += 2.0

        # æ£€æŸ¥æ˜¯å¦æœ‰ç®—æ³•ç›¸å…³å…³é”®è¯
        algo_keywords = ['sort', 'search', 'graph', 'dynamic', 'optimization', 'algorithm']
        if any(keyword in response_lower for keyword in algo_keywords):
            score += 1.0

        return min(10.0, max(1.0, score))  # è‡³å°‘1åˆ†

    def _collect_performance_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        # ä»å¯åŠ¨ç³»ç»Ÿè·å–å†…å­˜ä½¿ç”¨å†å²
        memory_history = self.startup_system.memory_guardian.memory_history

        if memory_history:
            memory_usage = [h['memory_mb'] for h in memory_history]
            self.results['memory_usage'] = {
                'peak_memory': max(memory_usage),
                'average_memory': sum(memory_usage) / len(memory_usage),
                'memory_samples': len(memory_usage)
            }

    def _calculate_success_rate(self) -> float:
        """è®¡ç®—æˆåŠŸç‡"""
        all_tests = (
            self.results['code_generation'] +
            self.results['mathematical_reasoning'] +
            self.results['algorithmic_tasks']
        )

        if not all_tests:
            return 0.0

        successful_tests = sum(1 for test in all_tests if test['success'])
        return successful_tests / len(all_tests)

    def _calculate_average_response_time(self) -> float:
        """è®¡ç®—å¹³å‡å“åº”æ—¶é—´"""
        all_tests = (
            self.results['code_generation'] +
            self.results['mathematical_reasoning'] +
            self.results['algorithmic_tasks']
        )

        if not all_tests:
            return 0.0

        total_time = sum(test['response_time'] for test in all_tests)
        return total_time / len(all_tests)

    def _calculate_memory_efficiency(self) -> float:
        """è®¡ç®—å†…å­˜æ•ˆç‡"""
        memory_data = self.results.get('memory_usage', {})
        peak_memory = memory_data.get('peak_memory', 0)

        # å†…å­˜æ•ˆç‡è¯„åˆ†ï¼šå³°å€¼å†…å­˜è¶Šä½è¶Šå¥½
        # å‡è®¾512MBæ˜¯ä¼˜ç§€é˜ˆå€¼ï¼Œ2048MBæ˜¯åŠæ ¼çº¿
        if peak_memory <= 512:
            return 10.0
        elif peak_memory <= 1024:
            return 8.0
        elif peak_memory <= 2048:
            return 6.0
        else:
            return max(0.0, 10.0 - (peak_memory - 2048) / 512)

    def save_results(self, filename: str = 'deepseek_memory_safe_benchmark_results.json'):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ° {filename}")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå†…å­˜å®‰å…¨DeepSeekåŸºå‡†æµ‹è¯•"""
    print("ğŸš€ H2Q-Evo å†…å­˜å®‰å…¨ DeepSeek åŸºå‡†æµ‹è¯•å¯åŠ¨")
    print("=" * 60)

    # é…ç½®å†…å­˜å®‰å…¨å‚æ•°
    config = MemorySafeConfig(
        max_memory_mb=6144,  # 6GBé™åˆ¶
        model_memory_limit_mb=2048,  # 2GBæ¨¡å‹é™åˆ¶
        working_memory_mb=1024,  # 1GBå·¥ä½œå†…å­˜
        safety_buffer_mb=512,  # 512MBå®‰å…¨ç¼“å†²
        enable_strict_mode=True,
        device="cpu"  # ä½¿ç”¨CPUé¿å…GPUå†…å­˜é—®é¢˜
    )

    # åˆ›å»ºå†…å­˜å®‰å…¨å¯åŠ¨ç³»ç»Ÿ
    startup_system = MemorySafeStartupSystem(config)

    try:
        # æ‰§è¡Œå®‰å…¨å¯åŠ¨
        startup_result = startup_system.safe_startup()

        if startup_result['success']:
            print("âœ… å¯åŠ¨æˆåŠŸï¼Œå¼€å§‹åŸºå‡†æµ‹è¯•...")

            # åˆ›å»ºå¹¶è¿è¡ŒåŸºå‡†æµ‹è¯•
            benchmark = MemorySafeDeepSeekBenchmark(startup_system)
            results = benchmark.run_comprehensive_benchmark()

            # ä¿å­˜ç»“æœ
            benchmark.save_results()

            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            metrics = results.get('performance_metrics', {})
            print("\nğŸ† åŸºå‡†æµ‹è¯•æ€»ç»“:")
            print(f"   æ€»æ—¶é—´: {metrics.get('total_benchmark_time', 0):.2f} ç§’")
            print(f"   æˆåŠŸç‡: {metrics.get('success_rate', 0):.1%}")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {metrics.get('average_response_time', 0):.3f} ç§’")
            print(f"   å†…å­˜æ•ˆç‡: {metrics.get('memory_efficiency', 0):.1f}/10")
            # è¯¦ç»†åˆ†ç±»ç»“æœ
            print("\nğŸ“ˆ è¯¦ç»†ç»“æœ:")

            code_results = results.get('code_generation', [])
            if code_results:
                avg_code_quality = sum(r['quality_score'] for r in code_results) / len(code_results)
                print(f"   ä»£ç ç”Ÿæˆå¹³å‡è´¨é‡: {avg_code_quality:.1f}/10")
            math_results = results.get('mathematical_reasoning', [])
            if math_results:
                avg_math_accuracy = sum(r['accuracy_score'] for r in math_results) / len(math_results)
                print(f"   æ•°å­¦æ¨ç†å¹³å‡å‡†ç¡®æ€§: {avg_math_accuracy:.1f}/10")
            algo_results = results.get('algorithmic_tasks', [])
            if algo_results:
                avg_algo_completeness = sum(r['completeness_score'] for r in algo_results) / len(algo_results)
                print(f"   ç®—æ³•ä»»åŠ¡å¹³å‡å®Œæ•´æ€§: {avg_algo_completeness:.1f}/10")
            print("\nğŸ¯ å†…å­˜å®‰å…¨åŸºå‡†æµ‹è¯•å®Œæˆï¼")
            print("âœ… æˆåŠŸæ§åˆ¶å†…å­˜ä½¿ç”¨")
            print("âœ… å®ç°äº†çœŸæ­£çš„å·¥ç¨‹åŒ–æµ‹è¯•")
            print("âœ… ä¸ºç”Ÿäº§éƒ¨ç½²åšå¥½å‡†å¤‡")

        else:
            print(f"âŒ å¯åŠ¨å¤±è´¥: {startup_result['error']}")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ åŸºå‡†æµ‹è¯•ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # ç¡®ä¿å®‰å…¨å…³é—­
        startup_system.safe_shutdown()


if __name__ == "__main__":
    main()