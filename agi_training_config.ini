# H2Q-Evo AGI持久化训练配置文件
# 这个文件定义了AGI系统的训练和进化参数

[training]
# 基础模型配置
base_model_name = "microsoft/DialoGPT-medium"
max_seq_length = 512
vocab_size = 50257

# 训练超参数
num_epochs = 100
batch_size = 4
gradient_accumulation_steps = 4
learning_rate = 0.00002
weight_decay = 0.01
warmup_steps = 100

# 检查点和评估
save_steps = 500
eval_steps = 250
logging_steps = 50
save_total_limit = 3

[optimization]
# 内存和性能优化
max_memory_gb = 8.0
use_gradient_checkpointing = true
use_mixed_precision = true

# LoRA配置
lora_r = 16
lora_alpha = 32
lora_dropout = 0.05
lora_target_modules = ["c_attn", "c_proj", "c_fc"]

[evolution]
# 进化配置
evolution_interval_hours = 24
generation_limit = 1000
fitness_threshold = 0.8

# 数据增强
enable_data_augmentation = true
augmentation_probability = 0.3

[monitoring]
# 监控配置
enable_wandb = true
wandb_project = "h2q-evo-persistent-agi"
log_level = "INFO"

# 健康检查
health_check_interval = 300
memory_warning_threshold = 0.8
memory_critical_threshold = 0.9

[data]
# 数据配置
data_sources = ["mathematical_reasoning", "code_generation", "conversation", "creative_writing"]
samples_per_source = 1000
validation_split = 0.1

# 流形编码配置
encoding_resolution = 0.01
compression_layers = 5

[manifold_encoding]
# 对数化流形编码配置
resolution = 0.01
manifold_dim = 3
spacetime_dim = 4
log_base = 2.718281828459045
encoding_scale = 100.0
fixed_point_tolerance = 0.001
max_fixed_point_iterations = 50

[system]
# 系统配置
project_root = "./agi_persistent_training"
checkpoint_dir = "${project_root}/checkpoints"
log_dir = "${project_root}/logs"
data_dir = "${project_root}/data"
evolution_state_file = "${project_root}/evolution_state.json"

# 并发配置
num_worker_threads = 4
max_concurrent_evaluations = 2

[experimental]
# 实验性功能
enable_quantum_inspired_training = false
enable_neuromorphic_computing = false
enable_self_modifying_code = false

# 高级进化策略
use_genetic_algorithms = false
use_neuroevolution = false
use_curriculum_learning = true