# train_decoder.py

import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import os

from tools.byte_loader import get_byte_dataloader
from h2q.hierarchical_system import H2Q_Hierarchical_System
from h2q.hierarchical_decoder import ConceptDecoder

# --- é…ç½® ---
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
BATCH_SIZE = 32
SEQ_LEN = 256 # å­—ç¬¦é•¿åº¦
LR = 1e-3
STEPS = 2000
SPELLING_WEIGHTS = "h2q_model_knot.pth"
HIERARCHY_WEIGHTS = "h2q_model_hierarchy.pth" # ä¸Šä¸€æ­¥ä¿å­˜çš„å±‚çº§æƒé‡

def train_decoder():
    print(f"ğŸš€ [H2Q-Decoder] å¯åŠ¨æ¦‚å¿µè§£ç è®­ç»ƒ... è®¾å¤‡: {DEVICE}")
    
    # 1. åŠ è½½å·²è®­ç»ƒå¥½çš„å±‚çº§ç³»ç»Ÿ (Encoder)
    encoder = H2Q_Hierarchical_System(vocab_size=257, dim=256, spelling_weights_path=SPELLING_WEIGHTS)
    
    # åŠ è½½ L1 æ¦‚å¿µå±‚æƒé‡
    if os.path.exists(HIERARCHY_WEIGHTS):
        print(f"ğŸ§Š åŠ è½½æ¦‚å¿µå±‚æƒé‡: {HIERARCHY_WEIGHTS}")
        encoder.load_state_dict(torch.load(HIERARCHY_WEIGHTS), strict=False)
    else:
        print("âš ï¸ æœªæ‰¾åˆ°æ¦‚å¿µå±‚æƒé‡ï¼Œè§£ç å™¨å°†åŸºäºéšæœºæ¦‚å¿µè¿›è¡Œè®­ç»ƒï¼ˆæ•ˆæœä¼šå·®ï¼‰")
        
    encoder.to(DEVICE)
    encoder.eval() # ç¼–ç å™¨å…¨å†»ç»“ï¼
    
    # 2. åˆå§‹åŒ–è§£ç å™¨ (Decoder)
    decoder = ConceptDecoder(dim=256, vocab_size=257, stride=8)
    decoder.to(DEVICE)
    
    # 3. æ•°æ®
    train_loader = get_byte_dataloader(split="train", batch_size=BATCH_SIZE, seq_len=SEQ_LEN)
    
    # 4. ä¼˜åŒ–å™¨ (åªä¼˜åŒ–è§£ç å™¨)
    optimizer = optim.AdamW(decoder.parameters(), lr=LR)
    
    # 5. è®­ç»ƒå¾ªç¯
    progress_bar = tqdm(range(STEPS), desc="Decoder Training")
    data_iter = iter(train_loader)
    
    for step in progress_bar:
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(train_loader)
            batch = next(data_iter)
            
        inputs = batch.to(DEVICE) # [B, 256]
        
        # --- Encoder å‰å‘ (è·å–æ¦‚å¿µ) ---
        with torch.no_grad():
            # æˆ‘ä»¬åªéœ€è¦ concept_stream (çœŸå®çš„æ¦‚å¿µæµ)ï¼Œä¸éœ€è¦ pred_concepts
            _, concept_stream = encoder(inputs) 
            # concept_stream: [B, 32, 256]
            
        # --- Decoder å‰å‘ (é‡æ„å­—ç¬¦) ---
        # æˆ‘ä»¬è¯•å›¾ä»æ¦‚å¿µæµè¿˜åŸå›åŸå§‹çš„ inputs
        recon_logits = decoder(concept_stream) # [B, 256, 257]
        
        # --- é‡æ„æŸå¤± ---
        loss = nn.CrossEntropyLoss()(recon_logits.reshape(-1, 257), inputs.reshape(-1))
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        progress_bar.set_postfix({"ReconLoss": f"{loss.item():.4f}"})
        
        # --- ç”Ÿæˆæ¼”ç¤º (éªŒè¯è§£ç èƒ½åŠ›) ---
        if step % 500 == 0 and step > 0:
            # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å‰ 32 ä¸ªå­—ç¬¦ (4ä¸ªæ¦‚å¿µ) è¿›è¡Œå±•ç¤º
            orig_bytes = inputs[0, :32].cpu().tolist()
            
            # è§£ç é¢„æµ‹
            pred_probs = torch.softmax(recon_logits[0, :32], dim=-1)
            pred_bytes = torch.argmax(pred_probs, dim=-1).cpu().tolist()
            
            try:
                orig_str = bytes(orig_bytes).decode('utf-8', errors='ignore').replace('\n', ' ')
                pred_str = bytes(pred_bytes).decode('utf-8', errors='ignore').replace('\n', ' ')
                tqdm.write(f"\nğŸ” [Step {step}]")
                tqdm.write(f"   åŸæ–‡: {orig_str}")
                tqdm.write(f"   é‡æ„: {pred_str}")
            except: pass

    print("âœ… è§£ç å™¨è®­ç»ƒå®Œæˆã€‚")
    torch.save(decoder.state_dict(), "h2q_model_decoder.pth")

if __name__ == "__main__":
    train_decoder()