#!/usr/bin/env python3
"""
æ€§èƒ½å®£ç§°å®¡è®¡éªŒè¯è„šæœ¬
Audit script for verifying README performance claims
"""

import torch
import torch.nn as nn
import time
import psutil
import tracemalloc
import numpy as np
from pathlib import Path
import json
import sys
import os

sys.path.insert(0, str(Path(__file__).parent))

print("="*80)
print("H2Q-Evo æ€§èƒ½å®£ç§°å®¡è®¡éªŒè¯ (Performance Claims Audit)")
print("="*80)
print(f"æ‰§è¡Œæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Pythonç‰ˆæœ¬: {sys.version}")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"è®¾å¤‡: {'MPS' if torch.backends.mps.is_available() else 'CPU'}")
print("="*80)

results = {
    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'claims': {}
}

# ============================================================================
# å®£ç§°1: 706K tok/s è®­ç»ƒåå
# Claim 1: 706K tokens/sec training throughput
# ============================================================================
print("\n[å®£ç§°1] è®­ç»ƒåå: 706K tokens/sec")
print("-"*80)

def test_training_throughput():
    """æµ‹è¯•è®­ç»ƒååé‡"""
    device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
    
    # ç®€å•Transformer-likeæ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self, vocab_size=50000, dim=256, seq_len=64):
            super().__init__()
            self.embed = nn.Embedding(vocab_size, dim)
            self.transformer = nn.TransformerEncoderLayer(d_model=dim, nhead=8, batch_first=True)
            self.head = nn.Linear(dim, vocab_size)
            
        def forward(self, x):
            h = self.embed(x)
            h = self.transformer(h)
            return self.head(h)
    
    batch_size = 64
    seq_len = 64
    vocab_size = 50000
    
    model = SimpleModel(vocab_size, dim=256, seq_len=seq_len).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()
    
    # é¢„çƒ­
    for _ in range(5):
        x = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)
        y = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)
        out = model(x)
        loss = criterion(out.view(-1, vocab_size), y.view(-1))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # æ­£å¼æµ‹è¯•
    iterations = 100
    torch.mps.synchronize() if hasattr(torch.backends, 'mps') else None
    
    start = time.perf_counter()
    for _ in range(iterations):
        x = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)
        y = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)
        out = model(x)
        loss = criterion(out.view(-1, vocab_size), y.view(-1))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    torch.mps.synchronize() if hasattr(torch.backends, 'mps') else None
    elapsed = time.perf_counter() - start
    
    total_tokens = batch_size * seq_len * iterations
    throughput = total_tokens / elapsed
    
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  åºåˆ—é•¿åº¦: {seq_len}")
    print(f"  è¿­ä»£æ¬¡æ•°: {iterations}")
    print(f"  æ€»è€—æ—¶: {elapsed:.2f}s")
    print(f"  å®é™…åå: {throughput:.0f} tokens/sec")
    print(f"  å®£ç§°åå: 706,000 tokens/sec")
    print(f"  è¾¾æˆç‡: {(throughput/706000)*100:.1f}%")
    
    if throughput < 706000:
        print(f"  âŒ æœªè¾¾åˆ°å®£ç§°å€¼ (å·®è·: {706000-throughput:.0f} tokens/sec)")
    else:
        print(f"  âœ… è¾¾åˆ°å®£ç§°å€¼")
    
    return {
        'claimed': 706000,
        'actual': float(throughput),
        'achievement_rate': float((throughput/706000)*100),
        'verified': 'yes' if throughput >= 706000 else 'no'
    }

try:
    results['claims']['training_throughput'] = test_training_throughput()
except Exception as e:
    print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
    results['claims']['training_throughput'] = {'error': str(e)}

# ============================================================================
# å®£ç§°2: 23.68Î¼s æ¨ç†å»¶è¿Ÿ
# Claim 2: 23.68 microseconds inference latency
# ============================================================================
print("\n[å®£ç§°2] æ¨ç†å»¶è¿Ÿ: 23.68Î¼s (per token)")
print("-"*80)

def test_inference_latency():
    """æµ‹è¯•å•tokenæ¨ç†å»¶è¿Ÿ"""
    device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
    
    # è½»é‡çº§æ¨ç†æ¨¡å‹
    class LightweightModel(nn.Module):
        def __init__(self, vocab_size=50000, dim=256):
            super().__init__()
            self.embed = nn.Embedding(vocab_size, dim)
            self.fc1 = nn.Linear(dim, dim)
            self.fc2 = nn.Linear(dim, vocab_size)
            
        def forward(self, x):
            h = self.embed(x)  # (B, 1, dim)
            h = torch.relu(self.fc1(h))
            return self.fc2(h)
    
    model = LightweightModel().to(device)
    model.eval()
    
    # å•tokenè¾“å…¥
    x = torch.randint(0, 50000, (1, 1)).to(device)
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(100):
            _ = model(x)
    
    # æ­£å¼æµ‹è¯•
    iterations = 1000
    torch.mps.synchronize() if hasattr(torch.backends, 'mps') else None
    
    latencies = []
    with torch.no_grad():
        for _ in range(iterations):
            start = time.perf_counter()
            _ = model(x)
            torch.mps.synchronize() if hasattr(torch.backends, 'mps') else None
            latencies.append((time.perf_counter() - start) * 1e6)  # è½¬æ¢ä¸ºå¾®ç§’
    
    avg_latency = np.mean(latencies)
    p50_latency = np.percentile(latencies, 50)
    p99_latency = np.percentile(latencies, 99)
    
    print(f"  è¿­ä»£æ¬¡æ•°: {iterations}")
    print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}Î¼s")
    print(f"  P50å»¶è¿Ÿ: {p50_latency:.2f}Î¼s")
    print(f"  P99å»¶è¿Ÿ: {p99_latency:.2f}Î¼s")
    print(f"  å®£ç§°å»¶è¿Ÿ: 23.68Î¼s")
    
    if avg_latency > 23.68:
        print(f"  âŒ è¶…å‡ºå®£ç§°å€¼ (æ…¢äº†: {avg_latency-23.68:.2f}Î¼s)")
    else:
        print(f"  âœ… ä¼˜äºå®£ç§°å€¼")
    
    return {
        'claimed': 23.68,
        'actual_mean': float(avg_latency),
        'actual_p50': float(p50_latency),
        'actual_p99': float(p99_latency),
        'verified': 'yes' if avg_latency <= 23.68 else 'no'
    }

try:
    results['claims']['inference_latency'] = test_inference_latency()
except Exception as e:
    print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
    results['claims']['inference_latency'] = {'error': str(e)}

# ============================================================================
# å®£ç§°3: 0.7MB å³°å€¼å†…å­˜
# Claim 3: 0.7MB peak memory
# ============================================================================
print("\n[å®£ç§°3] å³°å€¼å†…å­˜: 0.7MB")
print("-"*80)

def test_peak_memory():
    """æµ‹è¯•å³°å€¼å†…å­˜ä½¿ç”¨"""
    device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
    
    # å¯åŠ¨å†…å­˜è¿½è¸ª
    tracemalloc.start()
    
    # åˆ›å»ºè½»é‡çº§æ¨¡å‹
    class TinyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(128, 128)
            self.fc2 = nn.Linear(128, 10)
            
        def forward(self, x):
            return self.fc2(torch.relu(self.fc1(x)))
    
    model = TinyModel().to(device)
    
    # è¿è¡Œæ¨ç†
    x = torch.randn(1, 128).to(device)
    with torch.no_grad():
        _ = model(x)
    
    # è·å–å†…å­˜ç»Ÿè®¡
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    # è·å–è¿›ç¨‹å†…å­˜
    process = psutil.Process()
    process_mem = process.memory_info().rss / (1024 * 1024)  # MB
    
    peak_mb = peak / (1024 * 1024)
    
    print(f"  Pythonå¯¹è±¡å³°å€¼: {peak_mb:.2f}MB")
    print(f"  è¿›ç¨‹æ€»å†…å­˜: {process_mem:.2f}MB")
    print(f"  å®£ç§°å³°å€¼: 0.7MB")
    
    if peak_mb > 0.7:
        print(f"  âŒ è¶…å‡ºå®£ç§°å€¼ (å¤šç”¨äº†: {peak_mb-0.7:.2f}MB)")
        print(f"  æ³¨: 0.7MBæå°,å®é™…Pythonè¿è¡Œæ—¶å°±éœ€çº¦10-50MBåŸºç¡€å¼€é”€")
    else:
        print(f"  âœ… ç¬¦åˆå®£ç§°å€¼")
    
    return {
        'claimed': 0.7,
        'actual_peak_mb': float(peak_mb),
        'process_memory_mb': float(process_mem),
        'verified': 'yes' if peak_mb <= 0.7 else 'no',
        'note': '0.7MBä»…ä¸ºæ¨¡å‹å‚æ•°,ä¸å«Pythonè¿è¡Œæ—¶åŸºç¡€å†…å­˜'
    }

try:
    results['claims']['peak_memory'] = test_peak_memory()
except Exception as e:
    print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
    results['claims']['peak_memory'] = {'error': str(e)}

# ============================================================================
# å®£ç§°4: CIFAR-10 88.78% å‡†ç¡®ç‡
# Claim 4: CIFAR-10 88.78% accuracy
# ============================================================================
print("\n[å®£ç§°4] CIFAR-10å‡†ç¡®ç‡: 88.78%")
print("-"*80)
print("  âš ï¸  å®Œæ•´è®­ç»ƒéœ€è¦è¾ƒé•¿æ—¶é—´(çº¦30åˆ†é’Ÿ-2å°æ—¶)")
print("  æç¤º: å¯å•ç‹¬è¿è¡Œ benchmarks/cifar10_classification.py --epochs 10")
print("  å½“å‰: ä»…æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨")

cifar_script = Path(__file__).parent / "benchmarks" / "cifar10_classification.py"
if cifar_script.exists():
    print(f"  âœ… è®­ç»ƒè„šæœ¬å­˜åœ¨: {cifar_script}")
    print("  ğŸ“ éœ€æ‰‹åŠ¨è¿è¡ŒéªŒè¯:")
    print("      PYTHONPATH=. python3 h2q_project/benchmarks/cifar10_classification.py --epochs 10")
    results['claims']['cifar10_accuracy'] = {
        'claimed': 88.78,
        'script_exists': 'yes',  # æ”¹ä¸ºå­—ç¬¦ä¸²
        'verified': 'no',  # æ”¹ä¸ºå­—ç¬¦ä¸²
        'note': 'éœ€æ‰‹åŠ¨è¿è¡Œå®Œæ•´è®­ç»ƒéªŒè¯ (çº¦1-2å°æ—¶)',
        'command': 'PYTHONPATH=. python3 h2q_project/benchmarks/cifar10_classification.py --epochs 10'
    }
else:
    print(f"  âŒ è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨")
    results['claims']['cifar10_accuracy'] = {
        'claimed': 88.78,
        'script_exists': 'no',  # æ”¹ä¸ºå­—ç¬¦ä¸²
        'verified': 'no'  # æ”¹ä¸ºå­—ç¬¦ä¸²
    }

# ============================================================================
# æ€»ç»“
# ============================================================================
print("\n" + "="*80)
print("å®¡è®¡æ€»ç»“ (Audit Summary)")
print("="*80)

verified_count = sum(1 for claim in results['claims'].values() 
                     if isinstance(claim, dict) and claim.get('verified', 'no') == 'yes')
total_testable = sum(1 for claim in results['claims'].values() 
                     if isinstance(claim, dict) and 'verified' in claim)

print(f"\né€šè¿‡éªŒè¯: {verified_count}/{total_testable}")

for name, data in results['claims'].items():
    if isinstance(data, dict) and 'verified' in data:
        status = "âœ… éªŒè¯é€šè¿‡" if data['verified'] == 'yes' else "âŒ æœªé€šè¿‡éªŒè¯"
        print(f"  {name}: {status}")
        if 'actual' in data and 'claimed' in data:
            print(f"    å®£ç§°: {data['claimed']}, å®æµ‹: {data['actual']:.2f}")

# ä¿å­˜ç»“æœ
output_file = Path(__file__).parent.parent / "performance_audit_results.json"
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜: {output_file}")
print("="*80)
