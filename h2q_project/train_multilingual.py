# train_multilingual.py

import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
from h2q.system import AutonomousSystem
from h2q.hierarchical_decoder import ConceptDecoder
from tools.byte_loader import get_byte_dataloader
import os

# --- é…ç½® ---
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
BATCH_SIZE = 32
SEQ_LEN = 256
LR = 3e-4
STEPS = 3000
VOCAB_SIZE = 257
CORPUS_PATH = "mix_corpus.txt"

# ä¿å­˜è·¯å¾„ (è¦†ç›–æ—§çš„ï¼Œæˆ–è€…å­˜æ–°çš„)
KNOT_PATH = "h2q_model_knot.pth"
DECODER_PATH = "h2q_model_decoder.pth"

def train():
    print(f"ğŸš€ [H2Q-Babel] å¯åŠ¨å¤šè¯­è¨€æ··åˆè®­ç»ƒ... è®¾å¤‡: {DEVICE}")
    
    # 1. ç”Ÿæˆè¯­æ–™
    if not os.path.exists(CORPUS_PATH):
        from tools.mix_corpus_generator import generate_mix_corpus
        generate_mix_corpus()
    
    # 2. åˆå§‹åŒ–ç³»ç»Ÿ (L0 + Decoder)
    # æˆ‘ä»¬è¿™æ¬¡åŒæ—¶è®­ç»ƒ L0 å’Œ Decoderï¼Œä¸ºäº†å¿«é€Ÿé€‚é…
    system = AutonomousSystem(context_dim=256, action_dim=256)
    
    from h2q.knot_kernel import H2Q_Knot_Kernel
    system.dde.kernel = H2Q_Knot_Kernel(max_dim=256, vocab_size=VOCAB_SIZE, depth=6)
    
    decoder = ConceptDecoder(dim=256, vocab_size=VOCAB_SIZE, stride=1) # æ³¨æ„ï¼šå…ˆè®­ç»ƒ1:1è¿˜åŸ
    # ä¸ºäº†æ”¯æŒå¤šè¯­è¨€ï¼Œæˆ‘ä»¬å…ˆè®­ç»ƒ L0 çš„è‡ªç¼–ç èƒ½åŠ› (ä¸å‹ç¼©)ï¼Œç¡®ä¿ L0 èƒ½çœ‹æ‡‚ä¸­æ–‡
    # ç­‰ L0 ç¨³å®šäº†ï¼Œå†åšå‹ç¼©è®­ç»ƒã€‚è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç›´æ¥è®­ç»ƒ L0 + Decoder(Stride=1)
    
    system.dde.to(DEVICE)
    decoder.to(DEVICE)
    
    # 3. æ•°æ®
    train_loader = get_byte_dataloader(file_path=CORPUS_PATH, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)
    
    # 4. ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(list(system.dde.parameters()) + list(decoder.parameters()), lr=LR)
    
    progress_bar = tqdm(range(STEPS), desc="Multilingual Training")
    data_iter = iter(train_loader)
    
    for step in progress_bar:
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(train_loader)
            batch = next(data_iter)
            
        inputs = batch.to(DEVICE)
        
        # L0 ç¼–ç  (è·å–ç‰¹å¾)
        features, stab_loss = system.dde.kernel(inputs, return_features=True)
        
        # Decoder è§£ç 
        logits = decoder(features)
        
        # æŸå¤±
        recon_loss = nn.CrossEntropyLoss()(logits.reshape(-1, VOCAB_SIZE), inputs.reshape(-1))
        total_loss = recon_loss + 0.1 * stab_loss
        
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        progress_bar.set_postfix({"Recon": f"{recon_loss.item():.4f}"})
        
        if step % 500 == 0:
             # éªŒè¯ä¸­æ–‡è¿˜åŸ
            orig_bytes = inputs[0, :20].cpu().tolist()
            pred_bytes = torch.argmax(logits[0, :20], dim=-1).cpu().tolist()
            try:
                print(f"\nåŸæ–‡: {bytes(orig_bytes).decode('utf-8', 'ignore')}")
                print(f"è¿˜åŸ: {bytes(pred_bytes).decode('utf-8', 'ignore')}")
            except: pass

    print("âœ… å¤šè¯­è¨€è®­ç»ƒå®Œæˆã€‚")
    torch.save(system.dde.kernel.state_dict(), KNOT_PATH)
    # æ³¨æ„ï¼šè¿™é‡Œçš„ Decoder æ˜¯ Stride=1 çš„ï¼Œä»…ç”¨äºéªŒè¯ L0 çš„è¡¨è¾¾èƒ½åŠ›
    # å®é™…ä½¿ç”¨ä¸­ï¼ŒL0 è®­ç»ƒå¥½åï¼Œéœ€è¦é‡æ–°è®­ç»ƒ Stride=8 çš„ Decoder
    
if __name__ == "__main__":
    train()