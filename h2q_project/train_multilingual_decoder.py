# train_multilingual_decoder.py

import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import os

from tools.byte_loader import get_byte_dataloader
from h2q.hierarchical_system import H2Q_Hierarchical_System
from h2q.hierarchical_decoder import ConceptDecoder

# --- é…ç½® ---
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
BATCH_SIZE = 32
SEQ_LEN = 256
LR = 1e-3
STEPS = 3000 # å¤šè¯­è¨€éœ€è¦å¤šç»ƒä¸€ä¼šå„¿
VOCAB_SIZE = 257
CORPUS_PATH = "mix_corpus.txt" # [å…³é”®] å¿…é¡»ä½¿ç”¨æ··åˆè¯­æ–™

# æƒé‡è·¯å¾„
SPELLING_WEIGHTS = "h2q_model_knot.pth"
HIERARCHY_WEIGHTS = "h2q_model_hierarchy.pth"
DECODER_SAVE_PATH = "h2q_model_decoder.pth" # è¦†ç›–æ—§çš„è§£ç å™¨

def train_decoder():
    print(f"ğŸš€ [H2Q-Decoder] å¯åŠ¨å¤šè¯­è¨€è§£ç è®­ç»ƒ... è®¾å¤‡: {DEVICE}")
    
    # 1. åŠ è½½ç¼–ç å™¨ (L0 + L1) - å†»ç»“çŠ¶æ€
    encoder = H2Q_Hierarchical_System(vocab_size=257, dim=256, spelling_weights_path=SPELLING_WEIGHTS)
    
    if os.path.exists(HIERARCHY_WEIGHTS):
        print(f"ğŸ§Š åŠ è½½æ¦‚å¿µå±‚æƒé‡: {HIERARCHY_WEIGHTS}")
        encoder.load_state_dict(torch.load(HIERARCHY_WEIGHTS), strict=False)
    else:
        print("âŒ ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ°æ¦‚å¿µå±‚æƒé‡ï¼è¯·å…ˆè¿è¡Œ train_hierarchy.py")
        return
        
    encoder.to(DEVICE)
    encoder.eval() # ç¼–ç å™¨å…¨å†»ç»“
    
    # 2. åˆå§‹åŒ–è§£ç å™¨ (Stride=8, 3çº§å±•å¼€)
    decoder = ConceptDecoder(dim=256, vocab_size=257, stride=8)
    decoder.to(DEVICE)
    
    # 3. æ•°æ®åŠ è½½ (æ··åˆè¯­æ–™)
    train_loader = get_byte_dataloader(file_path=CORPUS_PATH, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)
    
    # 4. ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(decoder.parameters(), lr=LR)
    
    # 5. è®­ç»ƒå¾ªç¯
    progress_bar = tqdm(range(STEPS), desc="Multilingual Decoding")
    data_iter = iter(train_loader)
    
    for step in progress_bar:
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(train_loader)
            batch = next(data_iter)
            
        inputs = batch.to(DEVICE) # [B, 256]
        
        # --- Encoder å‰å‘ (è·å–æ¦‚å¿µæµ) ---
        with torch.no_grad():
            # concept_stream: [B, 32, 256]
            _, concept_stream = encoder(inputs) 
            
        # --- Decoder å‰å‘ (è¿˜åŸå­—ç¬¦) ---
        recon_logits = decoder(concept_stream) # [B, 256, 257]
        
        # --- æŸå¤± ---
        loss = nn.CrossEntropyLoss()(recon_logits.reshape(-1, VOCAB_SIZE), inputs.reshape(-1))
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        progress_bar.set_postfix({"Recon": f"{loss.item():.4f}"})
        
        # --- å®æ—¶éªŒè¯ (ä¸­è‹±æ··æ’) ---
        if step % 500 == 0 and step > 0:
            # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å‰ 48 ä¸ªå­—èŠ‚ (6ä¸ªæ¦‚å¿µ)
            orig_bytes = inputs[0, :48].cpu().tolist()
            pred_bytes = torch.argmax(recon_logits[0, :48], dim=-1).cpu().tolist()
            
            try:
                # ä½¿ç”¨ replace é¿å…æ¢è¡Œç¬¦ç ´åæ‰“å°æ ¼å¼
                orig_str = bytes(orig_bytes).decode('utf-8', errors='ignore').replace('\n', 'â')
                pred_str = bytes(pred_bytes).decode('utf-8', errors='ignore').replace('\n', 'â')
                tqdm.write(f"\nğŸ” [Step {step}]")
                tqdm.write(f"   åŸæ–‡: {orig_str}")
                tqdm.write(f"   è¿˜åŸ: {pred_str}")
            except: pass

    print("âœ… å¤šè¯­è¨€è§£ç å™¨è®­ç»ƒå®Œæˆã€‚")
    torch.save(decoder.state_dict(), DECODER_SAVE_PATH)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {DECODER_SAVE_PATH}")

if __name__ == "__main__":
    train_decoder()