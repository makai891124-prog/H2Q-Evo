# accelerate_gpt2.py

import torch
import time
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from h2q.system import AutonomousSystem

# --- é…ç½® ---
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

# æ³¨æ„ï¼šä¸ºäº†æ¼”ç¤ºé€Ÿåº¦ï¼Œæˆ‘ä»¬è¿™é‡Œä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œè€Œæ˜¯åˆå§‹åŒ–ä¸€ä¸ªéšæœºæ¨¡å‹
# å› ä¸ºè¦è®© Byte-Level æ¨¡å‹åŠ é€Ÿ Token-Level æ¨¡å‹éœ€è¦å¤æ‚çš„å¯¹é½è®­ç»ƒ
# è¿™é‡Œæˆ‘ä»¬ä¸»è¦å±•ç¤º H2Q æ¶æ„æœ¬èº«çš„æ¨ç†ååé‡æ½œåŠ›

def run_benchmark():
    print("ğŸš€ [H2Q] å¯åŠ¨åŸºå‡†æµ‹è¯•ï¼šGPT-2 vs H2Q ...")
    
    # 1. å‡†å¤‡æ•°æ®
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2-large")
    prompt = "The theory of relativity states that the laws of physics are the same for all non-accelerating observers."
    # é‡å¤ prompt ä»¥å¢åŠ é•¿åº¦ï¼Œè®©æµ‹è¯•æ›´å‡†ç¡®
    prompt = prompt * 5 
    
    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    input_ids = inputs["input_ids"]
    print(f"ğŸ“ è¾“å…¥é•¿åº¦: {input_ids.shape[1]} tokens")

    # --- åŸºå‡† A: GPT-2 Large ---
    print("\nğŸ¢ [åŸºå‡† A] GPT-2 Large (774M Params)...")
    big_model = GPT2LMHeadModel.from_pretrained("gpt2-large").to(DEVICE)
    big_model.eval()
    
    start = time.time()
    with torch.no_grad():
        # ç”Ÿæˆ 50 ä¸ª token
        _ = big_model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)
    end = time.time()
    time_big = end - start
    print(f"   â±ï¸ è€—æ—¶: {time_big:.4f}s")
    print(f"   âš¡ï¸ é€Ÿåº¦: {50/time_big:.2f} tokens/s")
    
    # é‡Šæ”¾æ˜¾å­˜
    del big_model
    torch.cuda.empty_cache() if torch.cuda.is_available() else None

    # --- åŸºå‡† B: H2Q (Knot Kernel) ---
    print("\nğŸ‡ [åŸºå‡† B] H2Q Knot Kernel (256 Dim)...")
    
    # åˆå§‹åŒ– H2Q ç³»ç»Ÿ (ä½¿ç”¨ Knot Kernel)
    h2q_sys = AutonomousSystem(context_dim=256, action_dim=256)
    from h2q.knot_kernel import H2Q_Knot_Kernel
    # è¿™é‡Œçš„ vocab_size è®¾ä¸º 50257 ä»¥æ¨¡æ‹Ÿå¤„ç†åŒæ ·çš„è¯è¡¨è´Ÿè½½
    h2q_sys.dde.kernel = H2Q_Knot_Kernel(max_dim=256, vocab_size=50257, depth=6)
    h2q_sys.dde.to(DEVICE)
    h2q_sys.dde.eval()
    
    # æ¨¡æ‹Ÿç”Ÿæˆå¾ªç¯
    # H2Q çš„ç”Ÿæˆé€»è¾‘ï¼šForward -> Argmax -> Concat
    curr_input = input_ids.clone()
    
    start = time.time()
    with torch.no_grad():
        for _ in range(50):
            # H2Q å‰å‘
            logits, _ = h2q_sys.dde.kernel(curr_input)
            # è´ªå©ªé‡‡æ ·
            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)
            curr_input = torch.cat([curr_input, next_token], dim=1)
    end = time.time()
    
    time_h2q = end - start
    print(f"   â±ï¸ è€—æ—¶: {time_h2q:.4f}s")
    print(f"   âš¡ï¸ é€Ÿåº¦: {50/time_h2q:.2f} tokens/s")
    
    # --- æ€»ç»“ ---
    speedup = time_big / time_h2q
    print(f"\nğŸ† H2Q åŠ é€Ÿæ¯”: {speedup:.2f}x")
    print("   (æ³¨ï¼šè¿™æ˜¯çº¯è®¡ç®—ååé‡å¯¹æ¯”ï¼Œå±•ç¤ºäº† H2Q æ¶æ„çš„æè‡´æ•ˆç‡)")

if __name__ == "__main__":
    run_benchmark()