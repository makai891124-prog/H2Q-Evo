# tools/extract_qwen_crystal.py

import torch
import torch.nn.functional as F
from transformers import AutoModel, AutoTokenizer
import os

# --- [ä¸­å›½åŒºåŠ é€Ÿ] ---
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

def extract_crystal():
    # ä½¿ç”¨ Qwen2.5-0.5Bï¼Œå®ƒéå¸¸å°(1GBå·¦å³)ï¼Œä½†ä¸­è‹±ä»£ç èƒ½åŠ›æå¼º
    model_name = "Qwen/Qwen2.5-0.5B"
    save_path = "h2q_qwen_crystal.pt"
    target_dim = 256

    print(f"ğŸ”® æ­£åœ¨åŠ è½½æ•™å¸ˆæ¨¡å‹: {model_name} ...")
    try:
        model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿ç½‘ç»œé€šç•…ï¼Œæˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ã€‚")
        return

    # è·å–åŸå§‹åµŒå…¥çŸ©é˜µ [Vocab_Size, Hidden_Dim] (é€šå¸¸æ˜¯ 151936 x 896)
    embeddings = model.get_input_embeddings().weight.detach()
    vocab_size, hidden_dim = embeddings.shape
    print(f"   åŸå§‹ç»´åº¦: {vocab_size} x {hidden_dim}")

    print("ğŸ“‰ æ­£åœ¨è¿›è¡Œ SVD ç»“æ™¶ (æå–æ ¸å¿ƒæ‹“æ‰‘)...")
    # ä½¿ç”¨ä½ç§© SVD æå–æœ€é‡è¦çš„ 256 ä¸ªè¯­ä¹‰ç»´åº¦
    # float32 ç²¾åº¦è®¡ç®—ä»¥ä¿è¯è´¨é‡
    U, S, V = torch.svd_lowrank(embeddings.float(), q=target_dim)
    
    # å‹ç¼©åçš„åµŒå…¥ [Vocab, 256]
    compressed_emb = U @ torch.diag(S)
    
    # å°„å½±å½’ä¸€åŒ– (æŠ•å½±åˆ° H2Q è¶…çƒé¢)
    projected_emb = F.normalize(compressed_emb, p=2, dim=-1)

    crystal = {
        "source": model_name,
        "embeddings": projected_emb.half(), # è½¬å› fp16 èŠ‚çœç©ºé—´
        "projection_matrix": V.half(),      # ä¿å­˜æŠ•å½±çŸ©é˜µ
        "vocab_size": vocab_size
    }

    torch.save(crystal, save_path)
    print(f"âœ… çœŸç†æ™¶ä½“å·²ä¿å­˜: {save_path}")
    print("   è¿™å—æ™¶ä½“åŒ…å«äº† Qwen å¯¹ä¸­æ–‡ã€è‹±æ–‡å’Œä»£ç çš„å…¨éƒ¨åº•å±‚è®¤çŸ¥ã€‚")

if __name__ == "__main__":
    extract_crystal()