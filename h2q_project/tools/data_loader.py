# tools/data_loader.py

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer
from datasets import load_dataset
import os

# --- [ä¸­å›½åŒºåŠ é€Ÿ] ---
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

class H2QTextDataset(Dataset):
    def __init__(self, split="train", seq_len=128, max_samples=None):
        print(f"ğŸ“š æ­£åœ¨åŠ è½½ WikiText-2 ({split}) æ•°æ®é›†...")
        self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.seq_len = seq_len
        
        # åŠ è½½æ•°æ®é›†
        dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split=split)
        
        # é¢„å¤„ç†ï¼šåˆå¹¶æ‰€æœ‰æ–‡æœ¬å¹¶åˆ‡åˆ†
        text = "\n".join([t for t in dataset["text"] if len(t) > 0])
        tokens = self.tokenizer.encode(text)
        
        # åˆ‡åˆ†æˆå›ºå®šé•¿åº¦çš„å—
        self.examples = []
        for i in range(0, len(tokens) - seq_len, seq_len):
            self.examples.append(tokens[i : i + seq_len])
            if max_samples and len(self.examples) >= max_samples:
                break
                
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: {len(self.examples)} ä¸ªæ ·æœ¬ (Seq_Len={seq_len})")

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        # è¿”å›çš„æ˜¯ LongTensor [Seq_Len]
        return torch.tensor(self.examples[idx], dtype=torch.long)

def get_dataloader(split="train", batch_size=8, seq_len=128):
    dataset = H2QTextDataset(split, seq_len)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    loader = get_dataloader(batch_size=2, seq_len=32)
    batch = next(iter(loader))
    print(f"Batch Shape: {batch.shape}")
    print(f"Sample: {batch[0]}")