#!/usr/bin/env python3
"""
æ·±åº¦æ€§èƒ½å®¡è®¡è„šæœ¬ - é’ˆå¯¹å››å…ƒæ•°æ¶æ„çš„æ¢ç®—å…¬å¹³æ€§å’Œæ½œåœ¨ä½œå¼Šè¡Œä¸ºå®¡è®¡
Deep Performance Audit - Focusing on quaternion architecture fairness and potential cheating

å…³æ³¨ç‚¹:
1. å‚æ•°é‡æ¢ç®—å…¬å¹³æ€§ (quaternion vs real-valued parameters)
2. å»¶è¿Ÿæµ‹è¯•çš„å®Œæ•´æ€§ (æ˜¯å¦è·³è¿‡å…³é”®æ­¥éª¤)
3. å†…å­˜æµ‹é‡çš„å‡†ç¡®æ€§ (æ˜¯å¦åªæµ‹é‡äº†éƒ¨åˆ†å†…å­˜)
4. CIFAR-10çœŸå®è¿è¡ŒéªŒè¯
"""

import torch
import torch.nn as nn
import time
import tracemalloc
import psutil
import os
import sys
import json
import numpy as np
from datetime import datetime

# ============================================================
# å®¡è®¡ 1: å‚æ•°é‡æ¢ç®—å…¬å¹³æ€§å®¡è®¡
# ============================================================
class QuaternionParameterAudit:
    """å®¡è®¡å››å…ƒæ•°å‚æ•°å’Œå®æ•°å‚æ•°çš„æ¢ç®—æ˜¯å¦å…¬å¹³"""
    
    def __init__(self):
        self.results = {}
    
    def count_params(self, model):
        """ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡"""
        total = sum(p.numel() for p in model.parameters() if p.requires_grad)
        return total
    
    def count_memory_footprint(self, model):
        """ç»Ÿè®¡æ¨¡å‹å†…å­˜å ç”¨ (å­—èŠ‚)"""
        total_bytes = 0
        for p in model.parameters():
            total_bytes += p.element_size() * p.nelement()
        return total_bytes
    
    def audit_quaternion_equivalence(self):
        """å®¡è®¡å››å…ƒæ•°å±‚çš„å‚æ•°ç­‰ä»·æ€§
        
        å…³é”®é—®é¢˜: 1ä¸ªquaternionå‚æ•° = 4ä¸ªrealå‚æ•°?
        - å¦‚æœæŒ‰4ä¸ªrealè®¡ç®—, åˆ™å‚æ•°é‡æ˜¯å…¬å¹³çš„
        - å¦‚æœæŒ‰1ä¸ªquaternionè®¡ç®—, åˆ™ä½ä¼°äº†4å€å¤æ‚åº¦
        """
        print("\n" + "="*80)
        print("å®¡è®¡ 1: å››å…ƒæ•°å‚æ•°æ¢ç®—å…¬å¹³æ€§")
        print("="*80)
        
        # æ¨¡æ‹Ÿç®€å•çš„quaternionå±‚
        class SimpleQuaternionLinear(nn.Module):
            """å››å…ƒæ•°çº¿æ€§å±‚: y = Wx + b (quaternionç‰ˆæœ¬)
            
            æ¯ä¸ªquaternionæœ‰4ä¸ªåˆ†é‡: (w, x, y, z)
            å‚æ•°é‡åº”è¯¥è®¡ä¸º: 4 * (in_features * out_features + out_features)
            """
            def __init__(self, in_features, out_features):
                super().__init__()
                # å››å…ƒæ•°æƒé‡: 4ä¸ªåˆ†é‡
                self.W_r = nn.Parameter(torch.randn(out_features, in_features))
                self.W_i = nn.Parameter(torch.randn(out_features, in_features))
                self.W_j = nn.Parameter(torch.randn(out_features, in_features))
                self.W_k = nn.Parameter(torch.randn(out_features, in_features))
                # å››å…ƒæ•°åç½®: 4ä¸ªåˆ†é‡
                self.b_r = nn.Parameter(torch.randn(out_features))
                self.b_i = nn.Parameter(torch.randn(out_features))
                self.b_j = nn.Parameter(torch.randn(out_features))
                self.b_k = nn.Parameter(torch.randn(out_features))
            
            def forward(self, x):
                # ç®€åŒ–ç‰ˆquaternionä¹˜æ³•
                return self.W_r @ x.T + self.b_r.unsqueeze(1)
        
        # å¯¹æ¯”æ¨¡å‹
        class RealLinear(nn.Module):
            """æ™®é€šå®æ•°çº¿æ€§å±‚"""
            def __init__(self, in_features, out_features):
                super().__init__()
                self.linear = nn.Linear(in_features, out_features)
            
            def forward(self, x):
                return self.linear(x)
        
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹
        in_dim, out_dim = 128, 256
        quat_model = SimpleQuaternionLinear(in_dim, out_dim)
        real_model = RealLinear(in_dim, out_dim)
        
        # ç»Ÿè®¡å‚æ•°
        quat_params = self.count_params(quat_model)
        real_params = self.count_params(real_model)
        quat_memory = self.count_memory_footprint(quat_model)
        real_memory = self.count_memory_footprint(real_model)
        
        print(f"\né…ç½®: in_dim={in_dim}, out_dim={out_dim}")
        print(f"  Quaternionå±‚å‚æ•°é‡: {quat_params:,} ä¸ª")
        print(f"  Realå±‚å‚æ•°é‡:       {real_params:,} ä¸ª")
        print(f"  å‚æ•°é‡æ¯”ä¾‹: {quat_params / real_params:.2f}x")
        print(f"  Quaternionå±‚å†…å­˜: {quat_memory / 1024:.2f} KB")
        print(f"  Realå±‚å†…å­˜:       {real_memory / 1024:.2f} KB")
        print(f"  å†…å­˜æ¯”ä¾‹: {quat_memory / real_memory:.2f}x")
        
        # å…³é”®ç»“è®º
        expected_ratio = 4.0  # ç†è®ºä¸Šåº”è¯¥æ˜¯4å€
        actual_ratio = quat_params / real_params
        is_fair = abs(actual_ratio - expected_ratio) < 0.1
        
        print(f"\nâœ… æ¢ç®—å…¬å¹³æ€§åˆ¤å®š:")
        print(f"  ç†è®ºæ¯”ä¾‹: {expected_ratio:.1f}x (1ä¸ªquaternion = 4ä¸ªrealåˆ†é‡)")
        print(f"  å®é™…æ¯”ä¾‹: {actual_ratio:.2f}x")
        print(f"  ç»“è®º: {'å…¬å¹³ âœ“' if is_fair else 'ä¸å…¬å¹³ âœ— (å‚æ•°é‡è®¡ç®—å¯èƒ½æœ‰è¯¯)'}")
        
        self.results['quaternion_equivalence'] = {
            'quaternion_params': quat_params,
            'real_params': real_params,
            'ratio': actual_ratio,
            'expected_ratio': expected_ratio,
            'is_fair': 'yes' if is_fair else 'no'
        }
        
        return is_fair

# ============================================================
# å®¡è®¡ 2: å»¶è¿Ÿæµ‹è¯•å®Œæ•´æ€§å®¡è®¡
# ============================================================
class LatencyTestIntegrityAudit:
    """å®¡è®¡å»¶è¿Ÿæµ‹è¯•æ˜¯å¦å­˜åœ¨ä½œå¼Šè¡Œä¸º"""
    
    def __init__(self):
        self.results = {}
    
    def audit_warmup_bias(self):
        """å®¡è®¡é¢„çƒ­æ¬¡æ•°æ˜¯å¦è¿‡å¤šå¯¼è‡´ä¸å…¬å¹³ä¼˜åŠ¿
        
        æ½œåœ¨ä½œå¼Š:
        - è¿‡åº¦é¢„çƒ­å¯¼è‡´æ‰€æœ‰æ•°æ®éƒ½åœ¨ç¼“å­˜ä¸­
        - åªæµ‹é‡ç¼“å­˜å‘½ä¸­çš„æƒ…å†µ,ä¸æµ‹é‡å†·å¯åŠ¨
        """
        print("\n" + "="*80)
        print("å®¡è®¡ 2: å»¶è¿Ÿæµ‹è¯•å®Œæ•´æ€§ - é¢„çƒ­åå·®")
        print("="*80)
        
        model = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 256)
        )
        device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        model.to(device)
        input_data = torch.randn(1, 256).to(device)
        
        # æµ‹è¯•ä¸åŒé¢„çƒ­æ¬¡æ•°çš„å½±å“
        warmup_configs = [0, 1, 5, 10, 50, 100]
        results = {}
        
        for warmup_count in warmup_configs:
            # é‡æ–°åŠ è½½æ¨¡å‹ä»¥æ¸…é™¤ç¼“å­˜
            model = nn.Sequential(
                nn.Linear(256, 512),
                nn.ReLU(),
                nn.Linear(512, 256)
            ).to(device)
            
            # é¢„çƒ­
            with torch.no_grad():
                for _ in range(warmup_count):
                    _ = model(input_data)
            
            # åŒæ­¥
            if hasattr(torch.backends, 'mps'):
                torch.mps.synchronize()
            
            # æµ‹é‡
            times = []
            with torch.no_grad():
                for _ in range(100):
                    start = time.perf_counter()
                    _ = model(input_data)
                    if hasattr(torch.backends, 'mps'):
                        torch.mps.synchronize()
                    end = time.perf_counter()
                    times.append((end - start) * 1e6)  # å¾®ç§’
            
            avg_latency = np.mean(times)
            std_latency = np.std(times)
            results[warmup_count] = {
                'mean': avg_latency,
                'std': std_latency
            }
            print(f"  é¢„çƒ­{warmup_count:3d}æ¬¡: å¹³å‡å»¶è¿Ÿ={avg_latency:8.2f}Î¼s, æ ‡å‡†å·®={std_latency:7.2f}Î¼s")
        
        # åˆ†æåå·®
        no_warmup = results[0]['mean']
        with_warmup = results[10]['mean']
        bias_percent = (no_warmup - with_warmup) / no_warmup * 100
        
        print(f"\nâœ… é¢„çƒ­åå·®åˆ†æ:")
        print(f"  æ— é¢„çƒ­å»¶è¿Ÿ:   {no_warmup:.2f}Î¼s")
        print(f"  10æ¬¡é¢„çƒ­å»¶è¿Ÿ: {with_warmup:.2f}Î¼s")
        print(f"  åå·®:         {bias_percent:.1f}%")
        print(f"  ç»“è®º: {'åˆç†' if bias_percent < 30 else 'å¯èƒ½å­˜åœ¨è¿‡åº¦é¢„çƒ­åå·®'}")
        
        self.results['warmup_bias'] = {
            'no_warmup_latency': no_warmup,
            'with_warmup_latency': with_warmup,
            'bias_percent': bias_percent,
            'is_fair': 'yes' if bias_percent < 30 else 'no'
        }
        
        return bias_percent < 30
    
    def audit_measurement_completeness(self):
        """å®¡è®¡æ˜¯å¦æµ‹é‡äº†å®Œæ•´çš„æ¨ç†æµç¨‹
        
        æ½œåœ¨ä½œå¼Š:
        - åªæµ‹é‡forwardä¸æµ‹é‡æ•°æ®åŠ è½½
        - åªæµ‹é‡å•ä¸ªtokenä¸æµ‹é‡å®Œæ•´åºåˆ—
        - è·³è¿‡åå¤„ç†æ­¥éª¤
        """
        print("\n" + "="*80)
        print("å®¡è®¡ 2: å»¶è¿Ÿæµ‹è¯•å®Œæ•´æ€§ - æµ‹é‡å®Œæ•´æ€§")
        print("="*80)
        
        model = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 256)
        )
        device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        model.to(device)
        
        # æµ‹è¯•ä¸åŒæµç¨‹çš„å»¶è¿Ÿ
        measurements = {}
        
        # 1. åªæµ‹é‡forward (æœ€å¸¸è§çš„ä½œå¼Šæ–¹å¼)
        input_data = torch.randn(1, 256).to(device)
        times = []
        with torch.no_grad():
            for _ in range(100):
                start = time.perf_counter()
                _ = model(input_data)
                if hasattr(torch.backends, 'mps'):
                    torch.mps.synchronize()
                end = time.perf_counter()
                times.append((end - start) * 1e6)
        measurements['forward_only'] = np.mean(times)
        
        # 2. æµ‹é‡forward + æ•°æ®ä¼ è¾“
        times = []
        with torch.no_grad():
            for _ in range(100):
                start = time.perf_counter()
                input_data = torch.randn(1, 256).to(device)
                _ = model(input_data)
                if hasattr(torch.backends, 'mps'):
                    torch.mps.synchronize()
                end = time.perf_counter()
                times.append((end - start) * 1e6)
        measurements['forward_with_transfer'] = np.mean(times)
        
        # 3. æµ‹é‡forward + åå¤„ç†
        times = []
        with torch.no_grad():
            for _ in range(100):
                start = time.perf_counter()
                output = model(input_data)
                result = torch.argmax(output, dim=-1)  # æ¨¡æ‹Ÿåå¤„ç†
                if hasattr(torch.backends, 'mps'):
                    torch.mps.synchronize()
                end = time.perf_counter()
                times.append((end - start) * 1e6)
        measurements['forward_with_postprocess'] = np.mean(times)
        
        # 4. å®Œæ•´æµç¨‹
        times = []
        with torch.no_grad():
            for _ in range(100):
                start = time.perf_counter()
                input_data = torch.randn(1, 256).to(device)
                output = model(input_data)
                result = torch.argmax(output, dim=-1)
                if hasattr(torch.backends, 'mps'):
                    torch.mps.synchronize()
                end = time.perf_counter()
                times.append((end - start) * 1e6)
        measurements['full_pipeline'] = np.mean(times)
        
        print(f"\nå»¶è¿Ÿæµ‹é‡å¯¹æ¯”:")
        for key, value in measurements.items():
            print(f"  {key:30s}: {value:8.2f}Î¼s")
        
        # åˆ†æ
        overhead = measurements['full_pipeline'] - measurements['forward_only']
        overhead_percent = overhead / measurements['full_pipeline'] * 100
        
        print(f"\nâœ… æµ‹é‡å®Œæ•´æ€§åˆ†æ:")
        print(f"  çº¯forward:    {measurements['forward_only']:.2f}Î¼s")
        print(f"  å®Œæ•´pipeline: {measurements['full_pipeline']:.2f}Î¼s")
        print(f"  overhead:     {overhead:.2f}Î¼s ({overhead_percent:.1f}%)")
        print(f"  ç»“è®º: å¦‚æœåªæµ‹forward, ä½ä¼°äº† {overhead_percent:.1f}% çš„çœŸå®å»¶è¿Ÿ")
        
        self.results['measurement_completeness'] = {
            'forward_only': measurements['forward_only'],
            'full_pipeline': measurements['full_pipeline'],
            'overhead_percent': overhead_percent
        }
        
        return measurements

# ============================================================
# å®¡è®¡ 3: å†…å­˜æµ‹é‡å‡†ç¡®æ€§å®¡è®¡
# ============================================================
class MemoryMeasurementAudit:
    """å®¡è®¡å†…å­˜æµ‹é‡æ˜¯å¦å‡†ç¡®å’Œå®Œæ•´"""
    
    def __init__(self):
        self.results = {}
    
    def audit_memory_measurement_methods(self):
        """å¯¹æ¯”ä¸åŒå†…å­˜æµ‹é‡æ–¹æ³•çš„å·®å¼‚
        
        æ½œåœ¨é—®é¢˜:
        - tracemallocåªæµ‹é‡Pythonå¯¹è±¡,ä¸æµ‹é‡PyTorchå¼ é‡
        - åªæµ‹é‡æ¨¡å‹å‚æ•°,ä¸æµ‹é‡æ¿€æ´»å†…å­˜
        - åªæµ‹é‡å³°å€¼,ä¸æµ‹é‡å®é™…è¿è¡Œæ—¶å†…å­˜
        """
        print("\n" + "="*80)
        print("å®¡è®¡ 3: å†…å­˜æµ‹é‡å‡†ç¡®æ€§ - ä¸åŒæµ‹é‡æ–¹æ³•å¯¹æ¯”")
        print("="*80)
        
        # åˆ›å»ºä¸€ä¸ªç®€å•æ¨¡å‹
        model = nn.Sequential(
            nn.Linear(1000, 2000),
            nn.ReLU(),
            nn.Linear(2000, 1000)
        )
        
        measurements = {}
        
        # æ–¹æ³•1: åªç»Ÿè®¡å‚æ•°å†…å­˜ (æœ€å°ä¼°è®¡)
        param_memory = sum(p.element_size() * p.nelement() for p in model.parameters())
        measurements['param_only'] = param_memory
        print(f"  æ–¹æ³•1 (å‚æ•°å†…å­˜):        {param_memory / 1024 / 1024:.4f} MB")
        
        # æ–¹æ³•2: tracemalloc (Pythonå¯¹è±¡)
        tracemalloc.start()
        snapshot1 = tracemalloc.take_snapshot()
        _ = [torch.randn(1000) for _ in range(100)]  # åˆ†é…ä¸€äº›å¯¹è±¡
        snapshot2 = tracemalloc.take_snapshot()
        top_stats = snapshot2.compare_to(snapshot1, 'lineno')
        tracemalloc_delta = sum(stat.size_diff for stat in top_stats)
        tracemalloc.stop()
        measurements['tracemalloc'] = tracemalloc_delta
        print(f"  æ–¹æ³•2 (tracemalloc):     {tracemalloc_delta / 1024 / 1024:.4f} MB")
        
        # æ–¹æ³•3: psutil (è¿›ç¨‹æ€»å†…å­˜)
        process = psutil.Process(os.getpid())
        mem_before = process.memory_info().rss
        # åˆ†é…ä¸€äº›æ•°æ®
        big_tensor = torch.randn(1000, 1000)
        _ = model(torch.randn(32, 1000))
        mem_after = process.memory_info().rss
        psutil_delta = mem_after - mem_before
        measurements['psutil'] = psutil_delta
        print(f"  æ–¹æ³•3 (psutilè¿›ç¨‹å†…å­˜): {psutil_delta / 1024 / 1024:.4f} MB")
        
        # æ–¹æ³•4: PyTorchç¼“å­˜ (GPU/MPS)
        device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        if device.type == "mps":
            # MPSå†…å­˜éš¾ä»¥ç›´æ¥æµ‹é‡,ä½¿ç”¨ä¼°ç®—
            model_mps = model.to(device)
            input_mps = torch.randn(32, 1000).to(device)
            _ = model_mps(input_mps)
            # ä¼°ç®—: å‚æ•° + æ¿€æ´»
            activation_memory = 32 * 1000 * 4 + 32 * 2000 * 4  # batch_size * dim * sizeof(float32)
            torch_memory = param_memory + activation_memory
            measurements['torch_estimate'] = torch_memory
            print(f"  æ–¹æ³•4 (PyTorchä¼°ç®—):     {torch_memory / 1024 / 1024:.4f} MB")
        
        # åˆ†æå·®å¼‚
        max_measurement = max(measurements.values())
        min_measurement = min(measurements.values())
        ratio = max_measurement / min_measurement
        
        print(f"\nâœ… å†…å­˜æµ‹é‡å·®å¼‚åˆ†æ:")
        print(f"  æœ€å°æµ‹é‡: {min_measurement / 1024 / 1024:.4f} MB")
        print(f"  æœ€å¤§æµ‹é‡: {max_measurement / 1024 / 1024:.4f} MB")
        print(f"  å·®å¼‚å€æ•°: {ratio:.2f}x")
        print(f"  ç»“è®º: ä¸åŒæµ‹é‡æ–¹æ³•å·®å¼‚æ˜¾è‘—, éœ€è¦æ˜ç¡®æŒ‡å‡ºæµ‹é‡æ–¹æ³•")
        
        self.results['memory_methods'] = {
            'measurements': {k: v / 1024 / 1024 for k, v in measurements.items()},
            'max_min_ratio': ratio
        }
        
        return measurements
    
    def audit_activation_memory(self):
        """å®¡è®¡æ¿€æ´»å†…å­˜æ˜¯å¦è¢«è€ƒè™‘
        
        å…³é”®é—®é¢˜: 0.01MBçš„å®£ç§°å¯èƒ½åªè®¡ç®—äº†å‚æ•°,æ²¡ç®—æ¿€æ´»
        """
        print("\n" + "="*80)
        print("å®¡è®¡ 3: å†…å­˜æµ‹é‡å‡†ç¡®æ€§ - æ¿€æ´»å†…å­˜")
        print("="*80)
        
        # åˆ›å»ºä¸åŒå¤§å°çš„æ¨¡å‹
        configs = [
            (256, 512, 256),
            (512, 1024, 512),
            (1024, 2048, 1024)
        ]
        
        for in_dim, hidden_dim, out_dim in configs:
            model = nn.Sequential(
                nn.Linear(in_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, out_dim)
            )
            
            # å‚æ•°å†…å­˜
            param_memory = sum(p.element_size() * p.nelement() for p in model.parameters())
            
            # æ¿€æ´»å†…å­˜ (batch_size=32)
            batch_size = 32
            activation_memory = (
                batch_size * in_dim * 4 +      # è¾“å…¥
                batch_size * hidden_dim * 4 +  # ç¬¬ä¸€å±‚è¾“å‡º
                batch_size * hidden_dim * 4 +  # ReLUè¾“å‡º
                batch_size * out_dim * 4       # æœ€ç»ˆè¾“å‡º
            )
            
            total_memory = param_memory + activation_memory
            
            print(f"\né…ç½®: {in_dim}->{hidden_dim}->{out_dim}, batch={batch_size}")
            print(f"  å‚æ•°å†…å­˜:   {param_memory / 1024 / 1024:.4f} MB ({param_memory / total_memory * 100:.1f}%)")
            print(f"  æ¿€æ´»å†…å­˜:   {activation_memory / 1024 / 1024:.4f} MB ({activation_memory / total_memory * 100:.1f}%)")
            print(f"  æ€»å†…å­˜:     {total_memory / 1024 / 1024:.4f} MB")
        
        print(f"\nâœ… æ¿€æ´»å†…å­˜å®¡è®¡ç»“è®º:")
        print(f"  æ¿€æ´»å†…å­˜é€šå¸¸å æ€»å†…å­˜çš„30-70%")
        print(f"  å¦‚æœå®£ç§°çš„0.01MBåªè®¡ç®—å‚æ•°,åˆ™å®é™…è¿è¡Œæ—¶å†…å­˜å¯èƒ½æ˜¯3-10å€")
        
        return True

# ============================================================
# å®¡è®¡ 4: CIFAR-10çœŸå®è¿è¡ŒéªŒè¯
# ============================================================
def run_cifar10_real_benchmark():
    """è¿è¡ŒCIFAR-10å®é™…è®­ç»ƒä»¥è·å–çœŸå®å‡†ç¡®ç‡"""
    print("\n" + "="*80)
    print("å®¡è®¡ 4: CIFAR-10çœŸå®è¿è¡ŒéªŒè¯")
    print("="*80)
    
    print("\næ£€æŸ¥CIFAR-10è®­ç»ƒè„šæœ¬...")
    script_path = "h2q_project/benchmarks/cifar10_classification.py"
    
    if not os.path.exists(script_path):
        print(f"  âœ— è„šæœ¬ä¸å­˜åœ¨: {script_path}")
        return {'script_exists': 'no'}
    
    print(f"  âœ“ è„šæœ¬å­˜åœ¨: {script_path}")
    print(f"\nâš ï¸  æ³¨æ„: å®Œæ•´è®­ç»ƒéœ€è¦1-2å°æ—¶")
    print(f"  å»ºè®®è¿è¡Œå‘½ä»¤:")
    print(f"    PYTHONPATH=. python3 {script_path} --epochs 10 --batch-size 128")
    print(f"\n  ä¸ºäº†å®¡è®¡ç›®çš„,å¯ä»¥è¿è¡Œå¿«é€Ÿç‰ˆæœ¬ (3ä¸ªepoch,éªŒè¯æ¶æ„):")
    print(f"    PYTHONPATH=. python3 {script_path} --epochs 3 --batch-size 128")
    
    # è¯¢é—®æ˜¯å¦è¿è¡Œ
    print(f"\n  æ˜¯å¦ç«‹å³è¿è¡Œ? (å®Œæ•´è®­ç»ƒçº¦éœ€1-2å°æ—¶)")
    print(f"    è¾“å…¥å‘½ä»¤æ‰‹åŠ¨è¿è¡Œ,æˆ–åœ¨æ­¤è„šæœ¬ä¸­è®¾ç½® AUTO_RUN_CIFAR10=True")
    
    return {
        'script_exists': 'yes',
        'script_path': script_path,
        'command': f'PYTHONPATH=. python3 {script_path} --epochs 10',
        'quick_command': f'PYTHONPATH=. python3 {script_path} --epochs 3'
    }

# ============================================================
# ä¸»å®¡è®¡æµç¨‹
# ============================================================
def run_deep_audit():
    """è¿è¡Œå®Œæ•´çš„æ·±åº¦å®¡è®¡"""
    print("="*80)
    print("ğŸ” H2Qæ€§èƒ½å®£ç§°æ·±åº¦å®¡è®¡")
    print("="*80)
    print(f"å®¡è®¡æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"è®¾å¤‡: {torch.device('mps' if torch.backends.mps.is_available() else 'cpu')}")
    
    audit_results = {
        'timestamp': datetime.now().isoformat(),
        'python_version': sys.version,
        'pytorch_version': torch.__version__,
        'device': str(torch.device('mps' if torch.backends.mps.is_available() else 'cpu')),
        'audits': {}
    }
    
    # å®¡è®¡1: å‚æ•°æ¢ç®—
    print("\n" + "ğŸ” å¼€å§‹å®¡è®¡1: å››å…ƒæ•°å‚æ•°æ¢ç®—å…¬å¹³æ€§" + "\n")
    param_audit = QuaternionParameterAudit()
    param_audit.audit_quaternion_equivalence()
    audit_results['audits']['quaternion_parameters'] = param_audit.results
    
    # å®¡è®¡2: å»¶è¿Ÿæµ‹è¯•
    print("\n" + "ğŸ” å¼€å§‹å®¡è®¡2: å»¶è¿Ÿæµ‹è¯•å®Œæ•´æ€§" + "\n")
    latency_audit = LatencyTestIntegrityAudit()
    latency_audit.audit_warmup_bias()
    latency_audit.audit_measurement_completeness()
    audit_results['audits']['latency_integrity'] = latency_audit.results
    
    # å®¡è®¡3: å†…å­˜æµ‹é‡
    print("\n" + "ğŸ” å¼€å§‹å®¡è®¡3: å†…å­˜æµ‹é‡å‡†ç¡®æ€§" + "\n")
    memory_audit = MemoryMeasurementAudit()
    memory_audit.audit_memory_measurement_methods()
    memory_audit.audit_activation_memory()
    audit_results['audits']['memory_accuracy'] = memory_audit.results
    
    # å®¡è®¡4: CIFAR-10
    print("\n" + "ğŸ” å¼€å§‹å®¡è®¡4: CIFAR-10çœŸå®è¿è¡Œ" + "\n")
    cifar10_result = run_cifar10_real_benchmark()
    audit_results['audits']['cifar10_benchmark'] = cifar10_result
    
    # ä¿å­˜ç»“æœ
    output_file = 'deep_performance_audit_results.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(audit_results, f, indent=2, ensure_ascii=False)
    
    print("\n" + "="*80)
    print("âœ… æ·±åº¦å®¡è®¡å®Œæˆ!")
    print("="*80)
    print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_file}")
    print(f"\nå…³é”®å‘ç°æ€»ç»“:")
    print(f"  1. å››å…ƒæ•°å‚æ•°æ¢ç®—: {'å…¬å¹³ âœ“' if audit_results['audits']['quaternion_parameters']['quaternion_equivalence']['is_fair'] == 'yes' else 'ä¸å…¬å¹³ âœ—'}")
    print(f"  2. å»¶è¿Ÿæµ‹è¯•é¢„çƒ­: {'åˆç† âœ“' if audit_results['audits']['latency_integrity']['warmup_bias']['is_fair'] == 'yes' else 'å­˜åœ¨åå·® âš ï¸'}")
    print(f"  3. å†…å­˜æµ‹é‡æ–¹æ³•å·®å¼‚: {audit_results['audits']['memory_accuracy']['memory_methods']['max_min_ratio']:.1f}x")
    print(f"  4. CIFAR-10è„šæœ¬: {'å­˜åœ¨ âœ“' if cifar10_result['script_exists'] == 'yes' else 'ä¸å­˜åœ¨ âœ—'}")
    
    return audit_results

if __name__ == "__main__":
    results = run_deep_audit()
