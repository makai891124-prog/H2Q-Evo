# train_discrete_cpu.py

import torch
import torch.nn as nn
from tqdm import tqdm
import psutil
import os

# å¯¼å…¥å†…æ ¸å’Œæ•°æ®åŠ è½½å™¨
from h2q.knot_kernel import H2Q_Knot_Kernel
from tools.vision_loader import get_vision_dataloader

# --- é…ç½® ---
DEVICE = torch.device("cpu") # [æ ¸å¿ƒ] å¼ºåˆ¶ä½¿ç”¨ CPU
BATCH_SIZE = 32
SEQ_LEN = 1024 # ä¿æŒä¸€ä¸ªåˆç†çš„åºåˆ—é•¿åº¦
LR = 1e-3      # ç¦»æ•£æ¢¯åº¦éœ€è¦è°ƒæ•´å­¦ä¹ ç‡
STEPS = 1000
VOCAB_SIZE = 257
GRAD_THRESHOLD = 1e-5 # ç¦»æ•£å¯¼æ•°é˜ˆå€¼

def get_memory_usage():
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024 # MB

def train_discrete_cpu():
    print(f"ğŸš€ [H2Q-Discrete] å¯åŠ¨ CPU ç¦»æ•£å¯¼æ•°è®­ç»ƒ... è®¾å¤‡: {DEVICE}")
    
    # 1. åˆå§‹åŒ–æ¨¡å‹
    model = H2Q_Knot_Kernel(max_dim=256, vocab_size=VOCAB_SIZE, depth=6).to(DEVICE)
    
    # 2. æ•°æ®
    loader = get_vision_dataloader(split="train", batch_size=BATCH_SIZE)
    loss_fn = nn.CrossEntropyLoss()
    
    model.train()
    progress_bar = tqdm(range(STEPS), desc="Discrete CPU Training")
    data_iter = iter(loader)
    
    initial_mem = get_memory_usage()
    print(f"   åˆå§‹å†…å­˜: {initial_mem:.2f} MB")
    
    for step in progress_bar:
        try:
            batch = next(data_iter).to(DEVICE)
        except StopIteration:
            data_iter = iter(loader)
            batch = next(data_iter).to(DEVICE)
            
        # --- [æ ¸å¿ƒ] ç¦»æ•£æ¢¯åº¦æ‰‹åŠ¨è®¡ç®— (åœ¨ no_grad ä¸‹) ---
        with torch.no_grad():
            
            # 1. è®¡ç®—åŸºå‡† Loss
            logits_base, stab_base = model(batch)
            loss_base = loss_fn(logits_base.reshape(-1, VOCAB_SIZE), batch.reshape(-1))
            
            # 2. éå†å‚æ•°ï¼Œè®¡ç®—ç¦»æ•£æ¢¯åº¦
            for param in model.parameters():
                
                # åˆ›å»ºä¸€ä¸ªä¸å‚æ•°åŒå½¢çš„æ¢¯åº¦å¼ é‡
                grad_discrete = torch.zeros_like(param.data)
                
                # è¿­ä»£å‚æ•°çš„æ¯ä¸€ä¸ªå…ƒç´  (è¿™å¾ˆæ…¢ï¼Œä½†èƒ½ç²¾ç¡®æ§åˆ¶)
                # ä¼˜åŒ–ï¼šæˆ‘ä»¬å¯ä»¥å¯¹æ•´ä¸ªå¼ é‡è¿›è¡Œæ‰°åŠ¨ï¼Œç„¶åç”¨ sign()
                
                # --- ä¼˜åŒ–ç‰ˆï¼šå¼ é‡çº§æ‰°åŠ¨ ---
                
                # a. è®¡ç®—æ‰°åŠ¨
                # æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå°çš„å›ºå®šæ‰°åŠ¨ï¼Œè€Œä¸æ˜¯éšæœºæ‰°åŠ¨
                perturbation = torch.full_like(param.data, 1e-4)
                
                # b. è®¡ç®— L(w + p)
                param.data.add_(perturbation)
                logits_plus, _ = model(batch)
                loss_plus = loss_fn(logits_plus.reshape(-1, VOCAB_SIZE), batch.reshape(-1))
                
                # c. è®¡ç®—æ¢¯åº¦æ–¹å‘
                # grad_direction > 0 -> å¢åŠ å‚æ•°ä¼šå¢åŠ  Loss
                # grad_direction < 0 -> å¢åŠ å‚æ•°ä¼šå‡å° Loss
                grad_direction = loss_plus - loss_base
                
                # d. [æ ¸å¿ƒ] ç¦»æ•£åŒ–æ¢¯åº¦
                # å¦‚æœæ¢¯åº¦å˜åŒ–å¤§äºé˜ˆå€¼ï¼Œåˆ™æ ‡è®°ä¸º +1 æˆ– -1
                # å¦åˆ™æ ‡è®°ä¸º 0 (æˆªæ–­)
                grad_discrete[grad_direction > GRAD_THRESHOLD] = 1.0
                grad_discrete[grad_direction < -GRAD_THRESHOLD] = -1.0
                
                # e. æ‰‹åŠ¨æ›´æ–°å‚æ•°
                # æˆ‘ä»¬å¸Œæœ›å‡å° Lossï¼Œæ‰€ä»¥è¦æ²¿ç€æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°
                param.data.sub_(LR * grad_discrete)
                
                # è¿˜åŸå‚æ•°ä»¥ä¾¿ä¸‹ä¸€æ¬¡è¿­ä»£ (è™½ç„¶æˆ‘ä»¬å·²ç»æ›´æ–°äº†ï¼Œä½†ä¸ºäº†é€»è¾‘æ¸…æ™°)
                # å®é™…ä¸Šï¼Œæˆ‘ä»¬åº”è¯¥å…ˆè®¡ç®—æ‰€æœ‰æ¢¯åº¦ï¼Œå†ç»Ÿä¸€æ›´æ–°
                # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬é‡‡ç”¨â€œåœ¨çº¿æ›´æ–°â€
                
            # ç›‘æ§
            if step % 10 == 0:
                mem_usage = get_memory_usage()
                progress_bar.set_postfix({
                    "Loss": f"{loss_base.item():.4f}", 
                    "Mem": f"{mem_usage:.2f} MB"
                })

    final_mem = get_memory_usage()
    print("âœ… è®­ç»ƒå®Œæˆã€‚")
    print(f"   æœ€ç»ˆå†…å­˜: {final_mem:.2f} MB")
    print(f"   å†…å­˜å¢é‡: {final_mem - initial_mem:.2f} MB")

if __name__ == "__main__":
    train_discrete_cpu()