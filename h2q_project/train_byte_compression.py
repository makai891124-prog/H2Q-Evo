# train_byte_compression.py

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from tqdm import tqdm
import os

from h2q.system import AutonomousSystem
from tools.byte_loader import get_byte_dataloader

# --- é…ç½® ---
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
BATCH_SIZE = 32  # å­—èŠ‚çº§å¯ä»¥åŠ å¤§ Batch
SEQ_LEN = 256    # å­—èŠ‚çº§éœ€è¦æ›´é•¿çš„åºåˆ—
LR = 5e-4        # å­—èŠ‚çº§é€šå¸¸éœ€è¦ç¨å¤§çš„å­¦ä¹ ç‡
STEPS = 1000     # è·‘ 1000 æ­¥

def run_byte_training(dim):
    print(f"\nğŸš€ [Byte-Experiment] å¯åŠ¨ç»´åº¦ Dim={dim} çš„è®­ç»ƒ...")
    
    # 1. åˆå§‹åŒ–ç³»ç»Ÿ (Vocab=257: 0-255 + EOS)
    # æ³¨æ„ï¼šè¿™é‡Œä¸å†åŠ è½½ GPT-2 æ™¶ä½“ï¼Œå› ä¸ºé‚£æ˜¯ Token çº§çš„
    # æˆ‘ä»¬ä»é›¶å¼€å§‹è®­ç»ƒå‡ ä½•ç»“æ„
    system = AutonomousSystem(context_dim=dim, action_dim=dim)
    
    # å¼ºåˆ¶é‡ç½®å†…æ ¸ä¸º Byte æ¨¡å¼
    from h2q.gut_kernel import H2Q_Geometric_Kernel
    system.dde.kernel = H2Q_Geometric_Kernel(dim=dim, vocab_size=257, depth=12)
    system.dde.to(DEVICE)
    
    # 2. æ•°æ®ä¸ä¼˜åŒ–å™¨
    train_loader = get_byte_dataloader(split="train", batch_size=BATCH_SIZE, seq_len=SEQ_LEN)
    optimizer = optim.AdamW(system.dde.parameters(), lr=LR)
    
    losses = []
    system.dde.train()
    
    progress_bar = tqdm(range(STEPS), desc=f"Dim={dim}")
    data_iter = iter(train_loader)
    
    for _ in progress_bar:
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(train_loader)
            batch = next(data_iter)
            
        inputs = batch.to(DEVICE)
        context = inputs[:, :-1]
        targets = inputs[:, 1:]
        
        logits, _ = system.dde.kernel(context)
        # Vocab = 257
        loss = nn.CrossEntropyLoss()(logits.reshape(-1, 257), targets.reshape(-1))
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())
        progress_bar.set_postfix({"Loss": f"{loss.item():.4f}"})
        
    return losses

def main():
    # æˆ‘ä»¬å¯¹æ¯” 256 (å…¨ç»´), 64 (4xå‹ç¼©), 16 (16xå‹ç¼©)
    dims = [256, 64, 16]
    results = {}
    
    for d in dims:
        results[d] = run_byte_training(d)
        
    print("\nğŸ“Š æ­£åœ¨ç»˜åˆ¶å­—èŠ‚çº§å¯¹æ¯”å›¾...")
    plt.figure(figsize=(12, 8))
    
    colors = {256: 'blue', 64: 'orange', 16: 'red'}
    
    for d in dims:
        plt.plot(results[d], label=f'Dim = {d}', color=colors[d], alpha=0.8, linewidth=1.5)
        
    plt.title("H2Q Byte-Level Dimensional Collapse: The True Isomorphism")
    plt.xlabel("Steps")
    plt.ylabel("Byte-Level Loss")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

if __name__ == "__main__":
    main()