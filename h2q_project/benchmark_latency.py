# benchmark_latency.py

import torch
import time
import numpy as np
from transformers import GPT2LMHeadModel, GPT2Config
from h2q.system import AutonomousSystem
from h2q.knot_kernel import H2Q_Knot_Kernel

# --- é…ç½® ---
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
BATCH_SIZE = 1
SEQ_LEN = 1024 # æµ‹è¯•é•¿åºåˆ—ï¼Œè¿™æ˜¯ Transformer çš„ç—›ç‚¹
VOCAB_SIZE = 50257
LOOPS = 50 # è·‘50æ¬¡å–å¹³å‡

def benchmark_model(name, model, input_tensor):
    print(f"\nğŸ”¥ æµ‹è¯• {name} ...")
    model.eval()
    
    # é¢„çƒ­ (Warmup)
    with torch.no_grad():
        _ = model(input_tensor)
    
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    if hasattr(torch.backends, 'mps'): torch.mps.synchronize()
    
    times = []
    with torch.no_grad():
        for _ in range(LOOPS):
            start = time.time()
            _ = model(input_tensor)
            
            if hasattr(torch.backends, 'mps'): torch.mps.synchronize()
            end = time.time()
            times.append(end - start)
            
    avg_time = np.mean(times)
    print(f"   â±ï¸ å¹³å‡å»¶è¿Ÿ: {avg_time*1000:.2f} ms")
    print(f"   âš¡ï¸ ååé‡: {BATCH_SIZE * SEQ_LEN / avg_time:.2f} tokens/s")
    return avg_time

def run_comparison():
    print(f"ğŸš€ [H2Q vs GPT-2] å†…æ ¸å»¶è¿ŸåŸºå‡†æµ‹è¯•")
    print(f"   é…ç½®: Batch={BATCH_SIZE}, SeqLen={SEQ_LEN}, Device={DEVICE}")
    
    # æ„é€ è¾“å…¥
    dummy_input = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LEN)).to(DEVICE)
    
    # --- 1. GPT-2 Large (774M) ---
    # æˆ‘ä»¬åªåŠ è½½é…ç½®ï¼Œä¸åŠ è½½æƒé‡ï¼Œåªæµ‹è®¡ç®—é‡
    config = GPT2Config.from_pretrained("gpt2-large")
    gpt2 = GPT2LMHeadModel(config).to(DEVICE)
    time_gpt2 = benchmark_model("GPT-2 Large (774M)", gpt2, dummy_input)
    del gpt2
    
    # --- 2. H2Q Knot Kernel (256 Dim) ---
    # æ¨¡æ‹ŸåŒæ ·çš„è¯è¡¨è¾“å‡º
    h2q_sys = AutonomousSystem(context_dim=256, action_dim=256)
    h2q_sys.dde.kernel = H2Q_Knot_Kernel(max_dim=256, vocab_size=VOCAB_SIZE, depth=12)
    h2q_sys.dde.to(DEVICE)
    
    # åŒ…è£…ä¸€ä¸‹ forward ä»¥åŒ¹é…æ¥å£
    class H2QWrapper(torch.nn.Module):
        def __init__(self, sys):
            super().__init__()
            self.sys = sys
        def forward(self, x):
            return self.sys.dde.kernel(x)
            
    h2q_model = H2QWrapper(h2q_sys)
    time_h2q = benchmark_model("H2Q Knot Kernel (256 Dim)", h2q_model, dummy_input)
    
    # --- æ€»ç»“ ---
    print(f"\nğŸ† æœ€ç»ˆç»“æœ:")
    print(f"   H2Q æ¯” GPT-2 Large å¿«: {time_gpt2 / time_h2q:.2f} å€")
    
    # ä¼°ç®—æ˜¾å­˜ä¼˜åŠ¿ (ç†è®ºå€¼)
    # GPT2-Large: ~3GB å‚æ•°
    # H2Q: ~50MB å‚æ•° (256ç»´)
    print(f"   æ˜¾å­˜å ç”¨ä¼˜åŠ¿ (ä¼°ç®—): ~60x")

if __name__ == "__main__":
    run_comparison()