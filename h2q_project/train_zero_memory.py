# train_zero_memory.py

import torch
import torch.nn as nn
from tqdm import tqdm
import psutil
import os
import copy

# å¯¼å…¥å†…æ ¸å’Œæ•°æ®åŠ è½½å™¨
from h2q.knot_kernel import H2Q_Knot_Kernel
from tools.vision_loader import get_vision_dataloader

# --- é…ç½® ---
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
BATCH_SIZE = 64 # æˆ‘ä»¬å¯ä»¥ä¿æŒä¸€ä¸ªåˆç†çš„ Batch Size
SEQ_LEN = 3072
LR = 1e-3       # æœ‰é™å·®åˆ†æ³•éœ€è¦ç¨å¤§çš„å­¦ä¹ ç‡
STEPS = 500     # æ¼”ç¤º 500 æ­¥
VOCAB_SIZE = 257
EPSILON = 1e-4  # æ‰°åŠ¨å¤§å°

def get_memory_usage():
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024 # MB

def train_zero_memory():
    print(f"ğŸš€ [H2Q-ZeroMem] å¯åŠ¨é›¶å†…å­˜æ¢¯åº¦è®­ç»ƒ... è®¾å¤‡: {DEVICE}")
    
    # 1. åˆå§‹åŒ–æ¨¡å‹
    # æˆ‘ä»¬ä½¿ç”¨æœ€ç®€å•çš„ Knot Kernel è¿›è¡ŒéªŒè¯
    model = H2Q_Knot_Kernel(max_dim=256, vocab_size=VOCAB_SIZE, depth=6).to(DEVICE)
    
    # 2. æ•°æ®
    loader = get_vision_dataloader(split="train", batch_size=BATCH_SIZE)
    loss_fn = nn.CrossEntropyLoss()
    
    model.train()
    progress_bar = tqdm(range(STEPS), desc="Zero-Memory Training")
    data_iter = iter(loader)
    
    initial_mem = get_memory_usage()
    print(f"   åˆå§‹å†…å­˜: {initial_mem:.2f} MB")
    
    for step in progress_bar:
        try:
            batch = next(data_iter).to(DEVICE)
        except StopIteration:
            data_iter = iter(loader)
            batch = next(data_iter).to(DEVICE)
            
        # --- [æ ¸å¿ƒ] é›¶å†…å­˜æ¢¯åº¦è®¡ç®— ---
        # æ•´ä¸ªè¿‡ç¨‹åœ¨ no_grad ä¸‹è¿›è¡Œ
        with torch.no_grad():
            
            # éå†æ¨¡å‹ä¸­çš„æ¯ä¸€ä¸ªå‚æ•°
            for param in model.parameters():
                
                # 1. åˆ›å»ºä¸€ä¸ªç”¨äºå­˜å‚¨æ¢¯åº¦çš„ç©ºé—´
                if not hasattr(param, 'grad_manual'):
                    param.grad_manual = torch.zeros_like(param.data)
                
                # 2. è¿­ä»£è®¡ç®—æ¯ä¸ªå…ƒç´ çš„æ¢¯åº¦
                # (è¿™æ˜¯ä¸€ä¸ªéå¸¸æ…¢çš„è¿‡ç¨‹ï¼Œä»…ç”¨äºéªŒè¯å†…å­˜)
                # ä¼˜åŒ–ï¼šæˆ‘ä»¬å¯ä»¥å¯¹æ•´ä¸ªå‚æ•°å¼ é‡è¿›è¡Œæ‰°åŠ¨
                
                # ä¿å­˜åŸå§‹å‚æ•°å€¼
                original_val = param.data.clone()
                
                # 3. è®¡ç®— L(w + e)
                param.data.add_(EPSILON)
                logits_plus, _ = model(batch)
                loss_plus = loss_fn(logits_plus.reshape(-1, VOCAB_SIZE), batch.reshape(-1))
                
                # 4. è®¡ç®— L(w - e)
                # è¿˜åŸå¹¶å‡å» epsilon
                param.data.copy_(original_val)
                param.data.sub_(EPSILON)
                logits_minus, _ = model(batch)
                loss_minus = loss_fn(logits_minus.reshape(-1, VOCAB_SIZE), batch.reshape(-1))
                
                # 5. è®¡ç®—æ¢¯åº¦
                grad = (loss_plus - loss_minus) / (2 * EPSILON)
                
                # å­˜å‚¨æ¢¯åº¦
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è®¡ç®—çš„æ˜¯æ•´ä¸ªå‚æ•°å¼ é‡çš„å¹³å‡æ¢¯åº¦ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€åŒ–
                # çœŸæ­£çš„æœ‰é™å·®åˆ†éœ€è¦å¯¹æ¯ä¸ªå…ƒç´ æ“ä½œï¼Œä¼šæ›´æ…¢
                param.grad_manual.fill_(grad)
                
                # è¿˜åŸå‚æ•°
                param.data.copy_(original_val)

            # --- æ‰‹åŠ¨æ›´æ–°æƒé‡ ---
            for param in model.parameters():
                param.data.sub_(LR * param.grad_manual)

        # ç›‘æ§
        if step % 10 == 0:
            mem_usage = get_memory_usage()
            # æˆ‘ä»¬ç”¨ loss_plus ä½œä¸ºå½“å‰ loss çš„è¿‘ä¼¼å€¼
            progress_bar.set_postfix({
                "Loss": f"{loss_plus.item():.4f}", 
                "Mem": f"{mem_usage:.2f} MB"
            })

    final_mem = get_memory_usage()
    print("âœ… è®­ç»ƒå®Œæˆã€‚")
    print(f"   æœ€ç»ˆå†…å­˜: {final_mem:.2f} MB")
    print(f"   å†…å­˜å¢é‡: {final_mem - initial_mem:.2f} MB")

if __name__ == "__main__":
    train_zero_memory()