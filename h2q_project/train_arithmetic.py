# train_arithmetic.py

import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import random
from h2q.hierarchical_system import H2Q_Hierarchical_System
from h2q.hierarchical_decoder import ConceptDecoder

# --- é…ç½® ---
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
BATCH_SIZE = 64
SEQ_LEN = 32 # çŸ­åºåˆ—ï¼š "123+456=579"
LR = 1e-3
STEPS = 5000
# å¤ç”¨ä½ è®­ç»ƒå¥½çš„æ— å™ªæƒé‡
SPELLING_WEIGHTS = "h2q_model_knot.pth" 
DECODER_WEIGHTS = "h2q_model_decoder.pth" # é‚£ä¸ª Loss=0.01 çš„å®Œç¾è§£ç å™¨

def generate_arithmetic_batch(batch_size):
    """ç”ŸæˆåŠ æ³•æ•°æ®æµ: '123+45=168' (è¡¥é½åˆ° SEQ_LEN)"""
    batch_data = []
    for _ in range(batch_size):
        a = random.randint(0, 9999)
        b = random.randint(0, 9999)
        c = a + b
        text = f"{a}+{b}={c}"
        # è¡¥é½
        bytes_data = list(text.encode('utf-8'))
        if len(bytes_data) < SEQ_LEN:
            bytes_data += [0] * (SEQ_LEN - len(bytes_data))
        else:
            bytes_data = bytes_data[:SEQ_LEN]
        batch_data.append(bytes_data)
    return torch.tensor(batch_data, dtype=torch.long).to(DEVICE)

def train_math_core():
    print(f"ğŸš€ [H2Q-Math] å¯åŠ¨ç®—æœ¯å‡ ä½•æ ¸å¿ƒè®­ç»ƒ... è®¾å¤‡: {DEVICE}")
    
    # 1. åŠ è½½å…¨å¥—ç³»ç»Ÿ (Encoder + Decoder)
    # æ³¨æ„ï¼šæˆ‘ä»¬è¿™æ¬¡è¦è®­ç»ƒ Encoder çš„ L1 å±‚æ¥å­¦ä¼šâ€œåŠ æ³•é€»è¾‘â€
    # L0 (æ‹¼å†™) å’Œ Decoder (è¿˜åŸ) ä¿æŒå†»ç»“ï¼
    
    encoder = H2Q_Hierarchical_System(vocab_size=257, dim=256, spelling_weights_path=SPELLING_WEIGHTS)
    decoder = ConceptDecoder(dim=256, vocab_size=257, stride=8)
    
    # åŠ è½½å®Œç¾çš„è§£ç å™¨æƒé‡
    if os.path.exists(DECODER_WEIGHTS):
        decoder.load_state_dict(torch.load(DECODER_WEIGHTS))
        print("âœ… å®Œç¾è§£ç å™¨å·²åŠ è½½ (ä½œä¸ºè¾“å‡ºæ ¡éªŒ)")
    
    encoder.to(DEVICE)
    decoder.to(DEVICE)
    
    # å†»ç»“ L0 å’Œ Decoderï¼Œåªè®­ç»ƒ L1 (æ¦‚å¿µæ ¸)
    # æˆ‘ä»¬å¸Œæœ› L1 å­¦ä¼šï¼š æ¦‚å¿µ(123+45) -> æ¦‚å¿µ(168)
    for p in encoder.spelling_kernel.parameters(): p.requires_grad = False
    for p in decoder.parameters(): p.requires_grad = False
    
    optimizer = optim.AdamW(encoder.concept_layers.parameters(), lr=LR)
    
    progress_bar = tqdm(range(STEPS), desc="Learning Addition Geometry")
    
    for step in progress_bar:
        inputs = generate_arithmetic_batch(BATCH_SIZE)
        
        # ç›®æ ‡ï¼šè¾“å…¥æ˜¯å®Œæ•´ç®—å¼ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹èƒ½å­¦ä¼šè¿™ç§åºåˆ—ç»“æ„
        # è‡ªå›å½’é¢„æµ‹ï¼šè¾“å…¥å‰ N ä¸ªï¼Œé¢„æµ‹ç¬¬ N+1 ä¸ª
        # ä½†ä¸ºäº†ç®€åŒ–éªŒè¯â€œè®¡ç®—èƒ½åŠ›â€ï¼Œæˆ‘ä»¬è¿™é‡Œåšè‡ªç¼–ç è®­ç»ƒ
        # çœ‹å®ƒèƒ½å¦åœ¨æ¦‚å¿µå±‚â€œå‹ç¼©â€å¹¶â€œæ— æŸè¿˜åŸâ€ç®—å¼
        
        # 1. ç¼–ç 
        _, concept_stream = encoder(inputs)
        
        # 2. è§£ç 
        recon_logits = decoder(concept_stream)
        
        # 3. æŸå¤± (å¿…é¡»æä½ï¼Œæ•°å­¦å®¹ä¸å¾—è¯¯å·®)
        loss = nn.CrossEntropyLoss()(recon_logits.reshape(-1, 257), inputs.reshape(-1))
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        progress_bar.set_postfix({"MathLoss": f"{loss.item():.4f}"})
        
        # éªŒè¯
        if step % 1000 == 0:
            # å–ä¸€ä¸ªæ ·æœ¬çœ‹æ•ˆæœ
            pred_bytes = torch.argmax(recon_logits[0], dim=-1).cpu().tolist()
            try:
                text = bytes(pred_bytes).decode('utf-8', errors='ignore').replace('\x00', '')
                tqdm.write(f"\nğŸ§® [Step {step}] è¿˜åŸç®—å¼: {text}")
            except: pass

    print("âœ… æ•°å­¦æ ¸å¿ƒéªŒè¯å®Œæˆã€‚")

if __name__ == "__main__":
    import os
    train_math_core()