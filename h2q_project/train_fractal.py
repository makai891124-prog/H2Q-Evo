# train_fractal.py

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from tqdm import tqdm
import os

# å¯¼å…¥ç³»ç»Ÿå’Œæ•°æ®åŠ è½½å™¨
from h2q.system import AutonomousSystem
from tools.byte_loader import get_byte_dataloader

# --- é…ç½® ---
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
BATCH_SIZE = 32   # å­—èŠ‚çº§ Batch å¯ä»¥å¤§ä¸€ç‚¹
SEQ_LEN = 256     # å­—èŠ‚æµéœ€è¦æ›´é•¿çš„ä¸Šä¸‹æ–‡
LR = 3e-4         # å­¦ä¹ ç‡
STEPS = 2000      # è®­ç»ƒæ­¥æ•°
VOCAB_SIZE = 257  # 0-255 + EOS

def train_fractal():
    print(f"ğŸš€ [H2Q-Fractal] å¯åŠ¨åˆ†å½¢æ¶æ„éªŒè¯... è®¾å¤‡: {DEVICE}")
    print("   æ ¸å¿ƒå‡è®¾: æ™ºèƒ½æºäº 2->256 çš„å¯¹ç§°æ€§ç ´ç¼ºå±•å¼€")
    
    # 1. åˆå§‹åŒ–ç³»ç»Ÿ
    # DDE ä¼šè‡ªåŠ¨è°ƒç”¨æˆ‘ä»¬åˆšåˆšæ›´æ–°çš„ H2Q_Geometric_Kernel (åŒ…å«åˆ†å½¢åµŒå…¥)
    system = AutonomousSystem(context_dim=256, action_dim=256)
    
    # å¼ºåˆ¶ç¡®ä¿å†…æ ¸å‚æ•°æ­£ç¡® (Byte-Level)
    from h2q.gut_kernel import H2Q_Geometric_Kernel
    system.dde.kernel = H2Q_Geometric_Kernel(dim=256, vocab_size=VOCAB_SIZE, depth=12)
    system.dde.to(DEVICE)
    
    # 2. æ•°æ®å‡†å¤‡ (WikiText Byte Stream)
    train_loader = get_byte_dataloader(split="train", batch_size=BATCH_SIZE, seq_len=SEQ_LEN)
    
    # 3. ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(system.dde.parameters(), lr=LR)
    
    # 4. è®­ç»ƒå¾ªç¯
    history = []
    system.dde.train()
    
    progress_bar = tqdm(range(STEPS), desc="Fractal Evolution")
    data_iter = iter(train_loader)
    
    for step in progress_bar:
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(train_loader)
            batch = next(data_iter)
            
        inputs = batch.to(DEVICE)
        context = inputs[:, :-1]
        targets = inputs[:, 1:]
        
        # å‰å‘ä¼ æ’­ (è§¦å‘åˆ†å½¢å±•å¼€)
        logits, _ = system.dde.kernel(context)
        
        # è®¡ç®— Loss
        loss = nn.CrossEntropyLoss()(logits.reshape(-1, VOCAB_SIZE), targets.reshape(-1))
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        history.append(loss.item())
        progress_bar.set_postfix({"Loss": f"{loss.item():.4f}"})
        
        # --- ç”Ÿæˆæ¼”ç¤º (æ¯500æ­¥) ---
        if step % 500 == 0 and step > 0:
            system.dde.eval()
            # ç§å­: "The " çš„å­—èŠ‚ç¼–ç 
            seed = torch.tensor([list(b"The ")], dtype=torch.long).to(DEVICE)
            
            with torch.no_grad():
                for _ in range(50): # ç”Ÿæˆ50ä¸ªå­—èŠ‚
                    logits, _ = system.dde.kernel(seed)
                    next_byte_logits = logits[:, -1, :]
                    probs = torch.nn.functional.softmax(next_byte_logits, dim=-1)
                    next_byte = torch.multinomial(probs, num_samples=1)
                    seed = torch.cat([seed, next_byte], dim=1)
            
            # è§£ç å­—èŠ‚æµ
            generated_bytes = seed[0].cpu().tolist()
            try:
                text = bytes(generated_bytes).decode('utf-8', errors='ignore')
                tqdm.write(f"\nğŸ” [Step {step}] åˆ†å½¢ç”Ÿæˆ: {repr(text)}")
            except:
                pass
            system.dde.train()

    print("âœ… åˆ†å½¢éªŒè¯å®Œæˆã€‚")
    
    # 5. ç»˜å›¾
    plt.figure(figsize=(10, 6))
    plt.plot(history, color='purple', alpha=0.7, label='Fractal Loss')
    plt.title("H2Q Fractal Embedding: Convergence Analysis")
    plt.xlabel("Steps")
    plt.ylabel("Byte-Level Loss")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

if __name__ == "__main__":
    train_fractal()