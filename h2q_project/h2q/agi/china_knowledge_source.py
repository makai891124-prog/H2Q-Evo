#!/usr/bin/env python3
"""ä¸­å›½å¤§é™†ç½‘ç»œç¯å¢ƒçŸ¥è¯†æº.

é’ˆå¯¹ä¸­å›½å¤§é™†ç½‘ç»œç¯å¢ƒä¼˜åŒ–çš„çŸ¥è¯†è·å–æ¨¡å—:
1. Hugging Face é•œåƒ (hf-mirror.com)
2. ç™¾åº¦ç™¾ç§‘ API
3. å›½å†…å¼€æºæ•°æ®é›†
4. æµå¼ä¸‹è½½æ”¯æŒ
"""

import os
import sys
import json
import time
import hashlib
import urllib.request
import urllib.error
import ssl
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Generator, Tuple
from dataclasses import dataclass, field
import threading
from queue import Queue

# ============================================================================
# é…ç½®
# ============================================================================

@dataclass
class ChinaSourceConfig:
    """ä¸­å›½æºé…ç½®."""
    # Hugging Face é•œåƒ
    hf_mirror: str = "https://hf-mirror.com"
    hf_endpoint: str = "https://hf-mirror.com"
    
    # å›½å†…å¯ç”¨æº
    baidu_baike_api: str = "https://baike.baidu.com/api/openapi/BaikeLemmaCardApi"
    zhihu_search: str = "https://www.zhihu.com/api/v4/search_v3"
    
    # æ•°æ®é›†é…ç½®
    datasets: List[str] = field(default_factory=lambda: [
        "shibing624/alpaca-zh",      # ä¸­æ–‡Alpaca
        "BelleGroup/train_0.5M_CN",  # Belleä¸­æ–‡æ•°æ®
        "fnlp/moss-sft-data",        # MOSSæ•°æ®
        "THUDM/webglm-qa",           # WebGLMé—®ç­”
    ])
    
    # ä¸‹è½½é…ç½®
    chunk_size: int = 8192           # æµå¼ä¸‹è½½å—å¤§å°
    timeout: int = 30                # è¯·æ±‚è¶…æ—¶
    max_retries: int = 3             # æœ€å¤§é‡è¯•æ¬¡æ•°
    cache_dir: str = ".cache/knowledge"


# ============================================================================
# SSL å’Œä»£ç†é…ç½®
# ============================================================================

def create_ssl_context() -> ssl.SSLContext:
    """åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼ˆå¤„ç†è¯ä¹¦é—®é¢˜ï¼‰."""
    ctx = ssl.create_default_context()
    ctx.check_hostname = False
    ctx.verify_mode = ssl.CERT_NONE
    return ctx


def get_opener() -> urllib.request.OpenerDirector:
    """è·å–URLæ‰“å¼€å™¨ï¼ˆæ”¯æŒä»£ç†ï¼‰."""
    handlers = []
    
    # æ£€æŸ¥ä»£ç†ç¯å¢ƒå˜é‡
    http_proxy = os.environ.get('HTTP_PROXY') or os.environ.get('http_proxy')
    https_proxy = os.environ.get('HTTPS_PROXY') or os.environ.get('https_proxy')
    
    if http_proxy or https_proxy:
        proxy_dict = {}
        if http_proxy:
            proxy_dict['http'] = http_proxy
        if https_proxy:
            proxy_dict['https'] = https_proxy
        handlers.append(urllib.request.ProxyHandler(proxy_dict))
    
    # SSLå¤„ç†
    handlers.append(urllib.request.HTTPSHandler(context=create_ssl_context()))
    
    return urllib.request.build_opener(*handlers)


# ============================================================================
# Hugging Face é•œåƒæ•°æ®é›†è®¿é—®
# ============================================================================

class HFMirrorDatasetLoader:
    """Hugging Face é•œåƒæ•°æ®é›†åŠ è½½å™¨."""
    
    def __init__(self, config: ChinaSourceConfig = None):
        self.config = config or ChinaSourceConfig()
        self.opener = get_opener()
        self.cache_dir = Path(self.config.cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡
        self.downloaded_bytes = 0
        self.items_loaded = 0
    
    def _make_request(self, url: str, headers: Dict = None) -> Optional[bytes]:
        """å‘é€è¯·æ±‚."""
        default_headers = {
            'User-Agent': 'H2Q-AGI/1.0 (Educational Research)',
            'Accept': 'application/json',
        }
        if headers:
            default_headers.update(headers)
        
        req = urllib.request.Request(url, headers=default_headers)
        
        for attempt in range(self.config.max_retries):
            try:
                with self.opener.open(req, timeout=self.config.timeout) as response:
                    return response.read()
            except Exception as e:
                if attempt < self.config.max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                continue
        
        return None
    
    def get_dataset_info(self, dataset_id: str) -> Optional[Dict]:
        """è·å–æ•°æ®é›†ä¿¡æ¯."""
        url = f"{self.config.hf_mirror}/api/datasets/{dataset_id}"
        
        data = self._make_request(url)
        if data:
            try:
                return json.loads(data.decode('utf-8'))
            except:
                pass
        return None
    
    def stream_dataset_samples(self, dataset_id: str, 
                                split: str = "train",
                                max_samples: int = 100) -> Generator[Dict, None, None]:
        """æµå¼è·å–æ•°æ®é›†æ ·æœ¬.
        
        ä½¿ç”¨ datasets åº“çš„æµå¼åŠ è½½æˆ–ç›´æ¥è¯·æ±‚parquet/jsonlæ–‡ä»¶.
        """
        # æ–¹æ¡ˆ1: å°è¯•ä½¿ç”¨ HF datasets streaming
        try:
            # è®¾ç½®é•œåƒç¯å¢ƒå˜é‡
            os.environ['HF_ENDPOINT'] = self.config.hf_endpoint
            
            from datasets import load_dataset
            
            dataset = load_dataset(
                dataset_id, 
                split=split, 
                streaming=True,
                trust_remote_code=True
            )
            
            count = 0
            for item in dataset:
                if count >= max_samples:
                    break
                yield item
                count += 1
                self.items_loaded += 1
            
            return
            
        except ImportError:
            pass  # datasets åº“æœªå®‰è£…
        except Exception as e:
            pass  # å…¶ä»–é”™è¯¯ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ
        
        # æ–¹æ¡ˆ2: ç›´æ¥è¯·æ±‚ README æˆ–ç¤ºä¾‹æ•°æ®
        yield from self._fetch_dataset_readme(dataset_id)
    
    def _fetch_dataset_readme(self, dataset_id: str) -> Generator[Dict, None, None]:
        """è·å–æ•°æ®é›† README ä½œä¸ºçŸ¥è¯†."""
        url = f"{self.config.hf_mirror}/{dataset_id}/raw/main/README.md"
        
        data = self._make_request(url)
        if data:
            content = data.decode('utf-8', errors='ignore')
            
            # åˆ†æ®µå¤„ç† README
            sections = content.split('\n## ')
            for i, section in enumerate(sections[:5]):  # æœ€å¤š5ä¸ªæ®µè½
                yield {
                    "source": "hf_mirror",
                    "dataset": dataset_id,
                    "section": i,
                    "content": section[:2000],  # é™åˆ¶é•¿åº¦
                    "timestamp": datetime.now().isoformat()
                }
                self.items_loaded += 1
    
    def download_sample_file(self, dataset_id: str, 
                             filename: str = "train.jsonl",
                             max_lines: int = 100) -> List[Dict]:
        """ä¸‹è½½æ ·æœ¬æ–‡ä»¶ï¼ˆæµå¼ï¼‰."""
        samples = []
        
        # å°è¯•å¤šç§æ–‡ä»¶æ ¼å¼
        possible_files = [
            f"{filename}",
            "data/train.jsonl",
            "train/data-00000-of-00001.parquet",
            "train.json",
        ]
        
        for file_path in possible_files:
            url = f"{self.config.hf_mirror}/{dataset_id}/resolve/main/{file_path}"
            
            try:
                req = urllib.request.Request(url, headers={
                    'User-Agent': 'H2Q-AGI/1.0'
                })
                
                with self.opener.open(req, timeout=self.config.timeout) as response:
                    line_count = 0
                    buffer = b""
                    
                    while line_count < max_lines:
                        chunk = response.read(self.config.chunk_size)
                        if not chunk:
                            break
                        
                        buffer += chunk
                        self.downloaded_bytes += len(chunk)
                        
                        # å¤„ç† JSONL
                        while b'\n' in buffer and line_count < max_lines:
                            line, buffer = buffer.split(b'\n', 1)
                            try:
                                item = json.loads(line.decode('utf-8'))
                                samples.append(item)
                                line_count += 1
                                self.items_loaded += 1
                            except:
                                continue
                    
                    if samples:
                        return samples
                        
            except Exception as e:
                continue
        
        return samples


# ============================================================================
# ç™¾åº¦ç™¾ç§‘çŸ¥è¯†è·å–
# ============================================================================

class BaiduBaikeAcquirer:
    """ç™¾åº¦ç™¾ç§‘çŸ¥è¯†è·å–å™¨."""
    
    def __init__(self):
        self.opener = get_opener()
        self.acquired_count = 0
    
    def fetch_lemma(self, keyword: str) -> Optional[Dict]:
        """è·å–è¯æ¡ä¿¡æ¯."""
        # ä½¿ç”¨ç™¾åº¦ç™¾ç§‘ OpenAPI (éœ€è¦ç”³è¯· appid)
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„çˆ¬å–æ–¹å¼
        
        url = f"https://baike.baidu.com/item/{urllib.parse.quote(keyword)}"
        
        try:
            req = urllib.request.Request(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml',
            })
            
            with self.opener.open(req, timeout=15) as response:
                html = response.read().decode('utf-8', errors='ignore')
                
                # ç®€å•æå–æ‘˜è¦ï¼ˆå®é™…ä½¿ç”¨éœ€è¦æ›´å®Œå–„çš„è§£æï¼‰
                import re
                
                # å°è¯•æå–æè¿°
                desc_match = re.search(r'<meta name="description" content="([^"]+)"', html)
                if desc_match:
                    summary = desc_match.group(1)
                else:
                    # å¤‡é€‰ï¼šæå–æ­£æ–‡å‰å‡ æ®µ
                    text_match = re.search(r'<div class="lemma-summary"[^>]*>(.*?)</div>', html, re.DOTALL)
                    if text_match:
                        summary = re.sub(r'<[^>]+>', '', text_match.group(1))[:500]
                    else:
                        return None
                
                self.acquired_count += 1
                
                return {
                    "title": keyword,
                    "summary": summary,
                    "source": "baidu_baike",
                    "timestamp": datetime.now().isoformat(),
                    "url": url
                }
                
        except Exception as e:
            return None
    
    def batch_fetch(self, keywords: List[str], delay: float = 1.0) -> List[Dict]:
        """æ‰¹é‡è·å–ï¼ˆå¸¦å»¶è¿Ÿï¼Œé¿å…é™æµï¼‰."""
        results = []
        
        for keyword in keywords:
            result = self.fetch_lemma(keyword)
            if result:
                results.append(result)
            time.sleep(delay)
        
        return results


# ============================================================================
# å›½å†…å¼€æºæ•°æ®é›†ç›®å½•
# ============================================================================

class ChinaOpenDatasets:
    """å›½å†…å¼€æºæ•°æ®é›†."""
    
    # å¯ç”¨çš„ä¸­æ–‡æ•°æ®é›†ï¼ˆHugging Face é•œåƒå¯è®¿é—®ï¼‰
    DATASETS = {
        # ä¸­æ–‡æŒ‡ä»¤æ•°æ®
        "instruction": [
            ("shibing624/alpaca-zh", "ä¸­æ–‡AlpacaæŒ‡ä»¤æ•°æ®"),
            ("BelleGroup/train_0.5M_CN", "Belleä¸­æ–‡æŒ‡ä»¤æ•°æ®"),
            ("fnlp/moss-002-sft-data", "MOSSæŒ‡ä»¤å¾®è°ƒæ•°æ®"),
            ("YeungNLP/firefly-train-1.1M", "æµè¤ä¸­æ–‡å¯¹è¯æ•°æ®"),
        ],
        
        # é—®ç­”æ•°æ®
        "qa": [
            ("THUDM/webglm-qa", "WebGLMé—®ç­”æ•°æ®"),
            ("suolyer/webqa", "ç½‘é¡µé—®ç­”æ•°æ®"),
            ("Duxiaoman-DI/FinCorpus", "é‡‘èé—®ç­”æ•°æ®"),
        ],
        
        # é€šç”¨æ–‡æœ¬
        "text": [
            ("pleisto/wikipedia-cn-20230720-filtered", "ä¸­æ–‡ç»´åŸºç™¾ç§‘"),
            ("liwu/MNBVC", "è¶…å¤§è§„æ¨¡ä¸­æ–‡è¯­æ–™"),
            ("Skywork/SkyPile-150B", "ä¸­æ–‡ç½‘é¡µè¯­æ–™"),
        ],
        
        # æ•°å­¦æ¨ç†
        "math": [
            ("TIGER-Lab/MathInstruct", "æ•°å­¦æŒ‡ä»¤æ•°æ®"),
            ("meta-math/MetaMathQA", "æ•°å­¦é—®ç­”æ•°æ®"),
        ],
        
        # ä»£ç 
        "code": [
            ("bigcode/starcoderdata", "ä»£ç æ•°æ®"),
            ("codeparrot/github-code", "GitHubä»£ç "),
        ]
    }
    
    @classmethod
    def get_datasets_by_category(cls, category: str) -> List[Tuple[str, str]]:
        """æŒ‰ç±»åˆ«è·å–æ•°æ®é›†åˆ—è¡¨."""
        return cls.DATASETS.get(category, [])
    
    @classmethod
    def get_all_datasets(cls) -> List[Tuple[str, str]]:
        """è·å–æ‰€æœ‰æ•°æ®é›†."""
        all_datasets = []
        for datasets in cls.DATASETS.values():
            all_datasets.extend(datasets)
        return all_datasets


# ============================================================================
# ç»¼åˆçŸ¥è¯†è·å–å™¨ï¼ˆä¸­å›½ä¼˜åŒ–ç‰ˆï¼‰
# ============================================================================

class ChinaKnowledgeAcquirer:
    """ä¸­å›½ç½‘ç»œç¯å¢ƒä¼˜åŒ–çš„çŸ¥è¯†è·å–å™¨."""
    
    def __init__(self, config: ChinaSourceConfig = None):
        self.config = config or ChinaSourceConfig()
        
        # ç»„ä»¶
        self.hf_loader = HFMirrorDatasetLoader(self.config)
        self.baike_acquirer = BaiduBaikeAcquirer()
        
        # ç»Ÿè®¡
        self.total_acquired = 0
        self.source_stats: Dict[str, int] = {}
        
        # çŸ¥è¯†ç¼“å­˜
        self.knowledge_cache: List[Dict] = []
        self.cache_lock = threading.Lock()
    
    def acquire_from_hf_dataset(self, dataset_id: str, 
                                 max_samples: int = 50) -> List[Dict]:
        """ä» HF é•œåƒè·å–æ•°æ®é›†æ ·æœ¬."""
        samples = []
        
        print(f"  ğŸ“¥ ä» HF é•œåƒè·å–: {dataset_id}")
        
        try:
            for item in self.hf_loader.stream_dataset_samples(dataset_id, max_samples=max_samples):
                samples.append({
                    "source": "hf_mirror",
                    "dataset": dataset_id,
                    "content": item,
                    "timestamp": datetime.now().isoformat()
                })
                
                if len(samples) >= max_samples:
                    break
            
            self._update_stats("hf_mirror", len(samples))
            print(f"    âœ… è·å– {len(samples)} æ¡æ ·æœ¬")
            
        except Exception as e:
            print(f"    âŒ è·å–å¤±è´¥: {e}")
        
        return samples
    
    def acquire_from_baike(self, keywords: List[str]) -> List[Dict]:
        """ä»ç™¾åº¦ç™¾ç§‘è·å–çŸ¥è¯†."""
        print(f"  ğŸ“– ä»ç™¾åº¦ç™¾ç§‘è·å–: {len(keywords)} ä¸ªå…³é”®è¯")
        
        results = self.baike_acquirer.batch_fetch(keywords, delay=0.5)
        
        self._update_stats("baidu_baike", len(results))
        print(f"    âœ… è·å– {len(results)} æ¡çŸ¥è¯†")
        
        return results
    
    def auto_acquire(self, categories: List[str] = None,
                     max_per_source: int = 20) -> List[Dict]:
        """è‡ªåŠ¨ä»å¤šä¸ªæºè·å–çŸ¥è¯†."""
        all_knowledge = []
        
        categories = categories or ["instruction", "qa", "math"]
        
        print("ğŸŒ å¼€å§‹è‡ªåŠ¨çŸ¥è¯†è·å–...")
        
        # 1. ä» HF é•œåƒè·å–æ•°æ®é›†æ ·æœ¬
        for category in categories:
            datasets = ChinaOpenDatasets.get_datasets_by_category(category)
            
            for dataset_id, desc in datasets[:2]:  # æ¯ç±»æœ€å¤š2ä¸ªæ•°æ®é›†
                samples = self.acquire_from_hf_dataset(dataset_id, max_samples=max_per_source)
                all_knowledge.extend(samples)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(1)
        
        # 2. ä»ç™¾åº¦ç™¾ç§‘è·å–å…³é”®è¯
        baike_keywords = [
            "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ",
            "è‡ªç„¶è¯­è¨€å¤„ç†", "è®¡ç®—æœºè§†è§‰", "å¼ºåŒ–å­¦ä¹ ",
            "æ•°å­¦", "çº¿æ€§ä»£æ•°", "æ¦‚ç‡è®º", "å¾®ç§¯åˆ†"
        ]
        
        baike_results = self.acquire_from_baike(baike_keywords[:max_per_source])
        all_knowledge.extend(baike_results)
        
        # ç¼“å­˜
        with self.cache_lock:
            self.knowledge_cache.extend(all_knowledge)
        
        print(f"\nğŸ“Š çŸ¥è¯†è·å–å®Œæˆ: å…± {len(all_knowledge)} æ¡")
        for source, count in self.source_stats.items():
            print(f"   - {source}: {count} æ¡")
        
        return all_knowledge
    
    def _update_stats(self, source: str, count: int):
        """æ›´æ–°ç»Ÿè®¡."""
        self.total_acquired += count
        self.source_stats[source] = self.source_stats.get(source, 0) + count
    
    def get_random_knowledge(self, n: int = 5) -> List[Dict]:
        """éšæœºè·å–ç¼“å­˜çš„çŸ¥è¯†."""
        import random
        
        with self.cache_lock:
            if not self.knowledge_cache:
                return []
            return random.sample(self.knowledge_cache, min(n, len(self.knowledge_cache)))


# ============================================================================
# ç½‘ç»œæµ‹è¯•
# ============================================================================

def test_china_network() -> Dict[str, bool]:
    """æµ‹è¯•ä¸­å›½ç½‘ç»œç¯å¢ƒè¿é€šæ€§."""
    results = {}
    opener = get_opener()
    
    test_urls = [
        ("baidu", "https://www.baidu.com"),
        ("hf_mirror", "https://hf-mirror.com"),
        ("baike", "https://baike.baidu.com"),
        ("github_mirror", "https://ghproxy.com"),
    ]
    
    print("ğŸ” æµ‹è¯•ä¸­å›½ç½‘ç»œç¯å¢ƒ...")
    
    for name, url in test_urls:
        try:
            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
            with opener.open(req, timeout=10) as response:
                results[name] = response.status == 200
                print(f"  âœ… {name}: å¯ç”¨")
        except Exception as e:
            results[name] = False
            print(f"  âŒ {name}: ä¸å¯ç”¨ ({e})")
    
    return results


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """æµ‹è¯•è¿è¡Œ."""
    print("=" * 60)
    print("H2Q AGI ä¸­å›½ç½‘ç»œæºæµ‹è¯•")
    print("=" * 60)
    
    # 1. ç½‘ç»œæµ‹è¯•
    network_status = test_china_network()
    print()
    
    # 2. çŸ¥è¯†è·å–æµ‹è¯•
    if network_status.get("hf_mirror") or network_status.get("baike"):
        acquirer = ChinaKnowledgeAcquirer()
        
        # è‡ªåŠ¨è·å–
        knowledge = acquirer.auto_acquire(
            categories=["instruction", "qa"],
            max_per_source=5
        )
        
        print(f"\nè·å–çš„çŸ¥è¯†æ ·æœ¬:")
        for i, item in enumerate(knowledge[:3]):
            print(f"\n[{i+1}] {item.get('source', 'unknown')}")
            content = str(item.get('content', item.get('summary', '')))[:200]
            print(f"    {content}...")
    else:
        print("âš ï¸ ç½‘ç»œä¸å¯ç”¨ï¼Œè·³è¿‡çŸ¥è¯†è·å–æµ‹è¯•")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    main()
