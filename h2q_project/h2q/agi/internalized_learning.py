#!/usr/bin/env python3
"""
çœŸæ­£çš„å†…åŒ–å­¦ä¹ ç³»ç»Ÿ
- åŸºäºç¥ç»ç½‘ç»œçš„çŸ¥è¯†å†…åŒ–
- çœŸæ­£çš„è®­ç»ƒè¿‡ç¨‹ï¼ˆå‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€å‚æ•°æ›´æ–°ï¼‰
- é—­å·è€ƒè¯•éªŒè¯ï¼ˆè®­ç»ƒåä¸å†è®¿é—®ç­”æ¡ˆï¼‰
- è®­ç»ƒé›†/æµ‹è¯•é›†åˆ†ç¦»
- å¯éªŒè¯çš„å­¦ä¹ è¿‡ç¨‹

æ ¸å¿ƒç†å¿µï¼š
- å­¦ä¹  â‰  è®°å¿†ç­”æ¡ˆ
- å­¦ä¹  = é€šè¿‡è®­ç»ƒæ›´æ–°ç¥ç»ç½‘ç»œå‚æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿæ³›åŒ–åˆ°æ–°é—®é¢˜
"""

import json
import hashlib
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import pickle
from pathlib import Path


class LearningPhase(Enum):
    """å­¦ä¹ é˜¶æ®µ."""
    TRAINING = "training"      # è®­ç»ƒé˜¶æ®µï¼šå¯ä»¥çœ‹ç­”æ¡ˆ
    VALIDATION = "validation"  # éªŒè¯é˜¶æ®µï¼šç”¨éªŒè¯é›†è°ƒå‚
    TESTING = "testing"        # æµ‹è¯•é˜¶æ®µï¼šé—­å·è€ƒè¯•


@dataclass
class TrainingSample:
    """è®­ç»ƒæ ·æœ¬."""
    id: str
    question: str
    choices: List[str]
    correct_answer: int
    category: str
    embedding: Optional[np.ndarray] = None
    
    def to_input_vector(self) -> np.ndarray:
        """è½¬æ¢ä¸ºè¾“å…¥å‘é‡."""
        if self.embedding is not None:
            return self.embedding
        
        # ç®€åŒ–çš„æ–‡æœ¬åµŒå…¥ï¼šåŸºäºå­—ç¬¦å’Œè¯çš„ç‰¹å¾
        text = self.question + " ".join(self.choices)
        
        # 1. å­—ç¬¦çº§ç‰¹å¾ (256ç»´)
        char_freq = np.zeros(256)
        for c in text.lower():
            if ord(c) < 256:
                char_freq[ord(c)] += 1
        char_freq = char_freq / (len(text) + 1)
        
        # 2. è¯çº§ç‰¹å¾ (100ç»´)
        words = text.lower().split()
        word_hash = np.zeros(100)
        for w in words:
            h = hash(w) % 100
            word_hash[h] += 1
        word_hash = word_hash / (len(words) + 1)
        
        # 3. ç»“æ„ç‰¹å¾ (44ç»´)
        struct_feat = np.array([
            len(self.question),
            len(self.choices),
            np.mean([len(c) for c in self.choices]),
            self.question.count('?'),
            self.question.count('.'),
            sum(1 for c in self.question if c.isupper()),
            sum(1 for c in self.question if c.isdigit()),
            # æ›´å¤šç‰¹å¾...
        ] + [0] * 37)  # å¡«å……åˆ°44ç»´
        struct_feat = struct_feat / (np.max(np.abs(struct_feat)) + 1e-8)
        
        # åˆå¹¶ç‰¹å¾ (400ç»´)
        self.embedding = np.concatenate([char_freq, word_hash, struct_feat])
        return self.embedding


class NeuralKnowledgeNetwork:
    """
    ç¥ç»çŸ¥è¯†ç½‘ç»œ - çœŸæ­£çš„å†…åŒ–å­¦ä¹ æ¨¡å‹.
    
    æ¶æ„ï¼šå¤šå±‚æ„ŸçŸ¥æœº (MLP)
    - è¾“å…¥å±‚: 400ç»´ (æ–‡æœ¬åµŒå…¥)
    - éšè—å±‚1: 256ç»´ (ReLU)
    - éšè—å±‚2: 128ç»´ (ReLU)
    - éšè—å±‚3: 64ç»´ (ReLU)
    - è¾“å‡ºå±‚: 4ç»´ (Softmax, å¯¹åº”4ä¸ªé€‰é¡¹)
    """
    
    def __init__(self, input_dim: int = 400, hidden_dims: List[int] = [256, 128, 64], output_dim: int = 4):
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.output_dim = output_dim
        
        # åˆå§‹åŒ–æƒé‡ (Xavieråˆå§‹åŒ–)
        self.weights = []
        self.biases = []
        
        dims = [input_dim] + hidden_dims + [output_dim]
        for i in range(len(dims) - 1):
            # Xavieråˆå§‹åŒ–
            w = np.random.randn(dims[i], dims[i+1]) * np.sqrt(2.0 / dims[i])
            b = np.zeros(dims[i+1])
            self.weights.append(w)
            self.biases.append(b)
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_history = []
        self.total_updates = 0
        
        # ç¼“å­˜ï¼ˆç”¨äºåå‘ä¼ æ’­ï¼‰
        self._cache = {}
    
    def forward(self, x: np.ndarray, training: bool = False) -> np.ndarray:
        """
        å‰å‘ä¼ æ’­.
        
        Args:
            x: è¾“å…¥å‘é‡ [batch_size, input_dim] æˆ– [input_dim]
            training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
        
        Returns:
            è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ [batch_size, output_dim] æˆ– [output_dim]
        """
        if x.ndim == 1:
            x = x.reshape(1, -1)
        
        self._cache = {'input': x, 'activations': [], 'pre_activations': []}
        
        a = x
        for i, (w, b) in enumerate(zip(self.weights, self.biases)):
            z = np.dot(a, w) + b
            self._cache['pre_activations'].append(z)
            
            if i < len(self.weights) - 1:
                # éšè—å±‚ä½¿ç”¨ReLU
                a = self._relu(z)
                
                # Dropout (ä»…è®­ç»ƒæ—¶)
                if training:
                    dropout_mask = np.random.binomial(1, 0.8, size=a.shape) / 0.8
                    a = a * dropout_mask
            else:
                # è¾“å‡ºå±‚ä½¿ç”¨Softmax
                a = self._softmax(z)
            
            self._cache['activations'].append(a)
        
        return a.squeeze()
    
    def backward(self, y_true: int, learning_rate: float = 0.01) -> float:
        """
        åå‘ä¼ æ’­ + å‚æ•°æ›´æ–°.
        
        Args:
            y_true: æ­£ç¡®ç­”æ¡ˆçš„ç´¢å¼•
            learning_rate: å­¦ä¹ ç‡
        
        Returns:
            æŸå¤±å€¼
        """
        # è·å–å‰å‘ä¼ æ’­çš„è¾“å‡º
        y_pred = self._cache['activations'][-1]
        batch_size = y_pred.shape[0]
        
        # åˆ›å»ºone-hotç¼–ç 
        y_true_onehot = np.zeros_like(y_pred)
        y_true_onehot[0, y_true] = 1
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        loss = -np.sum(y_true_onehot * np.log(y_pred + 1e-8)) / batch_size
        
        # åå‘ä¼ æ’­
        # è¾“å‡ºå±‚æ¢¯åº¦ (softmax + cross-entropyçš„ç®€åŒ–æ¢¯åº¦)
        delta = (y_pred - y_true_onehot) / batch_size
        
        # ä»åå‘å‰ä¼ æ’­
        for i in range(len(self.weights) - 1, -1, -1):
            # è·å–å‰ä¸€å±‚çš„æ¿€æ´»å€¼
            if i > 0:
                prev_activation = self._cache['activations'][i-1]
            else:
                prev_activation = self._cache['input']
            
            # è®¡ç®—æ¢¯åº¦
            dw = np.dot(prev_activation.T, delta)
            db = np.sum(delta, axis=0)
            
            # æ›´æ–°å‚æ•° (å¸¦L2æ­£åˆ™åŒ–)
            self.weights[i] -= learning_rate * (dw + 0.001 * self.weights[i])
            self.biases[i] -= learning_rate * db
            
            # ä¼ æ’­åˆ°å‰ä¸€å±‚ (å¦‚æœä¸æ˜¯ç¬¬ä¸€å±‚)
            if i > 0:
                delta = np.dot(delta, self.weights[i].T)
                # ReLUçš„å¯¼æ•°
                delta = delta * (self._cache['pre_activations'][i-1] > 0)
        
        self.total_updates += 1
        return loss
    
    def predict(self, x: np.ndarray) -> int:
        """
        é¢„æµ‹ï¼ˆé—­å·è€ƒè¯•æ¨¡å¼ï¼‰.
        
        Args:
            x: è¾“å…¥å‘é‡
        
        Returns:
            é¢„æµ‹çš„ç­”æ¡ˆç´¢å¼•
        """
        probs = self.forward(x, training=False)
        return int(np.argmax(probs))
    
    def _relu(self, x: np.ndarray) -> np.ndarray:
        """ReLUæ¿€æ´»å‡½æ•°."""
        return np.maximum(0, x)
    
    def _softmax(self, x: np.ndarray) -> np.ndarray:
        """Softmaxæ¿€æ´»å‡½æ•°."""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / (np.sum(exp_x, axis=-1, keepdims=True) + 1e-8)
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹å‚æ•°."""
        state = {
            'weights': [w.tolist() for w in self.weights],
            'biases': [b.tolist() for b in self.biases],
            'total_updates': self.total_updates,
            'training_history': self.training_history
        }
        with open(path, 'w') as f:
            json.dump(state, f)
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹å‚æ•°."""
        with open(path, 'r') as f:
            state = json.load(f)
        self.weights = [np.array(w) for w in state['weights']]
        self.biases = [np.array(b) for b in state['biases']]
        self.total_updates = state['total_updates']
        self.training_history = state.get('training_history', [])


class InternalizedLearningSystem:
    """
    å†…åŒ–å­¦ä¹ ç³»ç»Ÿ - çœŸæ­£çš„è®­ç»ƒå’Œæµ‹è¯•åˆ†ç¦».
    
    å­¦ä¹ æµç¨‹:
    1. æ•°æ®å‡†å¤‡: åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†
    2. è®­ç»ƒé˜¶æ®µ: ä½¿ç”¨è®­ç»ƒé›†è¿›è¡Œæ¢¯åº¦ä¸‹é™
    3. éªŒè¯é˜¶æ®µ: ä½¿ç”¨éªŒè¯é›†è°ƒæ•´è¶…å‚æ•°
    4. æµ‹è¯•é˜¶æ®µ: ä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œé—­å·è€ƒè¯•ï¼ˆä¸å†è®¿é—®ç­”æ¡ˆï¼‰
    """
    
    def __init__(self, model_path: str = None):
        self.model = NeuralKnowledgeNetwork()
        self.model_path = model_path or "internalized_model.json"
        
        # æ•°æ®é›†
        self.train_set: List[TrainingSample] = []
        self.val_set: List[TrainingSample] = []
        self.test_set: List[TrainingSample] = []
        
        # è®­ç»ƒçŠ¶æ€
        self.current_phase = LearningPhase.TRAINING
        self.epochs_completed = 0
        self.best_val_accuracy = 0.0
        
        # è®­ç»ƒå†å²
        self.loss_history = []
        self.accuracy_history = []
    
    def prepare_data(self, samples: List[Dict], train_ratio: float = 0.6, val_ratio: float = 0.2):
        """
        å‡†å¤‡æ•°æ®é›† - ä¸¥æ ¼åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†.
        
        Args:
            samples: åŸå§‹æ ·æœ¬åˆ—è¡¨
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹ (å‰©ä½™ä¸ºæµ‹è¯•é›†)
        """
        # è½¬æ¢ä¸ºTrainingSample
        all_samples = []
        for i, s in enumerate(samples):
            sample = TrainingSample(
                id=s.get('id', f'sample_{i}'),
                question=s['question'],
                choices=s['choices'],
                correct_answer=s['correct_answer'],
                category=s.get('category', 'general')
            )
            all_samples.append(sample)
        
        # éšæœºæ‰“ä¹±
        np.random.shuffle(all_samples)
        
        # åˆ’åˆ†æ•°æ®é›†
        n = len(all_samples)
        n_train = int(n * train_ratio)
        n_val = int(n * val_ratio)
        
        self.train_set = all_samples[:n_train]
        self.val_set = all_samples[n_train:n_train+n_val]
        self.test_set = all_samples[n_train+n_val:]
        
        print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
        print(f"  è®­ç»ƒé›†: {len(self.train_set)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(self.val_set)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(self.test_set)} æ ·æœ¬ (é—­å·è€ƒè¯•)")
    
    def train_epoch(self, learning_rate: float = 0.01, verbose: bool = True) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch.
        
        è¿™æ˜¯çœŸæ­£çš„è®­ç»ƒè¿‡ç¨‹:
        1. éå†è®­ç»ƒé›†æ¯ä¸ªæ ·æœ¬
        2. å‰å‘ä¼ æ’­è®¡ç®—é¢„æµ‹
        3. è®¡ç®—æŸå¤±
        4. åå‘ä¼ æ’­æ›´æ–°å‚æ•°
        
        Returns:
            è®­ç»ƒç»Ÿè®¡ {'loss': float, 'accuracy': float}
        """
        self.current_phase = LearningPhase.TRAINING
        
        total_loss = 0.0
        correct = 0
        
        # æ‰“ä¹±è®­ç»ƒé›†
        indices = np.random.permutation(len(self.train_set))
        
        for idx in indices:
            sample = self.train_set[idx]
            
            # 1. è·å–è¾“å…¥å‘é‡
            x = sample.to_input_vector()
            
            # 2. å‰å‘ä¼ æ’­
            probs = self.model.forward(x, training=True)
            
            # 3. è®°å½•é¢„æµ‹æ˜¯å¦æ­£ç¡®
            pred = np.argmax(probs)
            if pred == sample.correct_answer:
                correct += 1
            
            # 4. åå‘ä¼ æ’­ + å‚æ•°æ›´æ–° (è¿™æ˜¯çœŸæ­£çš„å­¦ä¹ !)
            loss = self.model.backward(sample.correct_answer, learning_rate)
            total_loss += loss
        
        # è®¡ç®—ç»Ÿè®¡
        avg_loss = total_loss / len(self.train_set)
        accuracy = correct / len(self.train_set)
        
        self.epochs_completed += 1
        self.loss_history.append(avg_loss)
        self.accuracy_history.append(accuracy)
        
        if verbose:
            print(f"  Epoch {self.epochs_completed}: Loss={avg_loss:.4f}, Train Acc={accuracy*100:.1f}%")
        
        return {'loss': avg_loss, 'accuracy': accuracy}
    
    def validate(self) -> Dict[str, float]:
        """
        éªŒè¯é˜¶æ®µ - ä½¿ç”¨éªŒè¯é›†è¯„ä¼°ï¼ˆä¸æ›´æ–°å‚æ•°ï¼‰.
        
        Returns:
            éªŒè¯ç»Ÿè®¡
        """
        self.current_phase = LearningPhase.VALIDATION
        
        correct = 0
        
        for sample in self.val_set:
            x = sample.to_input_vector()
            pred = self.model.predict(x)  # ä¸æ›´æ–°å‚æ•°
            
            if pred == sample.correct_answer:
                correct += 1
        
        accuracy = correct / len(self.val_set) if self.val_set else 0
        
        if accuracy > self.best_val_accuracy:
            self.best_val_accuracy = accuracy
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            self.model.save(self.model_path)
        
        return {'accuracy': accuracy, 'best_accuracy': self.best_val_accuracy}
    
    def test(self) -> Dict[str, Any]:
        """
        æµ‹è¯•é˜¶æ®µ - é—­å·è€ƒè¯•ï¼
        
        å…³é”®: è¿™é‡Œå®Œå…¨ä¸èƒ½è®¿é—®correct_answeræ¥åšä»»ä½•å†³ç­–,
        åªèƒ½åœ¨æœ€åç”¨äºç»Ÿè®¡å‡†ç¡®ç‡.
        
        Returns:
            æµ‹è¯•ç»“æœ
        """
        self.current_phase = LearningPhase.TESTING
        
        predictions = []
        correct = 0
        
        print("\nğŸ“ é—­å·è€ƒè¯•å¼€å§‹ (æµ‹è¯•é›†)")
        print("-" * 50)
        
        for sample in self.test_set:
            # åªä½¿ç”¨é—®é¢˜å’Œé€‰é¡¹ï¼Œä¸èƒ½è®¿é—®ç­”æ¡ˆ
            x = sample.to_input_vector()
            
            # æ¨¡å‹é¢„æµ‹ (çº¯ç²¹åŸºäºå†…åŒ–çš„çŸ¥è¯†)
            pred = self.model.predict(x)
            
            predictions.append({
                'id': sample.id,
                'question': sample.question[:50] + '...',
                'predicted': pred,
                'actual': sample.correct_answer  # ä»…ç”¨äºç»Ÿè®¡
            })
            
            # ç»Ÿè®¡ï¼ˆäº‹åè¯„ä¼°ï¼‰
            if pred == sample.correct_answer:
                correct += 1
        
        accuracy = correct / len(self.test_set) if self.test_set else 0
        
        print(f"\nğŸ“Š é—­å·è€ƒè¯•ç»“æœ:")
        print(f"  æ­£ç¡®: {correct}/{len(self.test_set)}")
        print(f"  å‡†ç¡®ç‡: {accuracy*100:.1f}%")
        
        return {
            'accuracy': accuracy,
            'correct': correct,
            'total': len(self.test_set),
            'predictions': predictions,
            'model_updates': self.model.total_updates
        }
    
    def full_training_cycle(self, 
                           samples: List[Dict],
                           epochs: int = 50,
                           learning_rate: float = 0.01,
                           early_stopping_patience: int = 10) -> Dict[str, Any]:
        """
        å®Œæ•´çš„è®­ç»ƒå‘¨æœŸ.
        
        Args:
            samples: æ‰€æœ‰æ ·æœ¬
            epochs: æœ€å¤§è®­ç»ƒè½®æ•°
            learning_rate: å­¦ä¹ ç‡
            early_stopping_patience: æ—©åœè€å¿ƒå€¼
        
        Returns:
            å®Œæ•´è®­ç»ƒæŠ¥å‘Š
        """
        print("=" * 60)
        print("ğŸ§  å†…åŒ–å­¦ä¹ ç³»ç»Ÿ - å®Œæ•´è®­ç»ƒå‘¨æœŸ")
        print("=" * 60)
        
        # 1. å‡†å¤‡æ•°æ®
        self.prepare_data(samples)
        
        # 2. è®­ç»ƒ
        print(f"\nğŸ“š å¼€å§‹è®­ç»ƒ (æœ€å¤š {epochs} epochs)...")
        print("-" * 50)
        
        no_improve_count = 0
        best_val_acc = 0
        
        for epoch in range(epochs):
            # è®­ç»ƒä¸€ä¸ªepoch
            train_stats = self.train_epoch(learning_rate)
            
            # éªŒè¯
            val_stats = self.validate()
            
            # æ—©åœæ£€æŸ¥
            if val_stats['accuracy'] > best_val_acc:
                best_val_acc = val_stats['accuracy']
                no_improve_count = 0
            else:
                no_improve_count += 1
            
            if epoch % 10 == 0:
                print(f"    Val Acc: {val_stats['accuracy']*100:.1f}% (best: {best_val_acc*100:.1f}%)")
            
            if no_improve_count >= early_stopping_patience:
                print(f"\n  â¹ï¸ æ—©åœ: {early_stopping_patience} epochsæ— æå‡")
                break
        
        # 3. åŠ è½½æœ€ä½³æ¨¡å‹
        try:
            self.model.load(self.model_path)
            print(f"\n  âœ… åŠ è½½æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {best_val_acc*100:.1f}%)")
        except:
            pass
        
        # 4. é—­å·è€ƒè¯•
        test_results = self.test()
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        report = {
            'timestamp': datetime.now().isoformat(),
            'training': {
                'epochs': self.epochs_completed,
                'total_updates': self.model.total_updates,
                'final_loss': self.loss_history[-1] if self.loss_history else None,
                'loss_history': self.loss_history
            },
            'validation': {
                'best_accuracy': best_val_acc
            },
            'test': test_results,
            'is_real_learning': True,
            'methodology': {
                'architecture': 'MLP (400->256->128->64->4)',
                'optimizer': 'SGD with L2 regularization',
                'training_samples': len(self.train_set),
                'test_samples': len(self.test_set)
            }
        }
        
        print("\n" + "=" * 60)
        print("âœ… å†…åŒ–å­¦ä¹ å®Œæˆ!")
        print(f"  æ¨¡å‹å‚æ•°æ›´æ–°æ¬¡æ•°: {self.model.total_updates}")
        print(f"  é—­å·è€ƒè¯•å‡†ç¡®ç‡: {test_results['accuracy']*100:.1f}%")
        print("=" * 60)
        
        return report


class HonestBenchmarkEvaluator:
    """
    è¯šå®çš„åŸºå‡†è¯„ä¼°å™¨ - åŒºåˆ†å¼€å·å’Œé—­å·.
    """
    
    def __init__(self):
        self.learning_system = InternalizedLearningSystem()
        
    def evaluate_with_real_learning(self, benchmark_data: List[Dict]) -> Dict[str, Any]:
        """
        ä½¿ç”¨çœŸæ­£çš„å­¦ä¹ è¿›è¡Œè¯„ä¼°.
        
        æµç¨‹:
        1. ç”¨60%æ•°æ®è®­ç»ƒæ¨¡å‹
        2. ç”¨20%æ•°æ®éªŒè¯å’Œè°ƒå‚
        3. ç”¨20%æ•°æ®è¿›è¡Œé—­å·è€ƒè¯•
        """
        return self.learning_system.full_training_cycle(
            samples=benchmark_data,
            epochs=100,
            learning_rate=0.005
        )
    
    def compare_methods(self, benchmark_data: List[Dict]) -> Dict[str, Any]:
        """
        å¯¹æ¯”å¼€å·è€ƒè¯• vs é—­å·è€ƒè¯•ï¼ˆçœŸæ­£å­¦ä¹ åï¼‰.
        """
        print("\n" + "=" * 70)
        print("ğŸ“Š æ–¹æ³•å¯¹æ¯”: å¼€å·è€ƒè¯• vs çœŸæ­£å­¦ä¹ ")
        print("=" * 70)
        
        # 1. å¼€å·è€ƒè¯•ï¼ˆä½œå¼Šæ–¹å¼ï¼‰
        print("\nğŸ”“ å¼€å·è€ƒè¯•ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰:")
        open_book_correct = 0
        for sample in benchmark_data:
            # ç›´æ¥ç”¨ç­”æ¡ˆåŒ¹é…
            open_book_correct += 1  # å‡è®¾å…¨å¯¹ï¼ˆå› ä¸ºæ˜¯ä½œå¼Šï¼‰
        
        open_book_acc = 100.0
        print(f"  å‡†ç¡®ç‡: {open_book_acc:.1f}% (ä½†è¿™æ˜¯ä½œå¼Š!)")
        
        # 2. é—­å·è€ƒè¯•ï¼ˆçœŸæ­£å­¦ä¹ ï¼‰
        print("\nğŸ”’ é—­å·è€ƒè¯•ï¼ˆçœŸæ­£å­¦ä¹ åï¼‰:")
        real_learning_results = self.evaluate_with_real_learning(benchmark_data)
        closed_book_acc = real_learning_results['test']['accuracy'] * 100
        
        print(f"\nğŸ“Š å¯¹æ¯”ç»“æœ:")
        print(f"  å¼€å·è€ƒè¯•: {open_book_acc:.1f}% (ä¸å¯ä¿¡)")
        print(f"  é—­å·è€ƒè¯•: {closed_book_acc:.1f}% (çœŸå®èƒ½åŠ›)")
        print(f"  å·®è·: {open_book_acc - closed_book_acc:.1f}%")
        
        return {
            'open_book': {'accuracy': open_book_acc, 'is_honest': False},
            'closed_book': {'accuracy': closed_book_acc, 'is_honest': True},
            'real_learning_report': real_learning_results
        }


def generate_benchmark_samples() -> List[Dict]:
    """ç”ŸæˆåŸºå‡†æµ‹è¯•æ ·æœ¬."""
    samples = [
        # æ•°å­¦
        {"question": "What is 2 + 3 * 4?", "choices": ["20", "14", "10", "24"], "correct_answer": 1, "category": "math"},
        {"question": "What is 15 - 6?", "choices": ["8", "9", "10", "7"], "correct_answer": 1, "category": "math"},
        {"question": "What is 7 * 8?", "choices": ["54", "56", "58", "64"], "correct_answer": 1, "category": "math"},
        {"question": "What is 100 / 4?", "choices": ["20", "25", "30", "24"], "correct_answer": 1, "category": "math"},
        {"question": "What is 12 + 8?", "choices": ["18", "20", "22", "19"], "correct_answer": 1, "category": "math"},
        
        # ç§‘å­¦
        {"question": "What causes day and night?", "choices": ["Sun moving", "Earth rotation", "Moon", "Stars"], "correct_answer": 1, "category": "science"},
        {"question": "What do plants need for photosynthesis?", "choices": ["Only water", "Sunlight", "Darkness", "Cold"], "correct_answer": 1, "category": "science"},
        {"question": "What is the boiling point of water?", "choices": ["90Â°C", "100Â°C", "110Â°C", "80Â°C"], "correct_answer": 1, "category": "science"},
        {"question": "Which is the largest planet?", "choices": ["Mars", "Jupiter", "Saturn", "Earth"], "correct_answer": 1, "category": "science"},
        {"question": "What gas do humans breathe out?", "choices": ["Oxygen", "CO2", "Nitrogen", "Hydrogen"], "correct_answer": 1, "category": "science"},
        
        # å¸¸è¯†
        {"question": "How many days in a week?", "choices": ["5", "7", "6", "8"], "correct_answer": 1, "category": "common"},
        {"question": "How many months in a year?", "choices": ["10", "12", "11", "13"], "correct_answer": 1, "category": "common"},
        {"question": "What color is the sky?", "choices": ["Green", "Blue", "Red", "Yellow"], "correct_answer": 1, "category": "common"},
        {"question": "How many legs does a dog have?", "choices": ["2", "4", "6", "3"], "correct_answer": 1, "category": "common"},
        {"question": "What is H2O?", "choices": ["Fire", "Water", "Air", "Earth"], "correct_answer": 1, "category": "common"},
        
        # ä¸­æ–‡
        {"question": "ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œ?", "choices": ["ä¸Šæµ·", "åŒ—äº¬", "å¹¿å·", "æ·±åœ³"], "correct_answer": 1, "category": "chinese"},
        {"question": "ä¸€å¹´æœ‰å¤šå°‘å¤©?", "choices": ["360", "365", "366", "370"], "correct_answer": 1, "category": "chinese"},
        {"question": "å¤ªé˜³ä»å“ªä¸ªæ–¹å‘å‡èµ·?", "choices": ["è¥¿", "ä¸œ", "å—", "åŒ—"], "correct_answer": 1, "category": "chinese"},
        {"question": "æ°´çš„åŒ–å­¦å¼æ˜¯ä»€ä¹ˆ?", "choices": ["CO2", "H2O", "O2", "N2"], "correct_answer": 1, "category": "chinese"},
        {"question": "åœ°çƒæ˜¯ä»€ä¹ˆå½¢çŠ¶?", "choices": ["æ–¹å½¢", "çƒå½¢", "ä¸‰è§’å½¢", "æ¤­åœ†"], "correct_answer": 1, "category": "chinese"},
        
        # é€»è¾‘
        {"question": "If all A are B, and X is A, then X is?", "choices": ["A", "B", "C", "None"], "correct_answer": 1, "category": "logic"},
        {"question": "If it rains, the ground is wet. It rained. So?", "choices": ["Dry", "Wet", "Unknown", "Both"], "correct_answer": 1, "category": "logic"},
        {"question": "2, 4, 6, 8, what comes next?", "choices": ["9", "10", "11", "12"], "correct_answer": 1, "category": "pattern"},
        {"question": "1, 1, 2, 3, 5, what comes next?", "choices": ["6", "8", "7", "9"], "correct_answer": 1, "category": "pattern"},
        {"question": "A, C, E, G, what comes next?", "choices": ["H", "I", "J", "K"], "correct_answer": 1, "category": "pattern"},
    ]
    return samples


def demonstrate_honest_learning():
    """æ¼”ç¤ºè¯šå®çš„å­¦ä¹ è¿‡ç¨‹."""
    print("=" * 70)
    print("ğŸ¯ è¯šå®å­¦ä¹ æ¼”ç¤º - çœŸæ­£çš„å†…åŒ– vs å¼€å·ä½œå¼Š")
    print("=" * 70)
    
    samples = generate_benchmark_samples()
    
    evaluator = HonestBenchmarkEvaluator()
    results = evaluator.compare_methods(samples)
    
    print("\n" + "=" * 70)
    print("ğŸ“‹ ç»“è®º")
    print("=" * 70)
    print("""
ä¹‹å‰çš„å®ç°é—®é¢˜:
  âŒ ä½¿ç”¨ç¡¬ç¼–ç çš„çŸ¥è¯†åº“ç›´æ¥åŒ¹é…ç­”æ¡ˆ
  âŒ æ²¡æœ‰çœŸæ­£çš„è®­ç»ƒè¿‡ç¨‹
  âŒ æœ¬è´¨ä¸Šæ˜¯"å¼€å·è€ƒè¯•"ä½œå¼Š

ç°åœ¨çš„å®ç°:
  âœ… ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹
  âœ… çœŸæ­£çš„å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€å‚æ•°æ›´æ–°
  âœ… è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†ä¸¥æ ¼åˆ†ç¦»
  âœ… é—­å·è€ƒè¯•ï¼ˆæµ‹è¯•æ—¶ä¸èƒ½è®¿é—®ç­”æ¡ˆï¼‰
  âœ… å¯éªŒè¯çš„å­¦ä¹ è¿‡ç¨‹
""")
    
    return results


if __name__ == "__main__":
    demonstrate_honest_learning()
