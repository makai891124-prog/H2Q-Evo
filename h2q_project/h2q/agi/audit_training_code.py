#!/usr/bin/env python3
"""
è®­ç»ƒä»£ç æ·±åº¦å®¡è®¡
Deep Audit of Training Code

æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä»»ä½•å½¢å¼çš„"ä½œå¼Š"æˆ–ä¸çœŸå®è®­ç»ƒ
"""

import os
import sys
from pathlib import Path
from datetime import datetime, timedelta

# ============================================================
# è®­ç»ƒä»£ç å®¡è®¡æŠ¥å‘Š
# ============================================================

def audit_training_code():
    """å®¡è®¡è®­ç»ƒä»£ç """
    
    print("\n" + "=" * 70)
    print("ğŸ” AGIè®­ç»ƒä»£ç æ·±åº¦å®¡è®¡æŠ¥å‘Š")
    print("=" * 70)
    print(f"å®¡è®¡æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    issues = []
    verified = []
    
    # ============================================================
    # å®¡æŸ¥1: æ•°æ®é›†å¤§å°
    # ============================================================
    print("ğŸ“Š å®¡æŸ¥1: æ•°æ®é›†å¤§å°")
    print("-" * 50)
    
    # ä»£ç é…ç½®
    dataset_size = 50000
    train_ratio = 0.9
    train_size = int(dataset_size * train_ratio)  # 45000
    val_size = dataset_size - train_size  # 5000
    batch_size = 64
    batches_per_epoch = train_size // batch_size  # 45000/64 = 703.125 â†’ 703
    
    print(f"  é…ç½®æ•°æ®é›†å¤§å°: {dataset_size:,}")
    print(f"  è®­ç»ƒé›†å¤§å°: {train_size:,} (90%)")
    print(f"  éªŒè¯é›†å¤§å°: {val_size:,} (10%)")
    print(f"  Batch Size: {batch_size}")
    print(f"  æ¯Epoch Batches: {batches_per_epoch}")
    
    # æ—¥å¿—éªŒè¯ - ä»æ—¥å¿—ä¸­æˆ‘ä»¬çœ‹åˆ° 704 batches
    log_batches = 704
    if abs(log_batches - batches_per_epoch) <= 1:
        verified.append("âœ… Batchæ•°é‡ä¸æ•°æ®é›†å¤§å°ä¸€è‡´")
        print(f"  æ—¥å¿—æ˜¾ç¤º: {log_batches} batches âœ… ä¸€è‡´")
    else:
        issues.append("âš ï¸ Batchæ•°é‡ä¸é¢„æœŸä¸ç¬¦")
        print(f"  æ—¥å¿—æ˜¾ç¤º: {log_batches} batches âŒ ä¸ä¸€è‡´")
    
    # ============================================================
    # å®¡æŸ¥2: EpochçœŸå®éå†
    # ============================================================
    print("\nğŸ“Š å®¡æŸ¥2: Epochéå†å®Œæ•´æ€§")
    print("-" * 50)
    
    # æ£€æŸ¥è®­ç»ƒå¾ªç¯
    print("  è®­ç»ƒå¾ªç¯ä»£ç å®¡æŸ¥:")
    print("    for batch_idx, batch in enumerate(self.train_loader):")
    print("        input_ids = batch['input_ids'].to(self.device)")
    print("        labels = batch['label'].to(self.device)")
    print("        ...")
    
    # è¿™æ˜¯æ ‡å‡†çš„DataLoaderéå†
    verified.append("âœ… ä½¿ç”¨æ ‡å‡†DataLoaderéå†")
    print("  ç»“è®º: ä½¿ç”¨PyTorch DataLoaderæ ‡å‡†éå† âœ…")
    
    # ============================================================
    # å®¡æŸ¥3: æ ·æœ¬è®¡æ•°å‡†ç¡®æ€§
    # ============================================================
    print("\nğŸ“Š å®¡æŸ¥3: æ ·æœ¬è®¡æ•°")
    print("-" * 50)
    
    # ä»£ç : self.stats['total_samples'] += labels.size(0)
    print("  è®¡æ•°ä»£ç : self.stats['total_samples'] += labels.size(0)")
    print("  æ¯ä¸ªbatchå®é™…è®¡æ•°ï¼Œéå›ºå®šå€¼ âœ…")
    
    # é¢„æœŸæ¯epochæ ·æœ¬æ•°
    expected_samples_per_epoch = train_size
    actual_per_epoch = batch_size * log_batches  # 64 * 704 = 45056 (çº¦)
    
    print(f"  é¢„æœŸæ¯epoch: {expected_samples_per_epoch:,}")
    print(f"  å®é™…çº¦: {actual_per_epoch:,}")
    
    if abs(actual_per_epoch - expected_samples_per_epoch) < 100:
        verified.append("âœ… æ¯Epochæ ·æœ¬æ•°æ­£ç¡®")
    
    # ============================================================
    # å®¡æŸ¥4: æ—¶é—´è®¡ç®—
    # ============================================================
    print("\nğŸ“Š å®¡æŸ¥4: æ—¶é—´è®¡ç®—")
    print("-" * 50)
    
    # ä»æ—¥å¿—åˆ†æçœŸå®æ—¶é—´
    # Epoch 1: 05:18:03 å¼€å§‹, 05:22:27 ç»“æŸ = 4åˆ†24ç§’
    # Epoch 2: 05:22:27 å¼€å§‹, 05:26:44 ç»“æŸ = 4åˆ†17ç§’
    
    epoch_durations = [
        ("Epoch 1", "05:18:03", "05:22:27", 264),  # 4:24
        ("Epoch 2", "05:22:27", "05:26:44", 257),  # 4:17
        ("Epoch 3", "05:26:44", "05:31:01", 257),  # 4:17
    ]
    
    print("  ä»æ—¥å¿—åˆ†æçš„Epochè€—æ—¶:")
    total_duration = 0
    for name, start, end, dur in epoch_durations:
        print(f"    {name}: {start} â†’ {end} = {dur//60}åˆ†{dur%60}ç§’")
        total_duration += dur
    
    avg_epoch_duration = total_duration / len(epoch_durations)
    print(f"  å¹³å‡æ¯Epoch: {avg_epoch_duration:.0f}ç§’ ({avg_epoch_duration/60:.1f}åˆ†é’Ÿ)")
    
    # è®¡ç®—5å°æ—¶èƒ½å®Œæˆå¤šå°‘epoch
    target_seconds = 5 * 3600  # 18000ç§’
    expected_epochs = target_seconds / avg_epoch_duration
    print(f"  5å°æ—¶é¢„è®¡å®Œæˆ: {expected_epochs:.0f} epochs")
    
    # éªŒè¯é€Ÿåº¦
    samples_per_epoch = train_size
    samples_per_second = samples_per_epoch / avg_epoch_duration
    print(f"  å¤„ç†é€Ÿåº¦: {samples_per_second:.0f} samples/s")
    
    # æ—¥å¿—æ˜¾ç¤ºé€Ÿåº¦æ˜¯ 177-181 samples/s
    log_speed = 180
    if abs(samples_per_second - log_speed) < 20:
        verified.append("âœ… å¤„ç†é€Ÿåº¦ä¸æ—¥å¿—ä¸€è‡´")
        print(f"  æ—¥å¿—é€Ÿåº¦: {log_speed} samples/s âœ… ä¸€è‡´")
    
    # ============================================================
    # å®¡æŸ¥5: æ¢¯åº¦è®¡ç®—
    # ============================================================
    print("\nğŸ“Š å®¡æŸ¥5: æ¢¯åº¦ä¸åå‘ä¼ æ’­")
    print("-" * 50)
    
    print("  ä»£ç å®¡æŸ¥:")
    print("    loss.backward()  # çœŸå®åå‘ä¼ æ’­")
    print("    torch.nn.utils.clip_grad_norm_(...)  # æ¢¯åº¦è£å‰ª")
    print("    self.optimizer.step()  # å‚æ•°æ›´æ–°")
    
    verified.append("âœ… æ ‡å‡†æ¢¯åº¦è®¡ç®—å’Œåå‘ä¼ æ’­")
    print("  ç»“è®º: ä½¿ç”¨æ ‡å‡†PyTorchè®­ç»ƒæµç¨‹ âœ…")
    
    # ============================================================
    # å®¡æŸ¥6: æ½œåœ¨é—®é¢˜ç‚¹åˆ†æ
    # ============================================================
    print("\nğŸ“Š å®¡æŸ¥6: æ½œåœ¨é—®é¢˜ç‚¹")
    print("-" * 50)
    
    potential_issues = []
    
    # é—®é¢˜1: æ•°æ®æ˜¯åˆæˆçš„
    print("  âš ï¸ æ•°æ®è´¨é‡:")
    print("     æ•°æ®é›†æ˜¯éšæœºç”Ÿæˆçš„åˆæˆæ•°æ®")
    print("     æ•°å­¦é—®é¢˜: a Â± b Ã— c çš„ç®€å•è®¡ç®—")
    print("     çŸ¥è¯†é—®é¢˜: éšæœºtokenåºåˆ—")
    print("     å½±å“: æ¨¡å‹å­¦ä¹ çš„æ˜¯æ¨¡å¼è€ŒéçœŸå®çŸ¥è¯†")
    potential_issues.append("æ•°æ®æ˜¯åˆæˆçš„ï¼ŒéçœŸå®æ•°æ®é›†")
    
    # é—®é¢˜2: ä»»åŠ¡è¿‡äºç®€å•
    print("\n  âš ï¸ ä»»åŠ¡å¤æ‚åº¦:")
    print("     4åˆ†ç±»ä»»åŠ¡ï¼ˆæ ¹æ®ç­”æ¡ˆç‰¹å¾åˆ†ç±»ï¼‰")
    print("     å¯¹äº7.35Må‚æ•°çš„æ¨¡å‹å¯èƒ½è¿‡äºç®€å•")
    print("     å‡†ç¡®ç‡å¾ˆå¿«è¾¾åˆ°50%+å¯èƒ½å› ä¸ºä»»åŠ¡ç®€å•")
    potential_issues.append("åˆ†ç±»ä»»åŠ¡å¯èƒ½è¿‡äºç®€å•")
    
    # é—®é¢˜3: æ²¡æœ‰ä½¿ç”¨çœŸå®æ•°æ®é›†
    print("\n  âš ï¸ çœŸå®æ€§:")
    print("     æœªä½¿ç”¨å…¬å¼€benchmarkæ•°æ®é›†")
    print("     æ— æ³•ä¸å…¶ä»–ç³»ç»Ÿå¯¹æ¯”")
    potential_issues.append("æœªä½¿ç”¨æ ‡å‡†benchmark")
    
    # ============================================================
    # æœ€ç»ˆç»“è®º
    # ============================================================
    print("\n" + "=" * 70)
    print("ğŸ“‹ å®¡è®¡ç»“è®º")
    print("=" * 70)
    
    print("\nâœ… å·²éªŒè¯çš„æ­£ç¡®ç‚¹:")
    for v in verified:
        print(f"   {v}")
    
    print("\nâš ï¸ æ½œåœ¨é—®é¢˜:")
    for p in potential_issues:
        print(f"   âš ï¸ {p}")
    
    print("\n" + "-" * 70)
    print("ğŸ” æœ€ç»ˆåˆ¤æ–­:")
    print("-" * 70)
    print("""
  ä»ä»£ç å±‚é¢æ¥çœ‹ï¼Œå½“å‰è®­ç»ƒå®ç°æ˜¯"è¯šå®"çš„ï¼š
  
  âœ… ä¸å­˜åœ¨ä½œå¼Š:
     - æ¯ä¸ªEpochç¡®å®éå†å®Œæ•´æ•°æ®é›† (704 batches Ã— 64 = 45,056æ ·æœ¬)
     - çœŸå®è¿›è¡Œæ¢¯åº¦è®¡ç®—å’Œåå‘ä¼ æ’­
     - æ—¶é—´ç»Ÿè®¡å‡†ç¡®ï¼ˆæ¯Epochçº¦4åˆ†é’Ÿï¼Œ5å°æ—¶çº¦70 epochsï¼‰
     - æ ·æœ¬è®¡æ•°çœŸå®
  
  âš ï¸ ä½†å­˜åœ¨"å¼±ç‚¹"ï¼ˆéä½œå¼Šï¼Œä½†å½±å“ä»·å€¼ï¼‰:
     1. æ•°æ®æ˜¯éšæœºç”Ÿæˆçš„åˆæˆæ•°æ®ï¼Œä¸æ˜¯çœŸå®æ•°æ®é›†
     2. ä»»åŠ¡æ˜¯ç®€å•çš„4åˆ†ç±»ï¼Œå¯¹AGIä»·å€¼æœ‰é™
     3. æœªä½¿ç”¨æ ‡å‡†benchmarkæ— æ³•è¯„ä¼°çœŸå®èƒ½åŠ›
     
  ğŸ“Œ å»ºè®®æ”¹è¿›:
     1. ä½¿ç”¨çœŸå®æ•°æ®é›†ï¼ˆå¦‚WikiTextã€OpenWebTextç­‰ï¼‰
     2. ä½¿ç”¨æ›´å¤æ‚çš„ä»»åŠ¡ï¼ˆè¯­è¨€å»ºæ¨¡ã€é—®ç­”ç­‰ï¼‰
     3. æ·»åŠ æ ‡å‡†benchmarkè¯„ä¼°
    """)
    
    return {
        'verified': verified,
        'potential_issues': potential_issues,
        'is_honest': True,
        'needs_improvement': True
    }


# ============================================================
# å®é™…æ—¥å¿—æ•°æ®éªŒç®—
# ============================================================

def verify_from_logs():
    """ä»æ—¥å¿—éªŒç®—çœŸå®æ€§"""
    
    print("\n" + "=" * 70)
    print("ğŸ“Š ä»æ—¥å¿—æ•°æ®éªŒç®—")
    print("=" * 70)
    
    # æ—¥å¿—æ•°æ®ç‚¹
    log_data = [
        # (epoch, batch, loss, acc, time)
        (1, 100, 1.2089, 0.3934, "05:18:42"),
        (1, 200, 1.1498, 0.4179, "05:19:18"),
        (1, 300, 1.1069, 0.4397, "05:19:55"),
        (1, 400, 1.0935, 0.4429, "05:20:31"),
        (1, 500, 1.0815, 0.4461, "05:21:07"),
        (1, 600, 1.0743, 0.4479, "05:21:43"),
        (1, 700, 1.0664, 0.4515, "05:22:19"),
    ]
    
    print("\nEpoch 1 Batchæ—¶é—´é—´éš”åˆ†æ:")
    print("-" * 50)
    
    # åˆ†ææ¯100ä¸ªbatchçš„æ—¶é—´é—´éš”
    # 64æ ·æœ¬ Ã— 100 batch = 6400æ ·æœ¬
    batch_interval = 36  # çº¦36ç§’å¤„ç†100ä¸ªbatch
    samples_per_interval = 64 * 100
    speed = samples_per_interval / batch_interval
    
    for i in range(1, len(log_data)):
        prev = log_data[i-1]
        curr = log_data[i]
        print(f"  Batch {prev[1]} â†’ {curr[1]}: "
              f"Loss {prev[2]:.4f} â†’ {curr[2]:.4f} | "
              f"Acc {prev[3]:.2%} â†’ {curr[3]:.2%}")
    
    print(f"\n  æ¯100 batchesè€—æ—¶: ~{batch_interval}ç§’")
    print(f"  æ ·æœ¬å¤„ç†: {samples_per_interval:,} æ ·æœ¬")
    print(f"  å®é™…é€Ÿåº¦: ~{speed:.0f} samples/s")
    
    # éªŒè¯å‡†ç¡®ç‡å˜åŒ–åˆç†æ€§
    print("\nå‡†ç¡®ç‡å˜åŒ–åˆ†æ:")
    print("-" * 50)
    
    acc_progression = [
        (1, 0.4516, 0.4736),  # Epoch 1: Train 45.16% â†’ Val 47.36%
        (2, 0.4862, 0.5060),  # Epoch 2: Train 48.62% â†’ Val 50.60%
        (3, 0.5074, 0.5160),  # Epoch 3: Train 50.74% â†’ Val 51.60%
        (4, 0.5093, 0.5140),  # Epoch 4: å°å¹…æ³¢åŠ¨
        (5, 0.5086, 0.5040),  # Epoch 5: ç•¥å¾®ä¸‹é™
        (6, 0.5162, 0.5166),  # Epoch 6: å›å‡
    ]
    
    print("  Epoch | Train Acc | Val Acc | å˜åŒ–")
    print("  " + "-" * 40)
    
    for i, (epoch, train, val) in enumerate(acc_progression):
        if i > 0:
            train_change = train - acc_progression[i-1][1]
            val_change = val - acc_progression[i-1][2]
            print(f"    {epoch}   | {train:.2%}   | {val:.2%}  | "
                  f"T{'+' if train_change >= 0 else ''}{train_change:.2%} "
                  f"V{'+' if val_change >= 0 else ''}{val_change:.2%}")
        else:
            print(f"    {epoch}   | {train:.2%}   | {val:.2%}  | åŸºå‡†")
    
    print("\n  è§‚å¯Ÿ:")
    print("  - å‡†ç¡®ç‡åœ¨50%é™„è¿‘éœ‡è¡ï¼ˆ4åˆ†ç±»éšæœºåŸºå‡†25%ï¼‰")
    print("  - å­˜åœ¨æ­£å¸¸çš„æ³¢åŠ¨å’Œå¶å°”ä¸‹é™")
    print("  - ç¬¦åˆçœŸå®è®­ç»ƒçš„ç‰¹å¾")
    
    return True


if __name__ == "__main__":
    result = audit_training_code()
    verify_from_logs()
    
    print("\n" + "=" * 70)
    print("ğŸ“‹ æ€»ç»“")
    print("=" * 70)
    print("""
å½“å‰è®­ç»ƒä»£ç æ˜¯è¯šå®çš„ï¼Œæ²¡æœ‰ä½œå¼Šã€‚

ä½†è¦æˆä¸ºçœŸæ­£æœ‰ä»·å€¼çš„AGIè®­ç»ƒï¼Œå»ºè®®ï¼š
1. å¼•å…¥çœŸå®æ•°æ®é›†ï¼ˆWikiText, C4, RedPajamaç­‰ï¼‰
2. ä½¿ç”¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼ˆNext Token Predictionï¼‰
3. æ·»åŠ æ ‡å‡†è¯„ä¼°ï¼ˆMMLU, HellaSwagç­‰benchmarkï¼‰
4. æ›´å¤§çš„æ¨¡å‹å’Œæ›´é•¿çš„è®­ç»ƒ
""")
