# h2q/fractal_embedding.py

import torch
import torch.nn as nn
import torch.nn.functional as F

class FractalEmbedding(nn.Module):
    """
    H2Q åˆ†å½¢åµŒå…¥ (Fractal Embedding)
    
    åŸç†ï¼š
    ä¸ç›´æ¥å­¦ä¹  256 ç»´çš„å‘é‡ï¼Œè€Œæ˜¯ä» 2 ç»´æ ¸å¿ƒå¼€å§‹ï¼Œé€šè¿‡é€’å½’çš„â€œå¯¹ç§°æ€§ç ´ç¼ºâ€å±•å¼€ã€‚
    
    æ•°å­¦è¿‡ç¨‹ï¼š
    1. åˆå§‹æ€ (Dim=2): [å­˜åœ¨, åå­˜åœ¨]
    2. å±•å¼€ç®—å­ T(v): v -> [v + Î´, v - Î´]
       å…¶ä¸­ Î´ (Delta) æ˜¯è¯¥å±‚çº§ç”Ÿæˆçš„â€œå·®å¼‚ä¿¡æ¯â€ã€‚
       å¦‚æœ Î´=0ï¼Œåˆ™ä¿¡æ¯å®Œå…¨å¤ç”¨ï¼ˆå¯¹ç§°æ€§ä¿æŒï¼‰ï¼›
       å¦‚æœ Î´!=0ï¼Œåˆ™äº§ç”Ÿåˆ†åŒ–ï¼ˆå¯¹ç§°æ€§ç ´ç¼ºï¼‰ã€‚
    
    å±‚çº§ï¼š2 -> 4 -> 8 -> 16 -> 32 -> 64 -> 128 -> 256
    """
    def __init__(self, vocab_size=257, target_dim=256):
        super().__init__()
        self.vocab_size = vocab_size
        self.target_dim = target_dim
        
        # 1. æ ¸å¿ƒç§å­ (The Seed): 2ç»´
        # ä»£è¡¨æœ€åŸºæœ¬çš„äºŒå…ƒå¯¹ç«‹ (é˜´/é˜³, 0/1)
        self.seed_emb = nn.Embedding(vocab_size, 2)
        
        # 2. å·®å¼‚ç”Ÿæˆå™¨ (Innovations)
        # æ¯ä¸€å±‚è´Ÿè´£ç”Ÿæˆâ€œå·®å¼‚â€ï¼Œæ¨åŠ¨ç»´åº¦ç¿»å€
        self.expanders = nn.ModuleList()
        
        current_dim = 2
        while current_dim < target_dim:
            # è¿™æ˜¯ä¸€ä¸ªéçº¿æ€§å˜æ¢ï¼Œç”¨äºä»å½“å‰çŠ¶æ€ç”Ÿæˆâ€œå˜åŒ–é‡â€
            # ä½¿ç”¨ Tanh é™åˆ¶å·®å¼‚çš„å¹…åº¦ï¼Œä¿è¯æ•°å€¼ç¨³å®šæ€§
            layer = nn.Sequential(
                nn.Linear(current_dim, current_dim),
                nn.Tanh() 
            )
            self.expanders.append(layer)
            current_dim *= 2
            
        print(f"ğŸŒŒ [Fractal] åˆ†å½¢æ ‘æ„å»ºå®Œæˆ: 2 -> ... -> {target_dim} (å…± {len(self.expanders)} æ¬¡åˆ†è£‚)")

    def forward(self, x):
        # x: [Batch, Seq]
        
        # 1. ç§å­èŒå‘
        h = self.seed_emb(x) # [B, S, 2]
        
        # 2. é€’å½’å±•å¼€ (Recursive Expansion)
        for expander in self.expanders:
            # è®¡ç®—å·®å¼‚é¡¹ delta
            delta = expander(h)
            
            # åˆ†è£‚ä¸å¤ç”¨ï¼š
            # å·¦æ”¯ï¼šç»§æ‰¿ + å˜å¼‚
            # å³æ”¯ï¼šç»§æ‰¿ - å˜å¼‚
            # è¿™ç§ç»“æ„å¼ºåˆ¶æ¨¡å‹ä¿ç•™ä¸Šä¸€å±‚çº§çš„â€œä¸­å¿ƒç‰¹å¾â€
            h = torch.cat([h + delta, h - delta], dim=-1)
            
        return h