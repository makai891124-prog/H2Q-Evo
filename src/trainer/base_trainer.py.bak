import torch
import torch.nn as nn
import torch.optim as optim
import os
from abc import ABC, abstractmethod

class BaseTrainer(ABC):
    def __init__(self, model: nn.Module, optimizer: optim.Optimizer, criterion, device, log_interval, checkpoint_dir):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        self.log_interval = log_interval
        self.checkpoint_dir = checkpoint_dir

        if not os.path.exists(self.checkpoint_dir):
            os.makedirs(self.checkpoint_dir)

        self.epoch = 0

    @abstractmethod
    def train_step(self, data, target):
        pass

    @abstractmethod
    def evaluate(self, data_loader):
        pass

    def train_epoch(self, data_loader, epoch):
        self.model.train()
        total_loss = 0
        for batch_idx, (data, target) in enumerate(data_loader):
            data, target = data.to(self.device), target.to(self.device)

            self.optimizer.zero_grad()
            loss = self.train_step(data, target)
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()

            if batch_idx % self.log_interval == 0:
                print(f"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(data_loader.dataset)} ({100. * batch_idx / len(data_loader):.0f}%)]\tLoss: {loss.item():.6f}")

        avg_loss = total_loss / len(data_loader)
        print(f"\nEpoch: {epoch}, Training Loss: {avg_loss:.4f}\n")
        return avg_loss

    def train(self, train_loader, val_loader, epochs):
        best_val_loss = float('inf')
        for epoch in range(self.epoch, epochs):
            self.epoch = epoch
            train_loss = self.train_epoch(train_loader, epoch)
            val_loss = self.evaluate(val_loader)

            # Checkpointing
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                self.save_checkpoint(epoch, val_loss, filename='best_model.pth')
            self.save_checkpoint(epoch, val_loss, filename='last_model.pth')

    def save_checkpoint(self, epoch, val_loss, filename='checkpoint.pth'):
        checkpoint_path = os.path.join(self.checkpoint_dir, filename)
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_loss': val_loss,
        }, checkpoint_path)
        print(f"Checkpoint saved to {checkpoint_path}")

    def load_checkpoint(self, filename='checkpoint.pth'):
        checkpoint_path = os.path.join(self.checkpoint_dir, filename)
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.epoch = checkpoint['epoch'] + 1
            val_loss = checkpoint['val_loss']
            print(f"Loaded checkpoint from {checkpoint_path} at epoch {self.epoch} with val_loss {val_loss}")
            return val_loss
        else:
            print(f"No checkpoint found at {checkpoint_path}, starting from scratch.")
            return float('inf')
