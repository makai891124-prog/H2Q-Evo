#!/usr/bin/env python3
"""
H2Q-Evo çœŸå®ä»£ç è¡¥å…¨ç³»ç»Ÿ
åŸºäºTransformerçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°çœŸæ­£çš„ä»£ç è¡¥å…¨èƒ½åŠ›
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import json
import os
import sys
from typing import Dict, List, Any, Optional
import numpy as np
from pathlib import Path

sys.path.append('/Users/imymm/H2Q-Evo')

from h2q_project.src.h2q.tokenizer_simple import default_tokenizer


class CodeDataset(Dataset):
    """ä»£ç æ•°æ®é›†"""

    def __init__(self, code_samples: List[str], tokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = []

        for code in code_samples:
            tokens = tokenizer.encode(code, add_specials=True, max_length=max_length)
            if len(tokens) >= 10:  # åªä¿ç•™æœ‰æ„ä¹‰çš„ä»£ç ç‰‡æ®µ
                self.samples.append(tokens)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        tokens = self.samples[idx]
        # åˆ›å»ºè¾“å…¥å’Œç›®æ ‡åºåˆ—
        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)
        target_ids = torch.tensor(tokens[1:], dtype=torch.long)
        return input_ids, target_ids


class CodeTransformer(nn.Module):
    """ä»£ç ç”ŸæˆTransformeræ¨¡å‹"""

    def __init__(self, vocab_size: int, hidden_dim: int = 512, num_layers: int = 6,
                 num_heads: int = 8, dropout: float = 0.1):
        super().__init__()

        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim

        # åµŒå…¥å±‚ - å¢åŠ åµŒå…¥ç»´åº¦
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.pos_embedding = nn.Embedding(1024, hidden_dim)  # æœ€å¤§åºåˆ—é•¿åº¦

        # Transformerå±‚ - å¢åŠ å±‚æ•°å’Œå‰é¦ˆç»´åº¦
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,  # å¢åŠ å‰é¦ˆç½‘ç»œç»´åº¦
            dropout=dropout,
            batch_first=True,
            activation='gelu'  # ä½¿ç”¨GELUæ¿€æ´»å‡½æ•°
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Layer normalization
        self.layer_norm = nn.LayerNorm(hidden_dim)

        # è¾“å‡ºå±‚
        self.lm_head = nn.Linear(hidden_dim, vocab_size)

        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ– - æ”¹è¿›çš„åˆå§‹åŒ–ç­–ç•¥"""
        if isinstance(module, nn.Linear):
            # ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ï¼Œæ ‡å‡†å·®æ ¹æ®è¾“å…¥ç»´åº¦è°ƒæ•´
            std = 0.02 if module.weight.shape[0] != self.vocab_size else 0.02 / (self.hidden_dim ** 0.5)
            torch.nn.init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            # åµŒå…¥å±‚ä½¿ç”¨è¾ƒå°çš„æ ‡å‡†å·®
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.ones_(module.weight)
            torch.nn.init.zeros_(module.bias)

    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        seq_len = input_ids.size(1)

        # ä½ç½®ç¼–ç 
        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)
        pos_emb = self.pos_embedding(positions)

        # è¯åµŒå…¥
        token_emb = self.embedding(input_ids)

        # ç»„åˆåµŒå…¥
        x = token_emb + pos_emb

        # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)
        causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=input_ids.device), diagonal=1)

        # Transformerå‰å‘ä¼ æ’­
        output = self.transformer(x, mask=causal_mask, src_key_padding_mask=~attention_mask)

        # Layer normalization
        output = self.layer_norm(output)

        # è¯­è¨€æ¨¡å‹å¤´
        logits = self.lm_head(output)

        return logits


class RealCodeCompletionSystem:
    """çœŸå®ä»£ç è¡¥å…¨ç³»ç»Ÿ"""

    def __init__(self, model_path: Optional[str] = None):
        self.tokenizer = default_tokenizer
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆ›å»ºæ¨¡å‹
        self.model = CodeTransformer(
            vocab_size=self.tokenizer.vocab_size,
            hidden_dim=512,
            num_layers=6,
            num_heads=8
        ).to(self.device)

        # åŠ è½½æˆ–è®­ç»ƒæ¨¡å‹
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        else:
            self.train_basic_model()

    def train_basic_model(self):
        """è®­ç»ƒåŸºç¡€ä»£ç ç”Ÿæˆæ¨¡å‹"""
        print("ğŸ—ï¸ è®­ç»ƒåŸºç¡€ä»£ç ç”Ÿæˆæ¨¡å‹...")

        # åˆ›å»ºè®­ç»ƒæ•°æ®
        code_samples = self._create_training_samples()

        # åˆ›å»ºæ•°æ®é›†
        dataset = CodeDataset(code_samples, self.tokenizer)
        dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

        # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ - æ”¹è¿›çš„è®­ç»ƒå‚æ•°
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-3, weight_decay=0.01, betas=(0.9, 0.999))
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)
        criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_id)

        # è®­ç»ƒå¾ªç¯ - æ›´é•¿çš„è®­ç»ƒæ—¶é—´
        num_epochs = 50  # å¢åŠ è®­ç»ƒè½®æ•°
        best_loss = float('inf')
        patience = 10
        patience_counter = 0

        print(f"ğŸ“š è®­ç»ƒæ•°æ®å¤§å°: {len(dataset)}")
        print(f"ğŸƒ å¼€å§‹è®­ç»ƒ {num_epochs} è½®...")

        for epoch in range(num_epochs):
            self.model.train()
            total_loss = 0
            num_batches = 0

            for batch_idx, (input_ids, target_ids) in enumerate(dataloader):
                input_ids = input_ids.to(self.device)
                target_ids = target_ids.to(self.device)

                # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
                attention_mask = (input_ids != self.tokenizer.pad_id)

                optimizer.zero_grad()

                # å‰å‘ä¼ æ’­
                logits = self.model(input_ids, attention_mask)

                # è®¡ç®—æŸå¤± (åªè®¡ç®—épadä½ç½®)
                loss = criterion(
                    logits.view(-1, self.tokenizer.vocab_size),
                    target_ids.view(-1)
                )

                # åå‘ä¼ æ’­
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                optimizer.step()

                total_loss += loss.item()
                num_batches += 1

                if batch_idx % 10 == 0:  # å‡å°‘æ‰“å°é¢‘ç‡
                    print(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")

            avg_loss = total_loss / num_batches
            scheduler.step()

            print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}")

            # æ—©åœæœºåˆ¶
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_model("/Users/imymm/H2Q-Evo/best_code_model.pth")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print("æ—©åœ: æŸå¤±ä¸å†ä¸‹é™")
                    break

        # åŠ è½½æœ€ä½³æ¨¡å‹
        if os.path.exists("/Users/imymm/H2Q-Evo/best_code_model.pth"):
            self.load_model("/Users/imymm/H2Q-Evo/best_code_model.pth")
            print("âœ… åŠ è½½æœ€ä½³æ¨¡å‹")

        print("ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆ!")

    def _create_training_samples(self) -> List[str]:
        """åˆ›å»ºè®­ç»ƒæ ·æœ¬ - æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†"""
        samples = [
            # PythonåŸºç¡€è¯­æ³• - æ›´å®Œæ•´çš„ç¤ºä¾‹ï¼Œå»é™¤å¤šä½™ç©ºæ ¼
            "def hello_world():\n    print('Hello, World!')\n    return True",
            "class Calculator:\n    def __init__(self):\n        self.result = 0\n\n    def add(self, x, y):\n        return x + y\n\n    def subtract(self, x, y):\n        return x - y",
            "import torch\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(10, 1)\n\n    def forward(self, x):\n        return self.linear(x)",
            "for i in range(10):\n    print(f'Number: {i}')\n    if i % 2 == 0:\n        print('Even')\n    else:\n        print('Odd')",
            "def fibonacci(n):\n    if n <= 1:\n        return n\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)",
            "try:\n    result = 10 / 2\n    print(f'Result: {result}')\nexcept ZeroDivisionError:\n    print('Cannot divide by zero')\nfinally:\n    print('Done')",
            "with open('file.txt', 'r') as f:\n    content = f.read()\n    print(content)",
            "def quicksort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quicksort(left) + middle + quicksort(right)",

            # JavaScriptç¤ºä¾‹
            "function greet(name) {\n    return `Hello, ${name}!`;\n}\n\nconsole.log(greet('World'));",
            "const add = (a, b) => a + b;\n\nconst result = add(5, 3);\nconsole.log(result);",
            "class Rectangle {\n    constructor(width, height) {\n        this.width = width;\n        this.height = height;\n    }\n\n    getArea() {\n        return this.width * this.height;\n    }\n}",

            # æ›´å¤šPythonç¤ºä¾‹
            "import json\n\ndata = {'name': 'Alice', 'age': 30}\njson_str = json.dumps(data)\nprint(json_str)",
            "from collections import Counter\n\nwords = ['apple', 'banana', 'apple', 'cherry']\nword_count = Counter(words)\nprint(word_count)",
            "def is_prime(n):\n    if n <= 1:\n        return False\n    for i in range(2, int(n**0.5) + 1):\n        if n % i == 0:\n            return False\n    return True",
            "import random\n\nnumbers = [random.randint(1, 100) for _ in range(10)]\nsorted_numbers = sorted(numbers)\nprint(sorted_numbers)",
            "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1",

            # ç®€å•çš„å‡½æ•°å®šä¹‰ - æ›´ç´§å‡‘
            "def sum_list(numbers):\n    total = 0\n    for num in numbers:\n        total += num\n    return total",
            "def max_value(a, b):\n    if a > b:\n        return a\n    else:\n        return b",
            "def reverse_string(s):\n    return s[::-1]",
            "def count_vowels(text):\n    vowels = 'aeiouAEIOU'\n    count = 0\n    for char in text:\n        if char in vowels:\n            count += 1\n    return count",

            # å˜é‡èµ‹å€¼å’Œè¡¨è¾¾å¼ - æ›´ç´§å‡‘
            "x = 5\ny = 10\nresult = x + y\nprint(result)",
            "name = 'Alice'\nage = 25\nmessage = f'Hello, {name}! You are {age} years old.'\nprint(message)",
            "numbers = [1, 2, 3, 4, 5]\nsquared = [x**2 for x in numbers]\nprint(squared)",

            # æ§åˆ¶æµ - æ›´ç´§å‡‘
            "if x > 0:\n    print('Positive')\nelif x < 0:\n    print('Negative')\nelse:\n    print('Zero')",
            "while n > 0:\n    print(n)\n    n -= 1",
            "for item in items:\n    if item.startswith('test'):\n        print(f'Found: {item}')\n        break"
        ]

        # æ•°æ®å¢å¼ºï¼šåˆ›å»ºæ›´å¤šå˜ä½“ï¼Œä½†å‡å°‘ç©ºæ ¼
        augmented_samples = []
        for sample in samples:
            augmented_samples.append(sample)
            # åªæ·»åŠ æœ‰æ„ä¹‰çš„å˜ä½“
            if 'def ' in sample and '(' in sample:
                func_name = sample.split('def ')[1].split('(')[0]
                augmented_samples.append(f"{func_name}()\n")
                augmented_samples.append(f"result = {func_name}(arg)\n")

        # è¿‡æ»¤æ‰åŒ…å«è¿‡å¤šç©ºæ ¼çš„æ ·æœ¬
        filtered_samples = []
        for sample in samples + augmented_samples:
            # è®¡ç®—ç©ºæ ¼æ¯”ä¾‹
            space_ratio = sample.count(' ') / len(sample) if len(sample) > 0 else 0
            if space_ratio < 0.3:  # ç©ºæ ¼æ¯”ä¾‹å°äº30%
                filtered_samples.append(sample)

        return filtered_samples

    def generate_completion(self, prompt: str, max_length: int = 100, temperature: float = 0.8,
                           top_k: int = 50, top_p: float = 0.9) -> str:
        """ç”Ÿæˆä»£ç è¡¥å…¨"""
        print(f"ğŸ”§ ç”Ÿæˆä»£ç è¡¥å…¨: {prompt[:50]}...")

        self.model.eval()

        # ç¼–ç æç¤º
        tokens = self.tokenizer.encode(prompt, add_specials=True, max_length=200)
        input_ids = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(self.device)

        generated_tokens = tokens.copy()

        with torch.no_grad():
            for i in range(max_length):
                # è·å–å½“å‰åºåˆ—
                current_ids = torch.tensor(generated_tokens, dtype=torch.long).unsqueeze(0).to(self.device)

                # é™åˆ¶åºåˆ—é•¿åº¦
                if current_ids.size(1) > 512:
                    current_ids = current_ids[:, -512:]

                # å‰å‘ä¼ æ’­
                logits = self.model(current_ids)

                # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                next_token_logits = logits[0, -1, :]

                # åº”ç”¨æ¸©åº¦
                if temperature > 0:
                    next_token_logits = next_token_logits / temperature

                # Top-k é‡‡æ ·
                if top_k > 0:
                    top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)
                    next_token_logits = torch.full_like(next_token_logits, float('-inf'))
                    next_token_logits[top_k_indices] = top_k_logits

                # Top-p (nucleus) é‡‡æ ·
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    sorted_probs = F.softmax(sorted_logits, dim=-1)
                    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

                    # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„token
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0

                    next_token_logits[sorted_indices[sorted_indices_to_remove]] = float('-inf')

                # è®¡ç®—æ¦‚ç‡
                probs = F.softmax(next_token_logits, dim=-1)

                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                next_token = torch.multinomial(probs, 1).item()

                print(f"  ç”Ÿæˆtoken {i+1}: {next_token} (prob: {probs[next_token]:.4f})")  # è°ƒè¯•ä¿¡æ¯

                # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
                generated_tokens.append(next_token)

                # æ£€æŸ¥åœæ­¢æ¡ä»¶
                if next_token == self.tokenizer.eos_id:
                    print(f"  é‡åˆ°EOS tokenï¼Œåœæ­¢ç”Ÿæˆ")
                    break

                # é˜²æ­¢è¿‡é•¿
                if len(generated_tokens) >= 300:
                    print(f"  è¾¾åˆ°æœ€å¤§é•¿åº¦ï¼Œåœæ­¢ç”Ÿæˆ")
                    break

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = self.tokenizer.decode(generated_tokens[len(tokens):], skip_specials=True)

        print(f"  ç”Ÿæˆçš„tokens: {generated_tokens[len(tokens):][:10]}")  # è°ƒè¯•ä¿¡æ¯
        print(f"  ç”Ÿæˆçš„æ–‡æœ¬: '{generated_text[:100]}'")  # è°ƒè¯•ä¿¡æ¯

        return generated_text

    def save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'vocab_size': self.tokenizer.vocab_size,
            'hidden_dim': self.model.hidden_dim
        }, path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")

    def load_model(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        print(f"ğŸ“¥ æ¨¡å‹å·²åŠ è½½: {path}")


def test_real_code_completion():
    """æµ‹è¯•çœŸå®ä»£ç è¡¥å…¨"""
    print("ğŸ§ª æµ‹è¯•çœŸå®ä»£ç è¡¥å…¨ç³»ç»Ÿ")
    print("=" * 50)

    # åˆ›å»ºç³»ç»Ÿ
    system = RealCodeCompletionSystem()

    # æµ‹è¯•æç¤º
    test_prompts = [
        "def calculate_fibonacci(n):",
        "class NeuralNetwork(nn.Module):",
        "import torch",
        "for i in range(",
        "def binary_search(arr, target):",
        "function greet(name) {",
        "const add = (a, b) =>"
    ]

    for prompt in test_prompts:
        print(f"\nğŸ“ æç¤º: {prompt}")

        # ç”Ÿæˆè¡¥å…¨
        completion = system.generate_completion(prompt, max_length=50)
        print(f"  è¡¥å…¨:\n{completion}")

        # æ˜¾ç¤ºå®Œæ•´ä»£ç 
        full_code = prompt + completion
        print(f"  å®Œæ•´ä»£ç :\n{full_code[:200]}...")

        # ç®€å•éªŒè¯è¯­æ³•
        if 'def ' in full_code or 'class ' in full_code or 'function ' in full_code:
            print("  âœ… åŒ…å«å‡½æ•°/ç±»å®šä¹‰")
        if '(' in full_code and ')' in full_code:
            print("  âœ… åŒ…å«å‡½æ•°è°ƒç”¨è¯­æ³•")
        if '{' in full_code and '}' in full_code:
            print("  âœ… åŒ…å«ä»£ç å—è¯­æ³•")

    # ä¿å­˜æ¨¡å‹
    system.save_model("/Users/imymm/H2Q-Evo/real_code_completion_model.pth")

    print("\nâœ… çœŸå®ä»£ç è¡¥å…¨æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    test_real_code_completion()