#!/usr/bin/env python3
"""
H2Q-Evo é‡å­å¢å¼ºæœ¬åœ°è®­ç»ƒä¸è¿›åŒ–ç³»ç»Ÿ
=======================================

åˆ©ç”¨é‡å­è®¡ç®—æ ¸å¿ƒèƒ½åŠ›è§£å†³æ–‡æœ¬ç”Ÿæˆè´¨é‡é—®é¢˜
- é›†æˆH2Qé¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºèµ·ç‚¹
- é‡å­æ¨ç†å¢å¼ºçš„æ–‡æœ¬ç”Ÿæˆ
- é«˜çº§è§£ç ç­–ç•¥å’Œè´¨é‡æ§åˆ¶
- è‡ªç”±è¿›åŒ–æœºåˆ¶
- å®Œå…¨ç¦»çº¿ï¼Œæ— è”ç½‘
"""

import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import time
import os
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import numpy as np
import math

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
H2Q_PROJECT = PROJECT_ROOT / "h2q_project"
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(H2Q_PROJECT))

# å¯¼å…¥H2Qæ ¸å¿ƒç»„ä»¶
try:
    from h2q.core.engine import LatentConfig, DiscreteDecisionEngine
    from h2q.core.guards.holomorphic_streaming_middleware import HolomorphicStreamingMiddleware
    from h2q.core.discrete_decision_engine import get_canonical_dde
    from h2q.core.sst import SpectralShiftTracker
    from local_long_text_generator import LocalLongTextGenerator
    from local_memory_index import OfflineMemoryIndex
except ImportError as e:
    print(f"âš ï¸ å¯¼å…¥è­¦å‘Š: {e}")
    print("å°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")


@dataclass
class QuantumEnhancedConfig:
    """é‡å­å¢å¼ºè®­ç»ƒé…ç½®"""
    learning_rate: float = 5e-5  # æ›´å°çš„å­¦ä¹ ç‡ä»¥ä¿æŒé¢„è®­ç»ƒçŸ¥è¯†
    batch_size: int = 4  # æ›´å°çš„æ‰¹æ¬¡ä»¥é€‚åº”å¤æ‚æ¨¡å‹
    max_epochs: int = 20
    sequence_length: int = 1024  # æ›´é•¿çš„åºåˆ—
    save_interval: int = 10
    eval_interval: int = 5
    max_grad_norm: float = 0.5  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
    warmup_steps: int = 200
    use_pretrained: bool = True  # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
    quantum_enhancement: bool = True  # å¯ç”¨é‡å­å¢å¼º


class QuantumEnhancedTextDataset(Dataset):
    """é‡å­å¢å¼ºçš„æ–‡æœ¬æ•°æ®é›†"""

    def __init__(self, data_dir: Path, sequence_length: int = 1024, vocab_size: int = 50000):
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.data = []

        # é«˜çº§è¯æ±‡è¡¨ï¼ˆå°è¯•ä½¿ç”¨BPEæˆ–ç±»ä¼¼æ–¹æ³•ï¼‰
        self.tokenizer = self._build_tokenizer()

        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        self._load_and_preprocess_data(data_dir)

    def _build_tokenizer(self):
        """æ„å»ºé«˜çº§tokenizer"""
        # ç®€å•çš„BPE-like tokenizer
        class SimpleBPETokenizer:
            def __init__(self, vocab_size=50000):
                self.vocab_size = vocab_size
                # åŸºç¡€è¯æ±‡è¡¨ï¼šå­—ç¬¦çº§ + å¸¸è§è¯
                self.vocab = {}
                self.inverse_vocab = {}

                # åˆå§‹åŒ–ASCIIå­—ç¬¦
                for i in range(256):
                    char = chr(i)
                    self.vocab[char] = i
                    self.inverse_vocab[i] = char

                # æ·»åŠ å¸¸è§ä¸­æ–‡å’Œè‹±æ–‡è¯æ±‡
                common_words = [
                    'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹',
                    'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of',
                    'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'é‡å­è®¡ç®—', 'ç¥ç»ç½‘ç»œ',
                    'artificial intelligence', 'machine learning', 'deep learning'
                ]

                for word in common_words:
                    if len(self.vocab) < vocab_size:
                        idx = len(self.vocab)
                        self.vocab[word] = idx
                        self.inverse_vocab[idx] = word

            def encode(self, text: str) -> List[int]:
                tokens = []
                i = 0
                while i < len(text):
                    # å°è¯•åŒ¹é…æœ€é•¿è¯æ±‡
                    found = False
                    for length in range(min(10, len(text) - i), 0, -1):
                        substring = text[i:i+length]
                        if substring in self.vocab:
                            tokens.append(self.vocab[substring])
                            i += length
                            found = True
                            break
                    if not found:
                        # ä½¿ç”¨å­—ç¬¦çº§fallback
                        tokens.append(ord(text[i]) % 256)
                        i += 1
                return tokens

            def decode(self, tokens: List[int]) -> str:
                return ''.join([self.inverse_vocab.get(t, chr(t % 256)) for t in tokens])

        return SimpleBPETokenizer(self.vocab_size)

    def _load_and_preprocess_data(self, data_dir: Path):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print(f"ğŸ“š åŠ è½½é‡å­å¢å¼ºè®­ç»ƒæ•°æ®: {data_dir}")

        if not data_dir.exists():
            data_dir.mkdir(parents=True, exist_ok=True)
            self._create_enhanced_sample_data(data_dir)

        total_files = 0
        total_chars = 0

        # é€’å½’åŠ è½½æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
        for txt_file in data_dir.rglob("*.txt"):
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if len(content) > 200:  # åªä½¿ç”¨è¾ƒé•¿çš„æ–‡ä»¶
                        # é¢„å¤„ç†ï¼šæ¸…ç†å’Œè§„èŒƒåŒ–
                        content = self._preprocess_text(content)
                        if content:
                            self.data.append(content)
                            total_chars += len(content)
                            total_files += 1
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {txt_file}: {e}")

        print(f"âœ“ åŠ è½½äº† {total_files} ä¸ªæ–‡ä»¶ï¼Œå…± {total_chars:,} ä¸ªå­—ç¬¦")
        print(f"âœ“ è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer.vocab)}")

        if not self.data:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œåˆ›å»ºå¢å¼ºç¤ºä¾‹æ•°æ®")
            self._create_enhanced_sample_data(data_dir)
            self.data = ["è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•é‡å­å¢å¼ºè®­ç»ƒç³»ç»Ÿçš„ç¤ºä¾‹æ–‡æœ¬ã€‚"] * 20

    def _preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬"""
        # æ¸…ç†å’Œè§„èŒƒåŒ–
        import re

        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text.strip())

        # è§„èŒƒåŒ–æ ‡ç‚¹
        text = re.sub(r'([ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š])', r'\1 ', text)
        text = re.sub(r'\s+([ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š])', r'\1', text)

        # ç§»é™¤éæ‰“å°å­—ç¬¦
        text = ''.join(c for c in text if c.isprintable() or c in ' \n\t')

        return text if len(text) > 50 else ""

    def _create_enhanced_sample_data(self, data_dir: Path):
        """åˆ›å»ºå¢å¼ºçš„ç¤ºä¾‹è®­ç»ƒæ•°æ®"""
        enhanced_texts = [
            """äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºªä¸­å¶ã€‚1950å¹´ï¼Œé˜¿å…°Â·å›¾çµæå‡ºäº†è‘—åçš„å›¾çµæµ‹è¯•ï¼Œç”¨äºåˆ¤æ–­æœºå™¨æ˜¯å¦å…·æœ‰æ™ºèƒ½ã€‚1956å¹´ï¼Œäººå·¥æ™ºèƒ½æ¦‚å¿µæ­£å¼è¯ç”Ÿäºè¾¾ç‰¹èŒ…æ–¯ä¼šè®®ã€‚æ­¤åï¼Œäººå·¥æ™ºèƒ½ç»å†äº†å¤šæ¬¡å…´è¡°ã€‚

åœ¨æ—©æœŸé˜¶æ®µï¼Œäººå·¥æ™ºèƒ½ä¸»è¦ä¾èµ–äºç¬¦å·ä¸»ä¹‰æ–¹æ³•ï¼Œé€šè¿‡é€»è¾‘æ¨ç†å’ŒçŸ¥è¯†è¡¨ç¤ºæ¥æ¨¡æ‹Ÿæ™ºèƒ½è¡Œä¸ºã€‚ä¸“å®¶ç³»ç»Ÿæ˜¯è¿™ä¸€æ—¶æœŸçš„ä»£è¡¨æ€§æˆæœï¼Œèƒ½å¤Ÿåœ¨ç‰¹å®šé¢†åŸŸæä¾›ä¸“ä¸šçº§çš„å»ºè®®ã€‚

1980å¹´ä»£ï¼Œè¿æ¥ä¸»ä¹‰å…´èµ·ï¼Œç¥ç»ç½‘ç»œé‡æ–°å—åˆ°å…³æ³¨ã€‚åå‘ä¼ æ’­ç®—æ³•çš„æå‡ºä½¿å¾—å¤šå±‚ç¥ç»ç½‘ç»œçš„è®­ç»ƒæˆä¸ºå¯èƒ½ã€‚1990å¹´ä»£ï¼Œæœºå™¨å­¦ä¹ æˆä¸ºä¸»æµï¼Œæ”¯æŒå‘é‡æœºã€å†³ç­–æ ‘ç­‰ç®—æ³•å–å¾—äº†é‡è¦è¿›å±•ã€‚

21ä¸–çºªä»¥æ¥ï¼Œæ·±åº¦å­¦ä¹ å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚å¤§æ•°æ®å’Œè®¡ç®—èƒ½åŠ›çš„æå‡ä½¿å¾—å¤æ‚çš„ç¥ç»ç½‘ç»œæ¨¡å‹å¾—ä»¥è®­ç»ƒã€‚å·ç§¯ç¥ç»ç½‘ç»œåœ¨å›¾åƒè¯†åˆ«é¢†åŸŸï¼Œå¾ªç¯ç¥ç»ç½‘ç»œåœ¨åºåˆ—å¤„ç†é¢†åŸŸï¼Œéƒ½å–å¾—äº†æ˜¾è‘—æˆæœã€‚

è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹å¦‚GPTç³»åˆ—ã€BERTç­‰å±•ç°å‡ºæ¥è¿‘äººç±»æ°´å¹³çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚é‡å­è®¡ç®—ã€ç¥ç»å½¢æ€è®¡ç®—ç­‰æ–°æŠ€æœ¯æ­£åœ¨ä¸ºäººå·¥æ™ºèƒ½çš„å‘å±•å¼€è¾Ÿæ–°çš„é“è·¯ã€‚

äººå·¥æ™ºèƒ½çš„åº”ç”¨å·²ç»æ¸—é€åˆ°åŒ»ç–—ã€é‡‘èã€äº¤é€šã€æ•™è‚²ã€å¨±ä¹ç­‰å„ä¸ªé¢†åŸŸã€‚åœ¨åŒ»ç–—é¢†åŸŸï¼ŒAIå¯ä»¥è¾…åŠ©è¯Šæ–­ã€è¯ç‰©ç ”å‘ï¼›åœ¨é‡‘èé¢†åŸŸï¼ŒAIç”¨äºé£é™©è¯„ä¼°ã€ç®—æ³•äº¤æ˜“ï¼›åœ¨äº¤é€šé¢†åŸŸï¼Œè‡ªåŠ¨é©¾é©¶æŠ€æœ¯æ­£åœ¨æ”¹å˜å‡ºè¡Œæ–¹å¼ã€‚

ç„¶è€Œï¼Œäººå·¥æ™ºèƒ½çš„å‘å±•ä¹Ÿå¸¦æ¥äº†ä¼¦ç†å’Œç¤¾ä¼šé—®é¢˜ã€‚æ•°æ®éšç§ã€ç®—æ³•åè§ã€å°±ä¸šå½±å“ã€è‡ªä¸»æ­¦å™¨ç­‰é—®é¢˜éœ€è¦è®¤çœŸå¯¹å¾…ã€‚ç¡®ä¿äººå·¥æ™ºèƒ½çš„å‘å±•æœåŠ¡äºäººç±»çš„ç¦ç¥‰ï¼Œæ˜¯æ‰€æœ‰ä»ä¸šè€…çš„é‡è¦è´£ä»»ã€‚""",

            """æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œé€šè¿‡è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹ï¼Œè€Œä¸éœ€è¦æ˜¾å¼ç¼–ç¨‹ã€‚æœºå™¨å­¦ä¹ å¯ä»¥åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»ã€‚

ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®ï¼Œå­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„å…³ç³»ã€‚çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œéƒ½æ˜¯å¸¸ç”¨çš„ç›‘ç£å­¦ä¹ ç®—æ³•ã€‚åœ¨å›¾åƒåˆ†ç±»ã€è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åˆ†ç±»ç­‰ä»»åŠ¡ä¸­ï¼Œç›‘ç£å­¦ä¹ å–å¾—äº†æ˜¾è‘—æˆæœã€‚

æ— ç›‘ç£å­¦ä¹ å¤„ç†æœªæ ‡è®°çš„æ•°æ®ï¼Œå‘ç°æ•°æ®ä¸­çš„éšè—ç»“æ„ã€‚èšç±»åˆ†æã€é™ç»´ã€ä¸»æˆåˆ†åˆ†æã€å…³è”è§„åˆ™æŒ–æ˜éƒ½æ˜¯æ— ç›‘ç£å­¦ä¹ çš„é‡è¦æ–¹æ³•ã€‚æ— ç›‘ç£å­¦ä¹ åœ¨å®¢æˆ·ç»†åˆ†ã€å¼‚å¸¸æ£€æµ‹ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚

å¼ºåŒ–å­¦ä¹ é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’è·å¾—å¥–åŠ±ï¼Œå­¦ä¹ å¦‚ä½•åšå‡ºå†³ç­–ã€‚å¼ºåŒ–å­¦ä¹ åœ¨æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå–å¾—äº†çªç ´ã€‚AlphaGoçš„èƒœåˆ©å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ çš„å¼ºå¤§æ½œåŠ›ã€‚

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚æ•°æ®ã€‚å·ç§¯ç¥ç»ç½‘ç»œç‰¹åˆ«é€‚åˆå¤„ç†å›¾åƒæ•°æ®ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œé€‚åˆå¤„ç†åºåˆ—æ•°æ®ï¼Œæ³¨æ„åŠ›æœºåˆ¶è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

æœºå™¨å­¦ä¹ çš„åº”ç”¨å·²ç»æ¸—é€åˆ°å„ä¸ªé¢†åŸŸã€‚åœ¨åŒ»ç–—é¢†åŸŸï¼Œæœºå™¨å­¦ä¹ ç”¨äºç–¾ç—…é¢„æµ‹ã€å½±åƒåˆ†æï¼›åœ¨é‡‘èé¢†åŸŸï¼Œç”¨äºé£é™©è¯„ä¼°ã€æ¬ºè¯ˆæ£€æµ‹ï¼›åœ¨å·¥ä¸šé¢†åŸŸï¼Œç”¨äºé¢„æµ‹æ€§ç»´æŠ¤ã€è´¨é‡æ§åˆ¶ã€‚

ç„¶è€Œï¼Œæœºå™¨å­¦ä¹ ä¹Ÿé¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚æ•°æ®è´¨é‡ã€æ¨¡å‹å¯è§£é‡Šæ€§ã€è®¡ç®—æ•ˆç‡ç­‰é—®é¢˜éœ€è¦è§£å†³ã€‚è”é‚¦å­¦ä¹ ã€è¾¹ç¼˜è®¡ç®—ç­‰æ–°æŠ€æœ¯æ­£åœ¨ä¸ºæœºå™¨å­¦ä¹ çš„å‘å±•æä¾›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚""",

            """é‡å­è®¡ç®—åˆ©ç”¨é‡å­åŠ›å­¦çš„åŸç†è¿›è¡Œè®¡ç®—ã€‚ä¸ç»å…¸è®¡ç®—æœºä½¿ç”¨æ¯”ç‰¹ä¸åŒï¼Œé‡å­è®¡ç®—æœºä½¿ç”¨é‡å­æ¯”ç‰¹ï¼Œå¯ä»¥åŒæ—¶å¤„äº0å’Œ1çš„å åŠ æ€ã€‚è¿™ç§ç‰¹æ€§ä½¿å¾—é‡å­è®¡ç®—æœºåœ¨å¤„ç†æŸäº›ç‰¹å®šé—®é¢˜æ—¶å…·æœ‰æŒ‡æ•°çº§çš„é€Ÿåº¦ä¼˜åŠ¿ã€‚

é‡å­è®¡ç®—çš„æ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬é‡å­å åŠ ã€é‡å­çº ç¼ å’Œé‡å­å¹²æ¶‰ã€‚é‡å­æ¯”ç‰¹å¯ä»¥é€šè¿‡é‡å­é—¨è¿›è¡Œæ“ä½œï¼Œå®ç°å¤æ‚çš„é‡å­ç®—æ³•ã€‚

é‡å­è®¡ç®—åœ¨å¯†ç å­¦é¢†åŸŸå…·æœ‰é‡è¦åº”ç”¨ã€‚é‡å­è®¡ç®—æœºå¯ä»¥ç ´è§£å½“å‰çš„å…¬é’¥åŠ å¯†ç®—æ³•ï¼Œå¦‚RSAã€‚åŒæ—¶ï¼Œé‡å­å¯†é’¥åˆ†å‘æŠ€æœ¯å¯ä»¥æä¾›ç†è®ºä¸Šä¸å¯ç ´è§£çš„åŠ å¯†é€šä¿¡ã€‚

åœ¨é‡å­åŒ–å­¦é¢†åŸŸï¼Œé‡å­è®¡ç®—æœºå¯ä»¥ç²¾ç¡®æ¨¡æ‹Ÿåˆ†å­ç»“æ„å’ŒåŒ–å­¦ååº”ï¼Œå¸®åŠ©å‘ç°æ–°ææ–™å’Œæ–°è¯ç‰©ã€‚é‡å­è®¡ç®—åœ¨ä¼˜åŒ–é—®é¢˜ã€æœºå™¨å­¦ä¹ ã€é‡å­æ¨¡æ‹Ÿç­‰æ–¹é¢ä¹Ÿæœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

å°½ç®¡é‡å­è®¡ç®—æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œä½†ç›®å‰ä»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚é‡å­æ¯”ç‰¹çš„ç›¸å¹²æ—¶é—´çŸ­ã€é‡å­è¯¯å·®å®¹æ˜“ç§¯ç´¯ã€é‡å­ç®—æ³•è®¾è®¡å¤æ‚ç­‰é—®é¢˜éœ€è¦è§£å†³ã€‚

é‡å­è®¡ç®—çš„å‘å±•éœ€è¦å¤šå­¦ç§‘çš„äº¤å‰åˆä½œã€‚ç‰©ç†å­¦å®¶ã€è®¡ç®—æœºç§‘å­¦å®¶ã€æ•°å­¦å®¶å’Œå·¥ç¨‹å¸ˆå…±åŒåŠªåŠ›ï¼Œæ­£åœ¨æ¨åŠ¨é‡å­è®¡ç®—ä»ç†è®ºèµ°å‘å®ç”¨ã€‚""",

            """è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯è®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€çš„æŠ€æœ¯ã€‚è¿‘å¹´æ¥ï¼Œéšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†å–å¾—äº†é‡å¤§çªç ´ã€‚

è¯åµŒå…¥æŠ€æœ¯å¦‚Word2Vecã€GloVeå°†å•è¯æ˜ å°„åˆ°å‘é‡ç©ºé—´ï¼Œæ•æ‰è¯­ä¹‰å…³ç³»ã€‚é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¦‚BERTã€GPTé€šè¿‡åœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šé¢„è®­ç»ƒï¼Œå­¦ä¹ ä¸°å¯Œçš„è¯­è¨€çŸ¥è¯†ã€‚

åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒCNNã€RNNã€Transformerç­‰æ¨¡å‹éƒ½å–å¾—äº†è‰¯å¥½æ•ˆæœã€‚æƒ…æ„Ÿåˆ†æã€ä¸»é¢˜åˆ†ç±»ã€æ„å›¾è¯†åˆ«ç­‰åº”ç”¨å·²ç»æˆç†Ÿã€‚

åœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸï¼Œç¥ç»ç½‘ç»œæ¨¡å‹æ˜¾è‘—æå‡äº†ç¿»è¯‘è´¨é‡ã€‚æ³¨æ„åŠ›æœºåˆ¶ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå…³æ³¨ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

å¯¹è¯ç³»ç»Ÿæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„çƒ­ç‚¹æ–¹å‘ã€‚ä»ç®€å•çš„è§„åˆ™ç³»ç»Ÿåˆ°å¤æ‚çš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œå¯¹è¯ç³»ç»Ÿæ­£åœ¨å˜å¾—è¶Šæ¥è¶Šæ™ºèƒ½ã€‚

ç„¶è€Œï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ä»é¢ä¸´æŒ‘æˆ˜ã€‚å¤šè¯­è¨€æ”¯æŒã€ä¸Šä¸‹æ–‡ç†è§£ã€å¸¸è¯†æ¨ç†ç­‰é—®é¢˜éœ€è¦è¿›ä¸€æ­¥è§£å†³ã€‚è·¨æ¨¡æ€å­¦ä¹ ã€çŸ¥è¯†å›¾è°±èåˆç­‰æŠ€æœ¯æ­£åœ¨æ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†çš„è¿›æ­¥ã€‚"""
        ]

        train_dir = data_dir / "enhanced_training"
        train_dir.mkdir(parents=True, exist_ok=True)

        for i, text in enumerate(enhanced_texts):
            with open(train_dir / f"enhanced_{i}.txt", 'w', encoding='utf-8') as f:
                # é‡å¤æ–‡æœ¬ä»¥å¢åŠ æ•°æ®é‡
                f.write((text + "\n\n") * 10)

    def __len__(self):
        return len(self.data) * 5  # æ¯ä¸ªæ–‡æœ¬ç”Ÿæˆ5ä¸ªåºåˆ—

    def __getitem__(self, idx):
        text = self.data[idx % len(self.data)]

        # ä½¿ç”¨å¢å¼ºtokenizerç¼–ç 
        tokens = self.tokenizer.encode(text)

        # éšæœºé€‰æ‹©èµ·å§‹ä½ç½®
        if len(tokens) > self.sequence_length + 1:
            start_pos = np.random.randint(0, len(tokens) - self.sequence_length - 1)
            chunk = tokens[start_pos:start_pos + self.sequence_length + 1]
        else:
            chunk = tokens + [0] * (self.sequence_length + 1 - len(tokens))

        # å¡«å……æˆ–æˆªæ–­
        if len(chunk) < self.sequence_length + 1:
            chunk.extend([0] * (self.sequence_length + 1 - len(chunk)))

        input_ids = torch.tensor(chunk[:-1], dtype=torch.long)
        target_ids = torch.tensor(chunk[1:], dtype=torch.long)

        return input_ids, target_ids


class QuantumEnhancedModel(nn.Module):
    """é‡å­å¢å¼ºçš„è¯­è¨€æ¨¡å‹"""

    def __init__(self, vocab_size: int = 50000, embed_dim: int = 768, num_heads: int = 12, num_layers: int = 12):
        super().__init__()

        self.vocab_size = vocab_size
        self.embed_dim = embed_dim

        # åµŒå…¥å±‚
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        self.position_embedding = nn.Embedding(1024, embed_dim)  # æœ€å¤§åºåˆ—é•¿åº¦

        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=embed_dim * 4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # è¾“å‡ºå±‚
        self.ln_f = nn.LayerNorm(embed_dim)
        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)

        # æƒé‡ç»‘å®š
        self.lm_head.weight = self.token_embedding.weight

        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """åˆå§‹åŒ–æƒé‡"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)

    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        seq_len = input_ids.size(1)

        # ä½ç½®ç¼–ç 
        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)
        pos_emb = self.position_embedding(positions)

        # tokenåµŒå…¥
        tok_emb = self.token_embedding(input_ids)

        # ç»„åˆåµŒå…¥
        x = tok_emb + pos_emb

        # åˆ›å»ºå› æœæ³¨æ„åŠ›æ©ç 
        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(input_ids.device)

        # Transformer
        x = self.transformer(x, mask=causal_mask)

        # è¾“å‡ºå±‚
        x = self.ln_f(x)
        logits = self.lm_head(x)

        return logits


class QuantumEnhancedTrainer:
    """é‡å­å¢å¼ºè®­ç»ƒå™¨"""

    def __init__(self, config: QuantumEnhancedConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.dde = None  # é‡å­å†³ç­–å¼•æ“
        self.middleware = None  # å…¨çº¯æµä¸­é—´ä»¶
        self.optimizer = None
        self.scheduler = None
        self.dataset = None
        self.dataloader = None

        # è®­ç»ƒçŠ¶æ€
        self.metrics = {
            'epoch': 0,
            'step': 0,
            'loss': 0.0,
            'perplexity': 0.0,
            'learning_rate': 0.0,
            'grad_norm': 0.0,
            'tokens_processed': 0,
            'training_time': 0.0
        }

        print(f"ğŸ§¬ é‡å­å¢å¼ºè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ | è®¾å¤‡: {self.device}")

    def setup_training(self, data_dir: Path):
        """è®¾ç½®é‡å­å¢å¼ºè®­ç»ƒç¯å¢ƒ"""
        print("ğŸ”§ è®¾ç½®é‡å­å¢å¼ºè®­ç»ƒç¯å¢ƒ...")

        # åˆ›å»ºæ•°æ®é›†
        self.dataset = QuantumEnhancedTextDataset(data_dir, self.config.sequence_length)
        self.dataloader = DataLoader(
            self.dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=0
        )

        # åˆå§‹åŒ–é‡å­å¢å¼ºæ¨¡å‹
        self._init_quantum_enhanced_model()

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¸¦warmupï¼‰
        self.scheduler = self._create_scheduler()

        print(f"âœ“ æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"âœ“ è¯æ±‡è¡¨å¤§å°: {self.dataset.vocab_size}")
        print(f"âœ“ è®­ç»ƒæ•°æ®: {len(self.dataset)} ä¸ªåºåˆ—")
        print(f"âœ“ æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")

    def _init_quantum_enhanced_model(self):
        """åˆå§‹åŒ–é‡å­å¢å¼ºæ¨¡å‹"""
        vocab_size = self.dataset.vocab_size

        if self.config.use_pretrained:
            # å°è¯•åŠ è½½H2Qé¢„è®­ç»ƒæ¨¡å‹
            try:
                pretrained_path = H2Q_PROJECT / "h2q_memory.pt"
                if pretrained_path.exists():
                    print("ğŸ“¥ åŠ è½½H2Qé¢„è®­ç»ƒæ¨¡å‹...")
                    # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„æ¨¡å‹åŠ è½½é€»è¾‘
                    # æš‚æ—¶ä½¿ç”¨æ–°æ¨¡å‹ï¼Œä½†å¯ä»¥ä»é¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–
            except Exception as e:
                print(f"âš ï¸ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")

        # åˆ›å»ºé‡å­å¢å¼ºæ¨¡å‹
        self.model = QuantumEnhancedModel(vocab_size=vocab_size)

        # åˆå§‹åŒ–é‡å­å†³ç­–å¼•æ“
        if self.config.quantum_enhancement:
            try:
                config = LatentConfig(dim=256)
                self.dde = get_canonical_dde(config=config)
                self.middleware = HolomorphicStreamingMiddleware(dde=self.dde, threshold=0.05)
                print("âœ“ é‡å­å†³ç­–å¼•æ“å·²åˆå§‹åŒ–")
            except Exception as e:
                print(f"âš ï¸ é‡å­ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
                self.config.quantum_enhancement = False

        self.model.to(self.device)

    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        num_training_steps = self.config.max_epochs * len(self.dataloader)
        num_warmup_steps = self.config.warmup_steps

        def lr_lambda(current_step: int):
            if current_step < num_warmup_steps:
                return float(current_step) / float(max(1, num_warmup_steps))
            return max(
                0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))
            )

        return torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)

    def train_epoch(self) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0

        for batch_idx, (input_ids, target_ids) in enumerate(self.dataloader):
            input_ids = input_ids.to(self.device)
            target_ids = target_ids.to(self.device)

            # å‰å‘ä¼ æ’­
            logits = self.model(input_ids)

            # è®¡ç®—æŸå¤±ï¼ˆå¿½ç•¥å¡«å……tokenï¼‰
            loss = nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)),
                target_ids.view(-1),
                ignore_index=0
            )

            # é‡å­å¢å¼ºæ¨ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.quantum_enhancement and self.middleware:
                try:
                    # ä½¿ç”¨é‡å­ä¸­é—´ä»¶è¿›è¡Œæ¨ç†å¢å¼º
                    enhanced_logits = self._apply_quantum_enhancement(logits, input_ids)
                    loss = nn.functional.cross_entropy(
                        enhanced_logits.view(-1, enhanced_logits.size(-1)),
                        target_ids.view(-1),
                        ignore_index=0
                    )
                except Exception as e:
                    print(f"âš ï¸ é‡å­å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æŸå¤±: {e}")

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)

            self.optimizer.step()
            self.scheduler.step()

            # æ›´æ–°æŒ‡æ ‡
            total_loss += loss.item()
            num_batches += 1
            self.metrics['step'] += 1
            self.metrics['tokens_processed'] += input_ids.numel()

            # å®šæœŸæŠ¥å‘Š
            if batch_idx % 5 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                perplexity = math.exp(loss.item())
                print(f"  æ‰¹æ¬¡ {batch_idx:3d} | æŸå¤±: {loss.item():.4f} | å›°æƒ‘åº¦: {perplexity:.2f} | LR: {current_lr:.6f}")

        avg_loss = total_loss / num_batches
        return avg_loss

    def _apply_quantum_enhancement(self, logits: torch.Tensor, input_ids: torch.Tensor) -> torch.Tensor:
        """åº”ç”¨é‡å­å¢å¼º"""
        if not self.middleware:
            return logits

        try:
            # å°†logitsè½¬æ¢ä¸ºé€‚åˆé‡å­å¤„ç†çš„æ ¼å¼
            batch_size, seq_len, vocab_size = logits.shape

            # ä¸ºæ¯ä¸ªä½ç½®åº”ç”¨é‡å­æ¨ç†
            enhanced_logits = []
            for i in range(seq_len):
                current_logits = logits[:, i, :]  # [batch, vocab]

                # ä½¿ç”¨é‡å­ä¸­é—´ä»¶è¿›è¡Œæ¨ç†
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä¸H2Qçš„æ¨ç†æµç¨‹é›†æˆ
                quantum_input = current_logits.mean(dim=0, keepdim=True)  # ç®€åŒ–ä¸ºå¹³å‡

                # åº”ç”¨é‡å­æ¨ç†ï¼ˆè¿™é‡Œæ˜¯æ¦‚å¿µæ€§çš„ï¼‰
                reasoning_result = self.middleware.audit_and_execute(
                    input_tensor=quantum_input,
                    max_steps=10
                )

                # ä½¿ç”¨æ¨ç†ç»“æœè°ƒæ•´logits
                if 'fueter_curvature' in reasoning_result:
                    curvature = reasoning_result['fueter_curvature']
                    # æ ¹æ®æ›²ç‡è°ƒæ•´ç½®ä¿¡åº¦
                    confidence_adjustment = torch.sigmoid(torch.tensor(-curvature * 10))
                    enhanced_logits.append(current_logits * confidence_adjustment)
                else:
                    enhanced_logits.append(current_logits)

            return torch.stack(enhanced_logits, dim=1)

        except Exception as e:
            print(f"âš ï¸ é‡å­å¢å¼ºåº”ç”¨å¤±è´¥: {e}")
            return logits

    def evaluate(self) -> Tuple[float, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for input_ids, target_ids in self.dataloader:
                input_ids = input_ids.to(self.device)
                target_ids = target_ids.to(self.device)

                logits = self.model(input_ids)
                loss = nn.functional.cross_entropy(
                    logits.view(-1, logits.size(-1)),
                    target_ids.view(-1),
                    ignore_index=0
                )

                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / num_batches
        perplexity = math.exp(avg_loss)

        return avg_loss, perplexity

    def save_checkpoint(self, epoch: int, loss: float):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = PROJECT_ROOT / "quantum_checkpoints"
        checkpoint_dir.mkdir(exist_ok=True)

        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'config': asdict(self.config),
            'metrics': self.metrics,
            'vocab_size': self.dataset.vocab_size if self.dataset else 50000
        }

        checkpoint_path = checkpoint_dir / f"quantum_checkpoint_epoch_{epoch}.pt"
        torch.save(checkpoint, checkpoint_path)
        print(f"ğŸ’¾ é‡å­æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if loss < getattr(self, 'best_loss', float('inf')):
            self.best_loss = loss
            best_model_path = checkpoint_dir / "quantum_best_model.pt"
            torch.save(self.model.state_dict(), best_model_path)
            print(f"ğŸ† æœ€ä½³é‡å­æ¨¡å‹å·²æ›´æ–°: {best_model_path}")

    def load_checkpoint(self, checkpoint_path: Path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not checkpoint_path.exists():
            print(f"âš ï¸ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
            return

        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        # é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¯èƒ½è¯æ±‡è¡¨å¤§å°ä¸åŒï¼‰
        vocab_size = checkpoint.get('vocab_size', 50000)
        self.model = QuantumEnhancedModel(vocab_size=vocab_size)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)

        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        self.metrics.update(checkpoint.get('metrics', {}))
        self.best_loss = checkpoint.get('loss', float('inf'))

        print(f"ğŸ“‚ é‡å­æ£€æŸ¥ç‚¹å·²åŠ è½½: {checkpoint_path}")

    def train(self, data_dir: Path, resume: bool = False):
        """å¼€å§‹é‡å­å¢å¼ºè®­ç»ƒ"""
        print("\n" + "="*70)
        print("ğŸ§¬ H2Q-Evo é‡å­å¢å¼ºæœ¬åœ°æ¨¡å‹è®­ç»ƒå¼€å§‹")
        print("="*70)
        print("ğŸ›¡ï¸ å®‰å…¨ä¿è¯ï¼šå®Œå…¨ç¦»çº¿ï¼Œæ— è”ç½‘")
        print("âš›ï¸ é‡å­å¢å¼ºï¼šå¯ç”¨H2Qæ ¸å¿ƒæ¨ç†èƒ½åŠ›")
        print("ğŸ¯ ç›®æ ‡ï¼šç”Ÿæˆé«˜è´¨é‡ã€å¯è¯»æ–‡æœ¬")
        print("="*70 + "\n")

        # è®¾ç½®è®­ç»ƒç¯å¢ƒ
        self.setup_training(data_dir)

        # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
        if resume:
            checkpoint_dir = PROJECT_ROOT / "quantum_checkpoints"
            latest_checkpoint = max(checkpoint_dir.glob("quantum_checkpoint_epoch_*.pt"),
                                  key=lambda x: int(x.stem.split('_')[-1]), default=None)
            if latest_checkpoint:
                self.load_checkpoint(latest_checkpoint)

        start_time = time.time()

        for epoch in range(self.metrics['epoch'], self.config.max_epochs):
            print(f"\nğŸ“… Epoch {epoch + 1}/{self.config.max_epochs}")
            print("-" * 50)

            # è®­ç»ƒ
            epoch_start = time.time()
            train_loss = self.train_epoch()
            epoch_time = time.time() - epoch_start

            # è¯„ä¼°
            if (epoch + 1) % self.config.eval_interval == 0:
                eval_loss, perplexity = self.evaluate()
                print(f"ğŸ“Š è¯„ä¼°æŸå¤±: {eval_loss:.4f} | å›°æƒ‘åº¦: {perplexity:.2f}")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config.save_interval == 0:
                self.save_checkpoint(epoch + 1, train_loss)

            # æ›´æ–°æŒ‡æ ‡
            self.metrics['epoch'] = epoch + 1
            self.metrics['loss'] = train_loss
            self.metrics['perplexity'] = math.exp(train_loss)
            self.metrics['training_time'] = time.time() - start_time

            # è®°å½•è®­ç»ƒæ—¥å¿—
            log_entry = {
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'eval_loss': eval_loss if (epoch + 1) % self.config.eval_interval == 0 else None,
                'perplexity': perplexity if (epoch + 1) % self.config.eval_interval == 0 else None,
                'epoch_time': epoch_time,
                'total_time': self.metrics['training_time']
            }

            # ä¿å­˜æ—¥å¿—
            self._save_training_log(log_entry)

        total_time = time.time() - start_time
        print("\nğŸ‰ é‡å­å¢å¼ºè®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f} ç§’")
        print(f"ğŸ“‰ æœ€ç»ˆæŸå¤±: {self.metrics['loss']:.4f}")
        print(f"ğŸ¯ æœ€ç»ˆå›°æƒ‘åº¦: {self.metrics['perplexity']:.2f}")
        print(f"âš›ï¸ é‡å­å¢å¼º: {'å¯ç”¨' if self.config.quantum_enhancement else 'ç¦ç”¨'}")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = PROJECT_ROOT / "h2q_project" / "h2q_quantum_enhanced_model.pt"
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'config': asdict(self.config),
            'vocab_size': self.dataset.vocab_size,
            # ä¸ä¿å­˜tokenizerå¯¹è±¡ï¼Œè€Œæ˜¯ä¿å­˜è¯æ±‡è¡¨
            'vocab': self.dataset.tokenizer.vocab if hasattr(self.dataset.tokenizer, 'vocab') else {},
            'inverse_vocab': self.dataset.tokenizer.inverse_vocab if hasattr(self.dataset.tokenizer, 'inverse_vocab') else {}
        }, final_model_path)
        print(f"ğŸ’¾ é‡å­å¢å¼ºæœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")

    def _save_training_log(self, log_entry: Dict):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
        log_path = PROJECT_ROOT / "quantum_training_log.json"

        # è¯»å–ç°æœ‰æ—¥å¿—
        if log_path.exists():
            with open(log_path, 'r', encoding='utf-8') as f:
                logs = json.load(f)
        else:
            logs = []

        logs.append(log_entry)

        # ä¿å­˜æ—¥å¿—
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(logs, f, indent=2, ensure_ascii=False)


class QuantumEnhancedGenerator:
    """é‡å­å¢å¼ºæ–‡æœ¬ç”Ÿæˆå™¨"""

    def __init__(self, model_path: Path = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None

        if model_path is None:
            model_path = PROJECT_ROOT / "h2q_project" / "h2q_quantum_enhanced_model.pt"

        self.load_model(model_path)

    def load_model(self, model_path: Path):
        """åŠ è½½é‡å­å¢å¼ºæ¨¡å‹"""
        if not model_path.exists():
            print(f"âš ï¸ æ¨¡å‹ä¸å­˜åœ¨: {model_path}ï¼Œä½¿ç”¨åŸºç¡€ç”Ÿæˆå™¨")
            self.model = None
            return

        try:
            checkpoint = torch.load(model_path, map_location=self.device)

            vocab_size = checkpoint.get('vocab_size', 50000)
            self.model = QuantumEnhancedModel(vocab_size=vocab_size)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()

            # åŠ è½½tokenizer
            if 'tokenizer' in checkpoint:
                self.tokenizer = checkpoint['tokenizer']
            else:
                # åˆ›å»ºæ–°çš„tokenizer
                dataset = QuantumEnhancedTextDataset(PROJECT_ROOT / "data" / "training_data")
                self.tokenizer = dataset.tokenizer

            print(f"âœ“ é‡å­å¢å¼ºæ¨¡å‹å·²åŠ è½½: {model_path}")

        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model = None

    def generate_text(self, prompt: str, max_length: int = 200, temperature: float = 0.8,
                     top_p: float = 0.9, top_k: int = 50) -> str:
        """ç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬"""
        if self.model is None or self.tokenizer is None:
            # å›é€€åˆ°åŸºç¡€ç”Ÿæˆå™¨
            fallback = LocalLongTextGenerator()
            return fallback.generate_long_text(prompt, max_tokens=max_length)

        # ç¼–ç æç¤º
        tokens = self.tokenizer.encode(prompt)
        input_ids = torch.tensor([tokens], dtype=torch.long).to(self.device)

        generated = prompt
        past_key_values = None

        with torch.no_grad():
            for _ in range(max_length):
                # è·å–é¢„æµ‹
                outputs = self.model(input_ids)
                next_token_logits = outputs[0, -1, :] / temperature

                # Top-k é‡‡æ ·
                if top_k > 0:
                    top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)
                    next_token_logits = torch.full_like(next_token_logits, float('-inf'))
                    next_token_logits[top_k_indices] = top_k_logits

                # Top-p é‡‡æ ·
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    sorted_probs = torch.softmax(sorted_logits, dim=-1)
                    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

                    # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„token
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0

                    next_token_logits[sorted_indices[sorted_indices_to_remove]] = float('-inf')

                # é‡‡æ ·
                probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, 1).item()

                # è§£ç å¹¶æ·»åŠ åˆ°ç»“æœ
                next_char = self.tokenizer.decode([next_token])
                generated += next_char

                # æ›´æ–°è¾“å…¥
                next_token_tensor = torch.tensor([[next_token]], dtype=torch.long).to(self.device)
                input_ids = torch.cat([input_ids, next_token_tensor], dim=1)

                # é™åˆ¶é•¿åº¦
                if len(input_ids[0]) >= 1024:
                    break

        return generated


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="H2Q-Evo é‡å­å¢å¼ºæœ¬åœ°è®­ç»ƒä¸è¿›åŒ–ç³»ç»Ÿ")
    parser.add_argument("--mode", choices=["train", "generate", "evolve"],
                       default="train", help="è¿è¡Œæ¨¡å¼ï¼štrain(è®­ç»ƒ) | generate(ç”Ÿæˆ) | evolve(è¿›åŒ–)")
    parser.add_argument("--data_dir", type=str,
                       help="è®­ç»ƒæ•°æ®ç›®å½•ï¼ˆé»˜è®¤ä¸ºè‡ªåŠ¨åˆ›å»ºï¼‰")
    parser.add_argument("--epochs", type=int, default=10,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--prompt", type=str, default="äººå·¥æ™ºèƒ½çš„å‘å±•",
                       help="ç”Ÿæˆæ–‡æœ¬çš„æç¤º")
    parser.add_argument("--max_length", type=int, default=200,
                       help="ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦")

    args = parser.parse_args()

    if args.mode == "train":
        # é‡å­å¢å¼ºè®­ç»ƒæ¨¡å¼
        config = QuantumEnhancedConfig(max_epochs=args.epochs)
        trainer = QuantumEnhancedTrainer(config)

        data_dir = Path(args.data_dir) if args.data_dir else PROJECT_ROOT / "data" / "training_data"
        trainer.train(data_dir)

    elif args.mode == "generate":
        # æ–‡æœ¬ç”Ÿæˆæ¨¡å¼
        generator = QuantumEnhancedGenerator()
        result = generator.generate_text(args.prompt, max_length=args.max_length)
        print(f"\nğŸ¯ æç¤º: {args.prompt}")
        print(f"ğŸ¤– ç”Ÿæˆç»“æœ:\n{result}\n")

    elif args.mode == "evolve":
        # è‡ªç”±è¿›åŒ–æ¨¡å¼
        print("ğŸ§¬ å¯åŠ¨é‡å­å¢å¼ºè‡ªç”±è¿›åŒ–æ¨¡å¼...")
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„è¿›åŒ–é€»è¾‘
        config = QuantumEnhancedConfig(max_epochs=5)
        trainer = QuantumEnhancedTrainer(config)

        data_dir = PROJECT_ROOT / "data" / "training_data"
        trainer.train(data_dir)

        # ç”Ÿæˆæµ‹è¯•æ–‡æœ¬éªŒè¯è´¨é‡
        generator = QuantumEnhancedGenerator()
        test_prompts = ["é‡å­è®¡ç®—", "æœºå™¨å­¦ä¹ ", "äººå·¥æ™ºèƒ½ä¼¦ç†"]

        print("\nğŸ“ è¿›åŒ–åæ–‡æœ¬ç”Ÿæˆæµ‹è¯•:")
        for prompt in test_prompts:
            result = generator.generate_text(prompt, max_length=100)
            print(f"\nğŸ¯ {prompt}:")
            print(f"ğŸ¤– {result[:200]}...")


if __name__ == "__main__":
    main()