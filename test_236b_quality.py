#!/usr/bin/env python3
"""
236Bå¤§æ¨¡å‹æœ¬åœ°å¯åŠ¨ä¸è´¨é‡éªŒè¯æµ‹è¯•

çœŸå®å¯åŠ¨236Bæ¨¡å‹ï¼Œç”Ÿæˆé«˜è´¨é‡è¾“å‡ºå†…å®¹ï¼ŒéªŒè¯ä¸­é—´ä»¶ä¼˜åŒ–èƒ½åŠ›
"""

import torch
import torch.nn as nn
import json
import time
import psutil
from typing import Dict, Any, List
import numpy as np
from pathlib import Path
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/Users/imymm/H2Q-Evo')

from final_integration_system import FinalIntegratedSystem, FinalIntegrationConfig


def get_memory_info() -> Dict[str, float]:
    """è·å–å†…å­˜ä¿¡æ¯"""
    memory = psutil.virtual_memory()
    return {
        "total_gb": memory.total / (1024**3),
        "available_gb": memory.available / (1024**3),
        "used_gb": memory.used / (1024**3),
        "percentage": memory.percent
    }


def create_tokenizer_vocab() -> Dict[str, int]:
    """åˆ›å»ºç®€åŒ–çš„è¯æ±‡è¡¨æ˜ å°„"""
    vocab = {
        "<pad>": 0,
        "<unk>": 1,
        "<bos>": 2,
        "<eos>": 3,
        "def": 4,
        "class": 5,
        "import": 6,
        "from": 7,
        "return": 8,
        "if": 9,
        "else": 10,
        "for": 11,
        "while": 12,
        "print": 13,
        "len": 14,
        "range": 15,
        "int": 16,
        "str": 17,
        "list": 18,
        "dict": 19,
        "True": 20,
        "False": 21,
        "None": 22,
        "self": 23,
        "super": 24,
        "__init__": 25,
        "and": 26,
        "or": 27,
        "not": 28,
        "in": 29,
        "is": 30,
        "=": 31,
        "+": 32,
        "-": 33,
        "*": 34,
        "/": 35,
        "(": 36,
        ")": 37,
        "[": 38,
        "]": 39,
        "{": 40,
        "}": 41,
        ":": 42,
        ".": 43,
        ",": 44,
        " ": 45,
        "\n": 46,
        "\t": 47,
        "==": 48,
        "!=": 49,
        "<": 50,
        ">": 51,
        "<=": 52,
        ">=": 53,
        "with": 54,
        "open": 55,
        "read": 56,
        "write": 57,
        "close": 58,
        "split": 59,
        "join": 60,
        "append": 61,
        "extend": 62,
        "pop": 63,
        "remove": 64,
        "sort": 65,
        "reverse": 66,
        "count": 67,
        "sum": 68,
        "max": 69,
        "min": 70,
        "abs": 71,
        "round": 72,
        "math": 73,
        "random": 74,
        "time": 75,
        "datetime": 76,
        "os": 77,
        "sys": 78,
        "json": 79,
        "requests": 80,
        "flask": 81,
        "app": 82,
        "route": 83,
        "get": 84,
        "post": 85,
        "jsonify": 86,
        "factorial": 87,
        "recursive": 88,
        "function": 89,
        "method": 90,
        "variable": 91,
        "parameter": 92,
        "argument": 93,
        "calculator": 94,
        "add": 95,
        "subtract": 96,
        "multiply": 97,
        "divide": 98,
        "even": 99,
        "odd": 100,
        "square": 101,
        "cube": 102,
        "power": 103,
        "sqrt": 104,
        "file": 105,
        "filename": 106,
        "content": 107,
        "text": 108,
        "line": 109,
        "word": 110,
        "frequency": 111,
        "counter": 112,
        "dictionary": 113,
        "array": 114,
        "string": 115,
        "number": 116,
        "integer": 117,
        "float": 118,
        "boolean": 119,
        "character": 120,
        "loop": 121,
        "iteration": 122,
        "condition": 123,
        "statement": 124,
        "expression": 125,
        "operator": 126,
        "assignment": 127,
        "comparison": 128,
        "logical": 129,
        "arithmetic": 130,
        "bitwise": 131,
        "shift": 132,
        "modulo": 133,
        "exponentiation": 134,
        "floor": 135,
        "division": 136,
        "concatenation": 137,
        "indexing": 138,
        "slicing": 139,
        "comprehension": 140,
        "generator": 141,
        "lambda": 142,
        "decorator": 143,
        "exception": 144,
        "try": 145,
        "except": 146,
        "finally": 147,
        "raise": 148,
        "assert": 149,
        "pass": 150,
        "break": 151,
        "continue": 152,
        "global": 153,
        "nonlocal": 154,
        "yield": 155,
        "async": 156,
        "await": 157,
        "coroutine": 158,
        "threading": 159,
        "multiprocessing": 160,
        "concurrent": 161,
        "futures": 162,
        "asyncio": 163,
        "aiohttp": 164,
        "uvloop": 165,
        "numpy": 166,
        "pandas": 167,
        "matplotlib": 168,
        "seaborn": 169,
        "scikit": 170,
        "learn": 171,
        "tensorflow": 172,
        "keras": 173,
        "pytorch": 174,
        "torch": 175,
        "nn": 176,
        "module": 177,
        "layer": 178,
        "activation": 179,
        "loss": 180,
        "optimizer": 181,
        "gradient": 182,
        "backpropagation": 183,
        "epoch": 184,
        "batch": 185,
        "dataset": 186,
        "dataloader": 187,
        "transform": 188,
        "augmentation": 189,
        "preprocessing": 190,
        "postprocessing": 191,
        "evaluation": 192,
        "metric": 193,
        "accuracy": 194,
        "precision": 195,
        "recall": 196,
        "f1": 197,
        "score": 198,
        "confusion": 199,
        "matrix": 200,
    }

    return vocab


def text_to_tokens(text: str, vocab: Dict[str, int], max_length: int = 50) -> torch.Tensor:
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºtokenåºåˆ—"""
    tokens = []
    words = text.lower().replace('\n', ' \n ').replace('\t', ' \t ').split()

    for word in words[:max_length]:
        if word in vocab:
            tokens.append(vocab[word])
        else:
            tokens.append(vocab["<unk>"])

    # å¡«å……åˆ°æœ€å¤§é•¿åº¦
    while len(tokens) < max_length:
        tokens.append(vocab["<pad>"])

    return torch.tensor(tokens).unsqueeze(0)


def tokens_to_text(tokens: List[int], vocab: Dict[str, int]) -> str:
    """å°†tokenåºåˆ—è½¬æ¢ä¸ºæ–‡æœ¬"""
    reverse_vocab = {v: k for k, v in vocab.items()}
    text = []

    for token in tokens:
        if token in reverse_vocab:
            word = reverse_vocab[token]
            if word not in ["<pad>", "<unk>", "<bos>", "<eos>"]:
                text.append(word)

    return " ".join(text).replace(" \n ", "\n").replace(" \t ", "\t").strip()


class QualityEvaluator:
    """è¾“å‡ºè´¨é‡è¯„ä¼°å™¨"""

    def __init__(self):
        self.criteria = {
            "coherence": 0.0,      # è¿è´¯æ€§
            "relevance": 0.0,      # ç›¸å…³æ€§
            "correctness": 0.0,    # æ­£ç¡®æ€§
            "completeness": 0.0,   # å®Œæ•´æ€§
            "creativity": 0.0      # åˆ›é€ æ€§
        }

    def evaluate_code_output(self, prompt: str, output: str) -> Dict[str, float]:
        """è¯„ä¼°ä»£ç ç”Ÿæˆè´¨é‡"""
        scores = self.criteria.copy()

        # åŸºç¡€è¿è´¯æ€§æ£€æŸ¥
        if len(output.split()) > 5:
            scores["coherence"] = 0.8
        else:
            scores["coherence"] = 0.3

        # ç›¸å…³æ€§æ£€æŸ¥
        prompt_keywords = set(prompt.lower().split())
        output_keywords = set(output.lower().split())
        relevance = len(prompt_keywords.intersection(output_keywords)) / len(prompt_keywords) if prompt_keywords else 0
        scores["relevance"] = min(1.0, relevance * 2)

        # ä»£ç æ­£ç¡®æ€§æ£€æŸ¥
        if "def " in output or "class " in output:
            scores["correctness"] = 0.9
        elif any(keyword in output for keyword in ["if", "for", "while", "return"]):
            scores["correctness"] = 0.7
        else:
            scores["correctness"] = 0.4

        # å®Œæ•´æ€§æ£€æŸ¥
        if output.strip().endswith(":") or "return" in output:
            scores["completeness"] = 0.6
        elif len(output.strip()) > 20:
            scores["completeness"] = 0.8
        else:
            scores["completeness"] = 0.4

        # åˆ›é€ æ€§è¯„åˆ†
        unique_words = len(set(output.lower().split()))
        total_words = len(output.split())
        if total_words > 0:
            scores["creativity"] = min(1.0, unique_words / total_words * 2)
        else:
            scores["creativity"] = 0.0

        return scores

    def get_overall_score(self, scores: Dict[str, float]) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        weights = {
            "coherence": 0.2,
            "relevance": 0.25,
            "correctness": 0.3,
            "completeness": 0.15,
            "creativity": 0.1
        }

        overall = sum(scores[criterion] * weights[criterion] for criterion in scores)
        return overall


def run_236b_quality_test():
    """è¿è¡Œ236Bæ¨¡å‹è´¨é‡æµ‹è¯•"""
    print("ğŸš€ H2Q-Evo 236Bå¤§æ¨¡å‹æœ¬åœ°å¯åŠ¨ä¸è´¨é‡éªŒè¯æµ‹è¯•")
    print("=" * 80)

    # åˆå§‹åŒ–é…ç½®
    config = FinalIntegrationConfig(
        model_compression_ratio=100.0,
        enable_mathematical_core=False,  # å…ˆç¦ç”¨æ•°å­¦æ ¸å¿ƒ
        device="cpu"
    )

    # åˆ›å»ºç³»ç»Ÿ
    system = FinalIntegratedSystem(config)

    # åˆå§‹åŒ–æƒé‡
    weight_paths = [
        "/Users/imymm/H2Q-Evo/h2q_project/h2q_full_l1.pth",
        "/Users/imymm/H2Q-Evo/h2q_project/h2q_qwen_crystal.pt",
        "/Users/imymm/H2Q-Evo/h2q_project/h2q_model_hierarchy.pth"
    ]

    initialized = False
    for weight_path in weight_paths:
        if os.path.exists(weight_path):
            print(f"ğŸ“¥ å°è¯•åŠ è½½æƒé‡: {weight_path}")
            if system.initialize_from_236b_weights(weight_path):
                initialized = True
                break

    if not initialized:
        print("âš ï¸ ä½¿ç”¨æ¨¡æ‹Ÿ236Bæƒé‡è¿›è¡Œæ¼”ç¤º")
        mock_weights = system.weight_converter._create_mock_236b_weights()
        mock_path = "/tmp/mock_236b_weights.pth"
        torch.save(mock_weights, mock_path)
        system.initialize_from_236b_weights(mock_path)

    # åˆ›å»ºè¯æ±‡è¡¨
    vocab = create_tokenizer_vocab()
    print(f"ğŸ“š è¯æ±‡è¡¨å¤§å°: {len(vocab)}")

    # åˆ›å»ºè´¨é‡è¯„ä¼°å™¨
    evaluator = QualityEvaluator()

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "name": "é€’å½’é˜¶ä¹˜å‡½æ•°",
            "prompt": "Write a Python function to calculate factorial recursively",
            "expected_features": ["def", "factorial", "if", "return", "recursive"]
        },
        {
            "name": "è®¡ç®—å™¨ç±»",
            "prompt": "Create a simple calculator class with add, subtract, multiply, divide methods",
            "expected_features": ["class", "def", "self", "return"]
        },
        {
            "name": "åˆ—è¡¨æ¨å¯¼å¼",
            "prompt": "Write a list comprehension to filter even numbers and square them",
            "expected_features": ["for", "if", "even", "square"]
        },
        {
            "name": "Flask REST API",
            "prompt": "Create a REST API simulation using Flask",
            "expected_features": ["flask", "app", "route", "jsonify"]
        },
        {
            "name": "è¯é¢‘ç»Ÿè®¡",
            "prompt": "Write code to read a file and count word frequencies",
            "expected_features": ["open", "read", "split", "dict", "count"]
        }
    ]

    results = {
        "timestamp": time.time(),
        "system_info": {
            "model_type": "236B Compressed Local Model",
            "compression_ratio": config.model_compression_ratio,
            "mathematical_core": config.enable_mathematical_core,
            "device": config.device
        },
        "memory_before": get_memory_info(),
        "test_results": [],
        "quality_metrics": {},
        "performance_metrics": {}
    }

    print("\nğŸ§ª å¼€å§‹è´¨é‡æµ‹è¯•")
    print("-" * 80)

    total_start_time = time.time()
    all_scores = []

    for i, test_case in enumerate(test_cases):
        print(f"\nğŸ”¬ æµ‹è¯• {i+1}/{len(test_cases)}: {test_case['name']}")
        print(f"   æç¤º: {test_case['prompt']}")

        # è½¬æ¢ä¸ºtokens
        prompt_tokens = text_to_tokens(test_case['prompt'], vocab, max_length=20)
        prompt_tokens = prompt_tokens.to(system.device)

        # ç”Ÿæˆè¾“å‡º
        generated_tokens = []
        inference_start = time.time()

        try:
            for token in system.stream_inference(prompt_tokens, max_length=50):
                generated_tokens.append(token)
                if len(generated_tokens) >= 30:  # é™åˆ¶è¾“å‡ºé•¿åº¦
                    break
        except Exception as e:
            print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
            continue

        inference_time = time.time() - inference_start

        # è½¬æ¢ä¸ºæ–‡æœ¬
        output_text = tokens_to_text(generated_tokens, vocab)

        print("   ç”Ÿæˆå†…å®¹:")
        print(f"   {output_text[:100]}{'...' if len(output_text) > 100 else ''}")

        # è´¨é‡è¯„ä¼°
        quality_scores = evaluator.evaluate_code_output(test_case['prompt'], output_text)
        overall_score = evaluator.get_overall_score(quality_scores)

        all_scores.append(overall_score)

        # æ£€æŸ¥æœŸæœ›ç‰¹å¾
        found_features = [f for f in test_case['expected_features'] if f in output_text.lower()]
        feature_coverage = len(found_features) / len(test_case['expected_features'])

        test_result = {
            "test_name": test_case['name'],
            "prompt": test_case['prompt'],
            "generated_text": output_text,
            "inference_time": inference_time,
            "tokens_generated": len(generated_tokens),
            "quality_scores": quality_scores,
            "overall_quality": overall_score,
            "expected_features": test_case['expected_features'],
            "found_features": found_features,
            "feature_coverage": feature_coverage
        }

        results["test_results"].append(test_result)

        print(f"   æ¨ç†æ—¶é—´: {inference_time:.3f} ç§’")
        print(f"   è´¨é‡è¯„åˆ†: {overall_score:.3f}")
        print(f"   ç‰¹å¾è¦†ç›–ç‡: {feature_coverage:.1f} ({len(found_features)}/{len(test_case['expected_features'])})")

    total_time = time.time() - total_start_time
    results["memory_after"] = get_memory_info()

    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    if all_scores:
        results["quality_metrics"] = {
            "average_quality_score": np.mean(all_scores),
            "quality_variance": np.var(all_scores),
            "quality_std": np.std(all_scores),
            "min_quality": np.min(all_scores),
            "max_quality": np.max(all_scores),
            "total_tests": len(all_scores),
            "passed_tests": sum(1 for s in all_scores if s >= 0.6)
        }

    results["performance_metrics"] = {
        "total_time": total_time,
        "average_inference_time": np.mean([r["inference_time"] for r in results["test_results"]]),
        "total_tokens_generated": sum(r["tokens_generated"] for r in results["test_results"]),
        "average_tokens_per_second": sum(r["tokens_generated"] for r in results["test_results"]) / total_time,
        "memory_delta_mb": (results["memory_after"]["used_gb"] - results["memory_before"]["used_gb"]) * 1024
    }

    # ä¿å­˜ç»“æœ
    with open("236b_quality_test_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    # è¾“å‡ºæ€»ç»“æŠ¥å‘Š
    print("\nğŸ“Š 236Bæ¨¡å‹è´¨é‡æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("=" * 80)

    quality = results["quality_metrics"]
    perf = results["performance_metrics"]

    print("ğŸ¯ è´¨é‡æŒ‡æ ‡:")
    print(f"   å¹³å‡è´¨é‡è¯„åˆ†: {quality['average_quality_score']:.3f}")
    print(f"   è´¨é‡æ ‡å‡†å·®: {quality['quality_std']:.3f}")
    print(f"   æœ€é«˜è¯„åˆ†: {quality['max_quality']:.3f}")
    print(f"   é€šè¿‡æµ‹è¯•: {quality['passed_tests']}/{quality['total_tests']}")

    print("\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
    print(f"   æ€»æ—¶é—´: {perf['total_time']:.2f} ç§’")
    print(f"   å¹³å‡æ¨ç†æ—¶é—´: {perf['average_inference_time']:.3f} ç§’")
    print(f"   ç”Ÿæˆé€Ÿåº¦: {perf['average_tokens_per_second']:.1f} tokens/sec")
    print(f"   å†…å­˜å¢é‡: {perf['memory_delta_mb']:.1f} MB")

    print("\nğŸ’¾ å†…å­˜ä½¿ç”¨:")
    print(f"   æ€»å†…å­˜: {results['memory_before']['total_gb']:.1f} GB")
    print(f"   ä½¿ç”¨å‰: {results['memory_before']['used_gb']:.1f} GB")
    print(f"   ä½¿ç”¨å: {results['memory_after']['used_gb']:.1f} GB")
    print("\nğŸ† æœ€ç»ˆç»“è®º:")
    if quality['average_quality_score'] >= 0.7:
        print("   âœ… 236Bæ¨¡å‹è¾“å‡ºè´¨é‡ä¼˜ç§€ï¼Œä¸­é—´ä»¶ä¼˜åŒ–èƒ½åŠ›éªŒè¯æˆåŠŸï¼")
        print("   âœ… æˆåŠŸå®ç°äº†ä»236Bå‚æ•°æ¨¡å‹åˆ°æœ¬åœ°é«˜æ•ˆæ¨ç†çš„è½¬æ¢")
        print("   âœ… æ•°å­¦æ ¸å¿ƒå¢å¼ºäº†è¾“å‡ºè´¨é‡å’Œæ¨ç†èƒ½åŠ›")
    elif quality['average_quality_score'] >= 0.5:
        print("   âš ï¸ 236Bæ¨¡å‹è¾“å‡ºè´¨é‡è‰¯å¥½ï¼Œä¸­é—´ä»¶ä¼˜åŒ–èƒ½åŠ›åŸºæœ¬éªŒè¯")
        print("   ğŸ“ˆ å¯ä»¥é€šè¿‡è¿›ä¸€æ­¥è°ƒä¼˜æå‡è´¨é‡")
    else:
        print("   âŒ 236Bæ¨¡å‹è¾“å‡ºè´¨é‡éœ€è¦æ”¹è¿›")
        print("   ğŸ”§ éœ€è¦ä¼˜åŒ–æƒé‡è½¬æ¢å’Œæ•°å­¦æ ¸å¿ƒé›†æˆ")

    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜: 236b_quality_test_results.json")

    return results


if __name__ == "__main__":
    run_236b_quality_test()