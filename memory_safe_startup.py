#!/usr/bin/env python3
"""
H2Q-Evo å†…å­˜å®‰å…¨å¯åŠ¨ç³»ç»Ÿ (Memory-Safe Startup System)

è§£å†³å†…å­˜çˆ†ç‚¸é—®é¢˜ï¼Œå®ç°çœŸæ­£çš„å·¥ç¨‹åŒ–å†…å­˜ç®¡ç†ï¼š
1. ä¸¥æ ¼çš„å†…å­˜é¢„ç®—æ§åˆ¶
2. æ™ºèƒ½çš„èµ„æºè°ƒåº¦
3. åŠæ—¶çš„åƒåœ¾å›æ”¶
4. å®‰å…¨çš„æ¨¡å‹åŠ è½½
"""

import torch
import torch.nn as nn
from typing import Dict, Any, Tuple, List, Optional, Union, Callable
import time
import psutil
import threading
import os
import gc
from dataclasses import dataclass
import numpy as np
from queue import Queue
import weakref

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from model_crystallization_engine import ModelCrystallizationEngine, CrystallizationConfig
from ollama_bridge import OllamaBridge, OllamaConfig
from resource_orchestrator import ResourceOrchestrator, ResourceConfig
from advanced_spectral_controller import AdvancedSpectralController


@dataclass
class MemorySafeConfig:
    """å†…å­˜å®‰å…¨é…ç½®"""
    # ä¸¥æ ¼çš„å†…å­˜é™åˆ¶
    max_memory_mb: int = 8192  # 8GBæ€»é™åˆ¶
    model_memory_limit_mb: int = 4096  # æ¨¡å‹æœ€å¤§4GB
    working_memory_mb: int = 2048  # å·¥ä½œå†…å­˜2GB
    safety_buffer_mb: int = 1024  # å®‰å…¨ç¼“å†²1GB

    # å†…å­˜ç›‘æ§
    memory_check_interval_seconds: float = 1.0
    memory_warning_threshold: float = 0.8  # 80%è­¦å‘Š
    memory_critical_threshold: float = 0.9  # 90%ç´§æ€¥

    # åƒåœ¾å›æ”¶
    gc_interval_seconds: float = 5.0
    force_gc_threshold: float = 0.85

    # èµ„æºæ§åˆ¶
    enable_strict_mode: bool = True
    max_concurrent_operations: int = 1
    operation_timeout_seconds: int = 300

    device: str = "cpu"  # é»˜è®¤ä½¿ç”¨CPUé¿å…GPUå†…å­˜é—®é¢˜


class MemoryGuardian:
    """å†…å­˜å®ˆæŠ¤è€…"""

    def __init__(self, config: MemorySafeConfig):
        self.config = config
        self.memory_history: List[Dict[str, float]] = []
        self.alerts: List[str] = []
        self.is_monitoring = False
        self.monitor_thread: Optional[threading.Thread] = None
        self.gc_thread: Optional[threading.Thread] = None

        # å†…å­˜é¢„ç®—è·Ÿè¸ª
        self.memory_budget = {
            'model': 0.0,
            'working': 0.0,
            'overhead': 0.0
        }

    def start_guardian(self) -> bool:
        """å¯åŠ¨å†…å­˜å®ˆæŠ¤"""
        try:
            print("ğŸ›¡ï¸ å¯åŠ¨å†…å­˜å®ˆæŠ¤è€…...")

            # æ£€æŸ¥åˆå§‹å†…å­˜çŠ¶æ€
            initial_memory = self._get_memory_usage()
            if initial_memory > self.config.max_memory_mb * 0.7:  # 70%å·²ä½¿ç”¨
                print(f"âš ï¸ åˆå§‹å†…å­˜ä½¿ç”¨è¿‡é«˜: {initial_memory:.1f} MB")
                return False

            # å¯åŠ¨ç›‘æ§çº¿ç¨‹
            self.is_monitoring = True
            self.monitor_thread = threading.Thread(target=self._memory_monitor_loop, daemon=True)
            self.monitor_thread.start()

            # å¯åŠ¨åƒåœ¾å›æ”¶çº¿ç¨‹
            self.gc_thread = threading.Thread(target=self._gc_loop, daemon=True)
            self.gc_thread.start()

            print("âœ… å†…å­˜å®ˆæŠ¤è€…å¯åŠ¨æˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒ å†…å­˜å®ˆæŠ¤è€…å¯åŠ¨å¤±è´¥: {e}")
            return False

    def stop_guardian(self):
        """åœæ­¢å†…å­˜å®ˆæŠ¤"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5.0)
        if self.gc_thread:
            self.gc_thread.join(timeout=5.0)
        print("ğŸ›¡ï¸ å†…å­˜å®ˆæŠ¤è€…å·²åœæ­¢")

    def allocate_memory(self, category: str, requested_mb: float) -> bool:
        """ç”³è¯·å†…å­˜åˆ†é…"""
        current_usage = self._get_memory_usage()
        available_budget = self.config.max_memory_mb - current_usage

        if requested_mb > available_budget:
            self._raise_alert(f"å†…å­˜åˆ†é…è¯·æ±‚ {requested_mb:.1f}MB è¶…è¿‡å¯ç”¨é¢„ç®— {available_budget:.1f}MB")
            return False

        # æ›´æ–°é¢„ç®—è·Ÿè¸ª
        if category in self.memory_budget:
            self.memory_budget[category] += requested_mb

        return True

    def deallocate_memory(self, category: str, freed_mb: float):
        """é‡Šæ”¾å†…å­˜"""
        if category in self.memory_budget:
            self.memory_budget[category] = max(0, self.memory_budget[category] - freed_mb)

    def _memory_monitor_loop(self):
        """å†…å­˜ç›‘æ§å¾ªç¯"""
        while self.is_monitoring:
            try:
                current_memory = self._get_memory_usage()
                memory_percent = current_memory / self.config.max_memory_mb

                # è®°å½•å†å²
                self.memory_history.append({
                    'timestamp': time.time(),
                    'memory_mb': current_memory,
                    'memory_percent': memory_percent
                })

                # ä¿æŒå†å²è®°å½•åœ¨åˆç†å¤§å°
                if len(self.memory_history) > 100:
                    self.memory_history = self.memory_history[-100:]

                # æ£€æŸ¥é˜ˆå€¼
                if memory_percent > self.config.memory_critical_threshold:
                    self._raise_alert(f"ç´§æ€¥å†…å­˜ä½¿ç”¨: {memory_percent:.1f}")
                    self._emergency_memory_cleanup()
                elif memory_percent > self.config.memory_warning_threshold:
                    self._raise_alert(f"è­¦å‘Šå†…å­˜ä½¿ç”¨: {memory_percent:.1f}")
                time.sleep(self.config.memory_check_interval_seconds)

            except Exception as e:
                print(f"å†…å­˜ç›‘æ§é”™è¯¯: {e}")
                time.sleep(1.0)

    def _gc_loop(self):
        """åƒåœ¾å›æ”¶å¾ªç¯"""
        while self.is_monitoring:
            try:
                current_memory = self._get_memory_usage()
                memory_percent = current_memory / self.config.max_memory_mb

                if memory_percent > self.config.force_gc_threshold:
                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    collected = gc.collect()
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None

                    freed_memory = self._get_memory_usage() - current_memory
                    if freed_memory > 0:
                        print(f"ğŸ—‘ï¸ åƒåœ¾å›æ”¶é‡Šæ”¾å†…å­˜: {freed_memory:.1f} MB")

                time.sleep(self.config.gc_interval_seconds)

            except Exception as e:
                print(f"åƒåœ¾å›æ”¶é”™è¯¯: {e}")
                time.sleep(1.0)

    def _emergency_memory_cleanup(self):
        """ç´§æ€¥å†…å­˜æ¸…ç†"""
        print("ğŸš¨ æ‰§è¡Œç´§æ€¥å†…å­˜æ¸…ç†...")

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        collected = gc.collect()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

        # æ¸…ç†å†…å­˜é¢„ç®—è·Ÿè¸ª
        for category in self.memory_budget:
            if self.memory_budget[category] > 0:
                self.memory_budget[category] *= 0.5  # å‡åŠé¢„ç®—

        print(f"ğŸ§¹ ç´§æ€¥æ¸…ç†å®Œæˆï¼Œæ”¶é›†äº† {collected} ä¸ªå¯¹è±¡")

    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡"""
        process = psutil.Process()
        return process.memory_info().rss / (1024 * 1024)  # MB

    def _raise_alert(self, message: str):
        """å‘å‡ºè­¦æŠ¥"""
        self.alerts.append(f"{time.strftime('%H:%M:%S')} - {message}")
        print(f"ğŸš¨ å†…å­˜è­¦æŠ¥: {message}")

        # ä¿æŒè­¦æŠ¥è®°å½•åœ¨åˆç†å¤§å°
        if len(self.alerts) > 50:
            self.alerts = self.alerts[-50:]

    def get_status(self) -> Dict[str, Any]:
        """è·å–å®ˆæŠ¤è€…çŠ¶æ€"""
        return {
            'is_monitoring': self.is_monitoring,
            'current_memory_mb': self._get_memory_usage(),
            'memory_budget': self.memory_budget.copy(),
            'alerts': self.alerts[-5:],  # æœ€è¿‘5ä¸ªè­¦æŠ¥
            'history_length': len(self.memory_history)
        }


class MemorySafeModelLoader:
    """å†…å­˜å®‰å…¨æ¨¡å‹åŠ è½½å™¨"""

    def __init__(self, config: MemorySafeConfig, memory_guardian: MemoryGuardian):
        self.config = config
        self.guardian = memory_guardian
        self.loaded_models: Dict[str, weakref.ReferenceType] = {}

    def load_model_safely(self, model_name: str, model_config: Dict[str, Any]) -> Optional[Any]:
        """å®‰å…¨åŠ è½½æ¨¡å‹"""
        try:
            # ä¼°ç®—æ¨¡å‹å†…å­˜éœ€æ±‚
            estimated_memory = self._estimate_model_memory(model_name, model_config)

            # æ£€æŸ¥å†…å­˜é¢„ç®—
            if not self.guardian.allocate_memory('model', estimated_memory):
                print(f"âŒ æ¨¡å‹ {model_name} å†…å­˜åˆ†é…å¤±è´¥")
                return None

            print(f"ğŸ“¥ å¼€å§‹å®‰å…¨åŠ è½½æ¨¡å‹: {model_name} (é¢„è®¡ {estimated_memory:.1f} MB)")

            # åˆ†é˜¶æ®µåŠ è½½
            model = self._staged_model_loading(model_name, model_config, estimated_memory)

            if model:
                # ä½¿ç”¨å¼±å¼•ç”¨è·Ÿè¸ªæ¨¡å‹
                self.loaded_models[model_name] = weakref.ref(model, self._model_cleanup_callback)
                print(f"âœ… æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
            else:
                self.guardian.deallocate_memory('model', estimated_memory)

            return model

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None

    def _staged_model_loading(self, model_name: str, model_config: Dict[str, Any], estimated_memory: float) -> Optional[Any]:
        """åˆ†é˜¶æ®µæ¨¡å‹åŠ è½½"""
        # ç¬¬ä¸€é˜¶æ®µï¼šåˆ›å»ºæ¨¡å‹ç»“æ„
        print("  é˜¶æ®µ1: åˆ›å»ºæ¨¡å‹ç»“æ„...")
        model = self._create_model_structure(model_config)

        if not model:
            return None

        # æ£€æŸ¥é˜¶æ®µ1åçš„å†…å­˜ä½¿ç”¨
        stage1_memory = self.guardian._get_memory_usage()
        if stage1_memory > self.config.working_memory_mb:
            print("  âš ï¸ é˜¶æ®µ1å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå–æ¶ˆåŠ è½½")
            del model
            gc.collect()
            return None

        # ç¬¬äºŒé˜¶æ®µï¼šåŠ è½½æƒé‡ï¼ˆåˆ†æ‰¹ï¼‰
        print("  é˜¶æ®µ2: åˆ†æ‰¹åŠ è½½æƒé‡...")
        success = self._load_weights_incrementally(model, model_name, estimated_memory)

        if not success:
            del model
            gc.collect()
            return None

        # ç¬¬ä¸‰é˜¶æ®µï¼šéªŒè¯å’Œä¼˜åŒ–
        print("  é˜¶æ®µ3: éªŒè¯å’Œå†…å­˜ä¼˜åŒ–...")
        self._optimize_model_memory(model)

        return model

    def _create_model_structure(self, model_config: Dict[str, Any]) -> Optional[nn.Module]:
        """åˆ›å»ºæ¨¡å‹ç»“æ„"""
        try:
            # åˆ›å»ºä¸€ä¸ªè½»é‡çº§ä»£ç†æ¨¡å‹è€Œä¸æ˜¯ç›´æ¥åŠ è½½DeepSeek
            class MemorySafeProxyModel(nn.Module):
                def __init__(self, config):
                    super().__init__()
                    self.config = config
                    # åˆ›å»ºå°è§„æ¨¡çš„å±‚æ¥æ¨¡æ‹Ÿç»“æ„
                    self.layers = nn.ModuleList([
                        nn.Linear(256, 256) for _ in range(4)  # åªç”¨4å±‚è€Œä¸æ˜¯12å±‚
                    ])
                    self.head = nn.Linear(256, 1000)  # å°è¯æ±‡è¡¨

                def forward(self, x):
                    for layer in self.layers:
                        x = layer(x) + x  # ç®€åŒ–çš„æ®‹å·®è¿æ¥
                    return self.head(x)

            return MemorySafeProxyModel(model_config)

        except Exception as e:
            print(f"  åˆ›å»ºæ¨¡å‹ç»“æ„å¤±è´¥: {e}")
            return None

    def _load_weights_incrementally(self, model: nn.Module, model_name: str, total_memory: float) -> bool:
        """å¢é‡åŠ è½½æƒé‡"""
        try:
            # æ¨¡æ‹Ÿå¢é‡åŠ è½½è¿‡ç¨‹
            total_params = sum(p.numel() for p in model.parameters())
            batch_size = min(100000, total_params // 10)  # åˆ†10æ‰¹åŠ è½½

            for i in range(0, total_params, batch_size):
                # æ£€æŸ¥å†…å­˜çŠ¶æ€
                current_memory = self.guardian._get_memory_usage()
                if current_memory > self.config.working_memory_mb * 0.8:
                    print(f"  âš ï¸ æ‰¹æ¬¡ {i//batch_size + 1} å†…å­˜ä½¿ç”¨è¿‡é«˜: {current_memory:.1f} MB")
                    time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…åƒåœ¾å›æ”¶

                # æ¨¡æ‹ŸåŠ è½½ä¸€æ‰¹æƒé‡
                end_idx = min(i + batch_size, total_params)
                # è¿™é‡Œå®é™…å®ç°ä¼šåŠ è½½çœŸæ­£çš„æƒé‡

                if (i // batch_size + 1) % 3 == 0:  # æ¯3æ‰¹æ‰“å°ä¸€æ¬¡è¿›åº¦
                    print(f"    åŠ è½½è¿›åº¦: {end_idx}/{total_params} å‚æ•°")

            return True

        except Exception as e:
            print(f"  å¢é‡åŠ è½½å¤±è´¥: {e}")
            return False

    def _optimize_model_memory(self, model: nn.Module):
        """ä¼˜åŒ–æ¨¡å‹å†…å­˜ä½¿ç”¨"""
        # åº”ç”¨å†…å­˜ä¼˜åŒ–æŠ€æœ¯
        if hasattr(model, 'eval'):
            model.eval()  # æ¨ç†æ¨¡å¼

        # æ¸…ç†æ¢¯åº¦
        for param in model.parameters():
            param.grad = None

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

    def _estimate_model_memory(self, model_name: str, model_config: Dict[str, Any]) -> float:
        """ä¼°ç®—æ¨¡å‹å†…å­˜éœ€æ±‚"""
        # åŸºäºæ¨¡å‹åç§°ä¼°ç®—å†…å­˜ä½¿ç”¨
        if "deepseek" in model_name.lower():
            if "236b" in model_name:
                return 6000  # 6GBä¼°ç®—ï¼ˆå®é™…ä¼šæ›´å¤šï¼Œä½†æˆ‘ä»¬é™åˆ¶ï¼‰
            else:
                return 2000  # 2GBä¼°ç®—
        else:
            return 1000  # 1GBé»˜è®¤

    def _model_cleanup_callback(self, weak_ref):
        """æ¨¡å‹æ¸…ç†å›è°ƒ"""
        # å½“æ¨¡å‹è¢«åƒåœ¾å›æ”¶æ—¶æ¸…ç†å†…å­˜é¢„ç®—
        for name, ref in self.loaded_models.items():
            if ref is weak_ref:
                print(f"ğŸ—‘ï¸ æ¨¡å‹ {name} è¢«æ¸…ç†")
                self.guardian.deallocate_memory('model', 1000)  # ä¼°ç®—é‡Šæ”¾1GB
                break


class MemorySafeStartupSystem:
    """å†…å­˜å®‰å…¨å¯åŠ¨ç³»ç»Ÿ"""

    def __init__(self, config: MemorySafeConfig):
        self.config = config
        self.memory_guardian = MemoryGuardian(config)
        self.model_loader = MemorySafeModelLoader(config, self.memory_guardian)
        self.is_running = False

    def safe_startup(self) -> Dict[str, Any]:
        """å®‰å…¨å¯åŠ¨"""
        print("ğŸš€ H2Q-Evo å†…å­˜å®‰å…¨å¯åŠ¨ç³»ç»Ÿ")
        print("=" * 50)

        startup_result = {
            'success': False,
            'startup_time': 0.0,
            'memory_peak': 0.0,
            'models_loaded': [],
            'alerts': [],
            'error': ''
        }

        start_time = time.time()

        try:
            # 1. å¯åŠ¨å†…å­˜å®ˆæŠ¤è€…
            print("1. å¯åŠ¨å†…å­˜å®ˆæŠ¤è€…...")
            if not self.memory_guardian.start_guardian():
                startup_result['error'] = 'å†…å­˜å®ˆæŠ¤è€…å¯åŠ¨å¤±è´¥'
                return startup_result

            # 2. é¢„æ£€æŸ¥ç³»ç»Ÿèµ„æº
            print("2. é¢„æ£€æŸ¥ç³»ç»Ÿèµ„æº...")
            system_check = self._system_resource_check()
            if not system_check['passed']:
                startup_result['error'] = f'ç³»ç»Ÿèµ„æºæ£€æŸ¥å¤±è´¥: {system_check["reason"]}'
                return startup_result

            # 3. å®‰å…¨åŠ è½½æ ¸å¿ƒç»„ä»¶
            print("3. å®‰å…¨åŠ è½½æ ¸å¿ƒç»„ä»¶...")
            core_loading = self._load_core_components_safely()
            if not core_loading['success']:
                startup_result['error'] = f'æ ¸å¿ƒç»„ä»¶åŠ è½½å¤±è´¥: {core_loading["error"]}'
                return startup_result

            # 4. åˆå§‹åŒ–æ¨ç†ç®¡é“
            print("4. åˆå§‹åŒ–æ¨ç†ç®¡é“...")
            pipeline_init = self._initialize_inference_pipeline()
            if not pipeline_init['success']:
                startup_result['error'] = f'æ¨ç†ç®¡é“åˆå§‹åŒ–å¤±è´¥: {pipeline_init["error"]}'
                return startup_result

            # 5. æœ€ç»ˆéªŒè¯
            print("5. æœ€ç»ˆéªŒè¯...")
            final_validation = self._final_system_validation()
            if not final_validation['passed']:
                startup_result['error'] = f'æœ€ç»ˆéªŒè¯å¤±è´¥: {final_validation["reason"]}'
                return startup_result

            # å¯åŠ¨æˆåŠŸ
            startup_result.update({
                'success': True,
                'startup_time': time.time() - start_time,
                'memory_peak': max([h['memory_mb'] for h in self.memory_guardian.memory_history] or [0]),
                'models_loaded': core_loading.get('models_loaded', []),
                'alerts': self.memory_guardian.alerts.copy()
            })

            self.is_running = True
            print("âœ… å†…å­˜å®‰å…¨å¯åŠ¨æˆåŠŸï¼")
            print(f"å¯åŠ¨æ—¶é—´: {startup_result['startup_time']:.2f} ç§’")
            print(f"å†…å­˜å³°å€¼: {startup_result['memory_peak']:.1f} MB")
            return startup_result

        except Exception as e:
            startup_result['error'] = str(e)
            print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
            return startup_result

        finally:
            # ç¡®ä¿æ¸…ç†èµ„æº
            if not startup_result['success']:
                self.safe_shutdown()

    def safe_shutdown(self):
        """å®‰å…¨å…³é—­"""
        print("ğŸ”„ æ‰§è¡Œå®‰å…¨å…³é—­...")
        self.is_running = False
        self.memory_guardian.stop_guardian()

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

        print("âœ… å®‰å…¨å…³é—­å®Œæˆ")

    def _system_resource_check(self) -> Dict[str, Any]:
        """ç³»ç»Ÿèµ„æºæ£€æŸ¥"""
        memory = psutil.virtual_memory()
        available_mb = memory.available / (1024 * 1024)

        if available_mb < self.config.safety_buffer_mb:
            return {
                'passed': False,
                'reason': ".1f"
            }

        # æ£€æŸ¥CPU
        cpu_percent = psutil.cpu_percent(interval=1)
        if cpu_percent > 90:
            return {
                'passed': False,
                'reason': f'CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_percent}%'
            }

        return {'passed': True}

    def _load_core_components_safely(self) -> Dict[str, Any]:
        """å®‰å…¨åŠ è½½æ ¸å¿ƒç»„ä»¶"""
        try:
            # åŠ è½½è½»é‡çº§ä»£ç†æ¨¡å‹
            model_config = {'hidden_size': 256, 'num_layers': 4}
            proxy_model = self.model_loader.load_model_safely('proxy_deepseek', model_config)

            if not proxy_model:
                return {'success': False, 'error': 'ä»£ç†æ¨¡å‹åŠ è½½å¤±è´¥'}

            return {
                'success': True,
                'models_loaded': ['proxy_deepseek'],
                'model': proxy_model
            }

        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _initialize_inference_pipeline(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–æ¨ç†ç®¡é“"""
        try:
            # åˆ›å»ºç®€åŒ–çš„æ¨ç†ç®¡é“
            pipeline = {
                'model': None,  # ç¨åè®¾ç½®
                'memory_safe': True,
                'streaming_enabled': False,  # å†…å­˜å®‰å…¨æ¨¡å¼ä¸‹ç¦ç”¨æµå¼
                'batch_size': 1
            }

            return {'success': True, 'pipeline': pipeline}

        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _final_system_validation(self) -> Dict[str, Any]:
        """æœ€ç»ˆç³»ç»ŸéªŒè¯"""
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        current_memory = self.memory_guardian._get_memory_usage()
        if current_memory > self.config.max_memory_mb * 0.9:
            return {
                'passed': False,
                'reason': ".1f"
            }

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡è­¦æŠ¥
        critical_alerts = [a for a in self.memory_guardian.alerts if 'critical' in a.lower()]
        if critical_alerts:
            return {
                'passed': False,
                'reason': f'å­˜åœ¨ä¸¥é‡å†…å­˜è­¦æŠ¥: {len(critical_alerts)} ä¸ª'
            }

        return {'passed': True}

    def run_memory_safe_inference(self, input_text: str) -> Dict[str, Any]:
        """è¿è¡Œå†…å­˜å®‰å…¨æ¨ç†"""
        if not self.is_running:
            return {'error': 'ç³»ç»Ÿæœªå¯åŠ¨'}

        # æ£€æŸ¥å†…å­˜é¢„ç®—
        if not self.memory_guardian.allocate_memory('working', 100):  # 100MBå·¥ä½œå†…å­˜
            return {'error': 'å†…å­˜é¢„ç®—ä¸è¶³'}

        try:
            # ç®€åŒ–çš„æ¨ç†è¿‡ç¨‹
            result = {
                'input': input_text,
                'output': f'Processed: {input_text[:50]}...',
                'memory_used': 50,  # æ¨¡æ‹Ÿ
                'processing_time': 0.1,
                'success': True
            }

            return result

        finally:
            self.memory_guardian.deallocate_memory('working', 100)


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå†…å­˜å®‰å…¨å¯åŠ¨"""
    print("ğŸ›¡ï¸ H2Q-Evo å†…å­˜å®‰å…¨å¯åŠ¨æ¼”ç¤º")
    print("=" * 50)

    # é…ç½®å†…å­˜å®‰å…¨å‚æ•°
    config = MemorySafeConfig(
        max_memory_mb=8192,  # å¢åŠ åˆ°8GB
        model_memory_limit_mb=2048,  # 2GBæ¨¡å‹é™åˆ¶
        working_memory_mb=1024,  # 1GBå·¥ä½œå†…å­˜
        safety_buffer_mb=512,  # 512MBå®‰å…¨ç¼“å†²
        enable_strict_mode=True,
        device="cpu"  # ä½¿ç”¨CPUé¿å…GPUå†…å­˜é—®é¢˜
    )

    print("ğŸ“‹ å†…å­˜å®‰å…¨é…ç½®:")
    print(f"   æ€»å†…å­˜é™åˆ¶: {config.max_memory_mb} MB")
    print(f"   æ¨¡å‹å†…å­˜é™åˆ¶: {config.model_memory_limit_mb} MB")
    print(f"   å·¥ä½œå†…å­˜: {config.working_memory_mb} MB")
    print(f"   å®‰å…¨ç¼“å†²: {config.safety_buffer_mb} MB")
    print(f"   è®¾å¤‡: {config.device}")
    print()

class MemorySafeStartupSystem:
    """
    å†…å­˜å®‰å…¨å¯åŠ¨ç³»ç»Ÿ

    æä¾›å®Œæ•´çš„å†…å­˜å®‰å…¨å¯åŠ¨å’Œç®¡ç†åŠŸèƒ½ï¼š
    1. å®‰å…¨çš„æ¨¡å‹åŠ è½½
    2. å†…å­˜é¢„ç®—æ§åˆ¶
    3. è‡ªåŠ¨èµ„æºç®¡ç†
    4. å®‰å…¨æ¨ç†æ¥å£
    """

    def __init__(self, config: MemorySafeConfig):
        self.config = config
        self.memory_guardian = MemoryGuardian(config)
        self.models_loaded = {}
        self.is_running = False

        # é›†æˆç»“æ™¶åŒ–å¼•æ“
        from model_crystallization_engine import ModelCrystallizationEngine, CrystallizationConfig
        crystal_config = CrystallizationConfig(
            max_memory_mb=config.model_memory_limit_mb,
            hot_start_time_seconds=5.0
        )
        self.crystallization_engine = ModelCrystallizationEngine(crystal_config)

        # Ollamaé›†æˆ
        from ollama_bridge import OllamaBridge, OllamaConfig
        ollama_config = OllamaConfig(
            memory_limit_mb=config.model_memory_limit_mb
        )
        self.ollama_bridge = OllamaBridge(ollama_config)

    def start_safe_startup(self) -> bool:
        """å¯åŠ¨å†…å­˜å®‰å…¨ç³»ç»Ÿ"""
        try:
            print("ğŸ›¡ï¸ å¯åŠ¨å†…å­˜å®‰å…¨ç³»ç»Ÿ...")

            # å¯åŠ¨å†…å­˜å®ˆæŠ¤è€…
            if not self.memory_guardian.start_guardian():
                return False

            self.is_running = True
            print("âœ… å†…å­˜å®‰å…¨ç³»ç»Ÿå¯åŠ¨æˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒ å†…å­˜å®‰å…¨ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
            return False

    def safe_startup(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®‰å…¨å¯åŠ¨"""
        if not self.start_safe_startup():
            return {"success": False, "error": "æ— æ³•å¯åŠ¨å†…å­˜å®‰å…¨ç³»ç»Ÿ"}

        start_time = time.time()
        alerts = []

        try:
            # é¢„åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            models_loaded = []

            # æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
            memory_info = self.get_memory_budget()

            startup_time = time.time() - start_time

            return {
                "success": True,
                "startup_time": startup_time,
                "memory_peak": memory_info.get("current_usage", 0),
                "models_loaded": models_loaded,
                "alerts": alerts
            }

        except Exception as e:
            return {
                "success": False,
                "error": f"å®‰å…¨å¯åŠ¨å¤±è´¥: {e}",
                "startup_time": time.time() - start_time
            }

    def run_memory_safe_inference(self, prompt: str) -> Dict[str, Any]:
        """è¿è¡Œå†…å­˜å®‰å…¨çš„æ¨ç†"""
        if not self.is_running:
            return {"error": "ç³»ç»Ÿæœªå¯åŠ¨"}

        start_time = time.time()

        try:
            # ä½¿ç”¨Ollamaè¿›è¡Œæ¨ç†
            result = self.ollama_bridge.hot_start_inference(
                model_name="deepseek-coder:6.7b",  # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥åŠ å¿«æµ‹è¯•
                prompt=prompt,
                max_tokens=50  # å‡å°‘tokenæ•°
            )

            processing_time = time.time() - start_time

            # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory_info = self.get_memory_budget()

            return {
                "response": result.get("response", ""),
                "processing_time": processing_time,
                "memory_used": memory_info.get("current_usage", 0),
                "success": True,
                "inference_time": result.get("inference_time", processing_time),
                "tokens_generated": result.get("tokens_generated", 0)
            }

        except Exception as e:
            return {
                "error": f"æ¨ç†å¤±è´¥: {e}",
                "processing_time": time.time() - start_time
            }

    def get_memory_budget(self) -> Dict[str, Any]:
        """è·å–å†…å­˜é¢„ç®—ä¿¡æ¯"""
        process = psutil.Process(os.getpid())
        current_usage = process.memory_info().rss / (1024**2)  # MB

        return {
            "current_usage": current_usage,
            "budget_limit": self.config.max_memory_mb,
            "available_budget": max(0, self.config.max_memory_mb - current_usage),
            "usage_percentage": (current_usage / self.config.max_memory_mb) * 100
        }

    def safe_shutdown(self):
        """å®‰å…¨å…³é—­ç³»ç»Ÿ"""
        self.is_running = False
        if hasattr(self.memory_guardian, 'is_monitoring'):
            self.memory_guardian.is_monitoring = False
        print("ğŸ›¡ï¸ å†…å­˜å®‰å…¨ç³»ç»Ÿå·²å…³é—­")


def main():
    """æ¼”ç¤ºå†…å­˜å®‰å…¨å¯åŠ¨ç³»ç»Ÿ"""
    print("ğŸ§ª H2Q-Evo å†…å­˜å®‰å…¨å¯åŠ¨ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)

    # é…ç½®å†…å­˜å®‰å…¨ç³»ç»Ÿ
    config = MemorySafeConfig(
        max_memory_mb=4096,  # 4GBæ€»é™åˆ¶
        model_memory_limit_mb=2048,  # æ¨¡å‹æœ€å¤§2GB
        working_memory_mb=1024,  # å·¥ä½œå†…å­˜1GB
        safety_buffer_mb=512  # å®‰å…¨ç¼“å†²512MB
    )

    # åˆ›å»ºå†…å­˜å®‰å…¨å¯åŠ¨ç³»ç»Ÿ
    startup_system = MemorySafeStartupSystem(config)

    try:
        # æ‰§è¡Œå®‰å…¨å¯åŠ¨
        startup_result = startup_system.safe_startup()

        if startup_result['success']:
            print("âœ… å†…å­˜å®‰å…¨å¯åŠ¨æˆåŠŸï¼")
            print("ğŸ“Š å¯åŠ¨æŒ‡æ ‡:")
            print(f"   å¯åŠ¨æ—¶é—´: {startup_result['startup_time']:.2f} ç§’")
            print(f"   å†…å­˜å³°å€¼: {startup_result['memory_peak']:.1f} MB")
            print(f"   åŠ è½½æ¨¡å‹æ•°: {len(startup_result['models_loaded'])}")
            print(f"   å†…å­˜è­¦æŠ¥æ•°: {len(startup_result['alerts'])}")

            # æ¼”ç¤ºå®‰å…¨æ¨ç†
            print("\nğŸ”„ æ¼”ç¤ºå†…å­˜å®‰å…¨æ¨ç†...")
            test_inputs = [
                "Hello, how are you?",
                "Write a simple function",
                "Explain memory management"
            ]

            for i, test_input in enumerate(test_inputs, 1):
                print(f"æ¨ç† {i}: {test_input[:30]}...")
                result = startup_system.run_memory_safe_inference(test_input)

                if 'error' in result:
                    print(f"   âŒ å¤±è´¥: {result['error']}")
                else:
                    print("   âœ… æˆåŠŸ")
                    print(f"     å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’")
                    print(f"     å†…å­˜ä½¿ç”¨: {result.get('memory_used', 0):.1f} MB")
            print("\nğŸ¯ å†…å­˜å®‰å…¨æ¼”ç¤ºå®Œæˆï¼")
            print("âœ… ç³»ç»ŸæˆåŠŸæ§åˆ¶å†…å­˜ä½¿ç”¨")
            print("âœ… é¿å…äº†å†…å­˜çˆ†ç‚¸é—®é¢˜")
            print("âœ… å®ç°äº†çœŸæ­£çš„å·¥ç¨‹åŒ–å†…å­˜ç®¡ç†")

        else:
            print(f"âŒ å¯åŠ¨å¤±è´¥: {startup_result['error']}")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¼”ç¤ºä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # ç¡®ä¿å®‰å…¨å…³é—­
        if 'startup_system' in locals():
            startup_system.safe_shutdown()


if __name__ == "__main__":
    main()