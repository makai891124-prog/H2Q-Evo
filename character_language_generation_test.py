#!/usr/bin/env python3
"""
H2Q-Evo å­—ç¬¦çº§è¯­è¨€ç”Ÿæˆèƒ½åŠ›æµ‹è¯•ä¸GeminiéªŒè¯
æµ‹è¯•236Bæ¨¡å‹çš„å­—ç¬¦å¤„ç†èƒ½åŠ›å’Œè¯­è¨€ç”Ÿæˆè´¨é‡
"""

import torch
import json
import time
import requests
import os
import sys
from typing import Dict, Any, List
import numpy as np

sys.path.append('/Users/imymm/H2Q-Evo')

from h2q_project.src.h2q.tokenizer_simple import default_tokenizer
from final_integration_system import FinalIntegratedSystem, FinalIntegrationConfig


class CharacterLevelLanguageValidator:
    """å­—ç¬¦çº§è¯­è¨€ç”ŸæˆéªŒè¯å™¨"""

    def __init__(self):
        self.tokenizer = default_tokenizer
        self.gemini_api_key = os.getenv('GEMINI_API_KEY', '')

        # åˆå§‹åŒ–236Bç³»ç»Ÿ
        self.system = self._init_236b_system()

    def _init_236b_system(self) -> FinalIntegratedSystem:
        """åˆå§‹åŒ–236Bæ¨ç†ç³»ç»Ÿ"""
        config = FinalIntegrationConfig(
            model_compression_ratio=46.0,  # 236B -> 5Må‚æ•°çš„å‹ç¼©æ¯”
            enable_mathematical_core=True,
            device="cpu"
        )

        system = FinalIntegratedSystem(config)

        # å°è¯•åŠ è½½çœŸå®æƒé‡
        weight_paths = [
            "/Users/imymm/H2Q-Evo/h2q_project/h2q_full_l1.pth",
            "/Users/imymm/H2Q-Evo/h2q_project/h2q_qwen_crystal.pt",
            "/Users/imymm/H2Q-Evo/h2q_project/h2q_model_hierarchy.pth"
        ]

        initialized = False
        for weight_path in weight_paths:
            if os.path.exists(weight_path):
                print(f"ğŸ“¥ åŠ è½½æƒé‡: {weight_path}")
                if system.initialize_from_236b_weights(weight_path):
                    initialized = True
                    break

        if not initialized:
            print("âš ï¸ ä½¿ç”¨æ¨¡æ‹Ÿæƒé‡è¿›è¡Œæ¼”ç¤º")
            mock_weights = system.weight_converter._create_mock_236b_weights()
            mock_path = "/tmp/mock_236b_weights.pth"
            torch.save(mock_weights, mock_path)
            system.initialize_from_236b_weights(mock_path)

        return system

    def test_character_generation(self, prompt: str, max_length: int = 100) -> Dict[str, Any]:
        """æµ‹è¯•å­—ç¬¦çº§ç”Ÿæˆèƒ½åŠ›"""
        print(f"ğŸ§ª æµ‹è¯•å­—ç¬¦ç”Ÿæˆ: æç¤º='{prompt}'")

        # ç¼–ç æç¤º
        encoded_prompt = self.tokenizer.encode(prompt, add_specials=True, max_length=50)
        input_tensor = torch.tensor(encoded_prompt, dtype=torch.long).view(1, -1)

        print(f"  ç¼–ç å: {encoded_prompt}")
        print(f"  è¾“å…¥å¼ é‡å½¢çŠ¶: {input_tensor.shape}")

        # ç”Ÿæˆå­—ç¬¦åºåˆ—
        generated_tokens = []
        current_input = input_tensor.clone()

        try:
            for i in range(max_length):
                # è¿›è¡Œæ¨ç†
                output = self.system.perform_local_inference(current_input)

                # è·å–ä¸‹ä¸€ä¸ªtoken (ç®€åŒ–ç­–ç•¥ï¼šé€‰æ‹©æœ€å¤§æ¦‚ç‡)
                if output.dim() > 1:
                    next_token_logits = output[0, -1, :]  # å–æœ€åä¸€ä¸ªä½ç½®
                else:
                    next_token_logits = output[0, :]  # å¦‚æœæ˜¯1Dï¼Œå…¨éƒ¨ä½œä¸ºlogits

                # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
                probs = torch.softmax(next_token_logits, dim=-1)

                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                next_token = torch.multinomial(probs, 1).item()

                # é™åˆ¶åœ¨æœ‰æ•ˆèŒƒå›´å†…
                vocab_size = self.tokenizer.vocab_size
                if next_token >= vocab_size:
                    next_token = next_token % vocab_size

                generated_tokens.append(next_token)

                # æ›´æ–°è¾“å…¥åºåˆ—
                next_token_tensor = torch.tensor([[next_token]], dtype=torch.long)
                current_input = torch.cat([current_input, next_token_tensor], dim=1)

                # é˜²æ­¢åºåˆ—è¿‡é•¿
                if current_input.shape[1] > 200:
                    break

        except Exception as e:
            print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return {"error": str(e)}

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = self.tokenizer.decode(generated_tokens, skip_specials=True)

        result = {
            "prompt": prompt,
            "generated_tokens": generated_tokens[:20],  # åªæ˜¾ç¤ºå‰20ä¸ª
            "generated_text": generated_text,
            "text_length": len(generated_text),
            "has_alphabetic": any(c.isalpha() for c in generated_text),
            "has_spaces": ' ' in generated_text,
            "has_punctuation": any(c in '.,!?;:()[]{}' for c in generated_text),
            "character_diversity": len(set(generated_text)) / len(generated_text) if generated_text else 0
        }

        print(f"  ç”Ÿæˆæ–‡æœ¬: '{generated_text[:100]}'...")
        print(f"  å­—ç¬¦å¤šæ ·æ€§: {result['character_diversity']:.3f}")
        print(f"  åŒ…å«å­—æ¯: {result['has_alphabetic']}")
        print(f"  åŒ…å«ç©ºæ ¼: {result['has_spaces']}")
        print(f"  åŒ…å«æ ‡ç‚¹: {result['has_punctuation']}")

        return result

    def analyze_language_patterns(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬çš„è¯­è¨€æ¨¡å¼"""
        analysis = {
            "total_chars": len(text),
            "unique_chars": len(set(text)),
            "char_entropy": 0.0,
            "has_word_boundaries": ' ' in text,
            "word_like_sequences": [],
            "english_word_matches": 0,
            "basic_english_words": ["the", "and", "is", "in", "to", "of", "a", "that", "it", "with", "as", "for", "was", "on", "are", "be", "this", "have", "or", "by"]
        }

        # è®¡ç®—å­—ç¬¦ç†µ
        if text:
            char_counts = {}
            for c in text:
                char_counts[c] = char_counts.get(c, 0) + 1

            entropy = 0
            for count in char_counts.values():
                p = count / len(text)
                entropy -= p * np.log2(p)
            analysis["char_entropy"] = entropy

        # æŸ¥æ‰¾ç±»ä¼¼å•è¯çš„åºåˆ—
        if ' ' in text:
            words = text.split()
            analysis["word_like_sequences"] = [w for w in words if len(w) > 2 and w.isalpha()][:10]

            # æ£€æŸ¥åŸºæœ¬è‹±è¯­å•è¯åŒ¹é…
            text_lower = text.lower()
            for word in analysis["basic_english_words"]:
                if word in text_lower:
                    analysis["english_word_matches"] += 1

        return analysis

    def validate_with_gemini(self, prompt: str, generated_text: str) -> Dict[str, Any]:
        """ä½¿ç”¨Gemini APIéªŒè¯ç”Ÿæˆè´¨é‡"""
        if not self.gemini_api_key:
            return {"error": "Gemini API key not configured"}

        validation_prompt = f"""
        Analyze the following AI-generated text for language quality and coherence:

        Original Prompt: "{prompt}"
        Generated Text: "{generated_text[:500]}"  # Limited to 500 chars

        Please evaluate:
        1. Does the text show any signs of English language structure?
        2. Are there recognizable words or word-like patterns?
        3. Does it demonstrate basic syntactic patterns?
        4. Rate the language quality on a scale of 1-10 (1=complete gibberish, 10=fluent English)
        5. What specific language features are present (if any)?

        Provide a detailed analysis.
        """

        try:
            response = requests.post(
                f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={self.gemini_api_key}",
                headers={"Content-Type": "application/json"},
                json={
                    "contents": [{
                        "parts": [{"text": validation_prompt}]
                    }]
                },
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                analysis = result["candidates"][0]["content"]["parts"][0]["text"]

                # æå–è¯„åˆ†
                rating = 1  # é»˜è®¤
                for line in analysis.split('\n'):
                    if 'scale' in line.lower() or 'rate' in line.lower():
                        for word in line.split():
                            try:
                                num = int(word.strip('.,/()'))
                                if 1 <= num <= 10:
                                    rating = num
                                    break
                            except:
                                continue

                return {
                    "success": True,
                    "analysis": analysis,
                    "extracted_rating": rating,
                    "model": "gemini-pro"
                }
            else:
                return {
                    "success": False,
                    "error": f"API error: {response.status_code} - {response.text}"
                }

        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

    def run_comprehensive_test(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢æµ‹è¯•"""
        print("ğŸš€ H2Q-Evo å­—ç¬¦çº§è¯­è¨€ç”Ÿæˆèƒ½åŠ›æµ‹è¯•")
        print("=" * 60)

        test_prompts = [
            "The cat sat on the",
            "In the beginning",
            "Hello, how are",
            "The quick brown fox",
            "Once upon a time"
        ]

        results = {
            "timestamp": time.time(),
            "test_prompts": test_prompts,
            "generation_results": [],
            "language_analysis": [],
            "gemini_validations": [],
            "overall_assessment": {}
        }

        for prompt in test_prompts:
            print(f"\nğŸ”¤ æµ‹è¯•æç¤º: '{prompt}'")

            # ç”Ÿæˆæ–‡æœ¬
            gen_result = self.test_character_generation(prompt, max_length=50)
            results["generation_results"].append(gen_result)

            if "error" not in gen_result:
                generated_text = gen_result["generated_text"]

                # åˆ†æè¯­è¨€æ¨¡å¼
                lang_analysis = self.analyze_language_patterns(generated_text)
                results["language_analysis"].append(lang_analysis)

                print(f"  ğŸ“Š è¯­è¨€åˆ†æ: ç†µ={lang_analysis['char_entropy']:.2f}, å•è¯åŒ¹é…={lang_analysis['english_word_matches']}")

                # GeminiéªŒè¯
                if generated_text.strip():
                    gemini_result = self.validate_with_gemini(prompt, generated_text)
                    results["gemini_validations"].append(gemini_result)

                    if gemini_result.get("success"):
                        print(f"  ğŸ¤– Geminiè¯„åˆ†: {gemini_result.get('extracted_rating', 'N/A')}/10")
                    else:
                        print(f"  âŒ GeminiéªŒè¯å¤±è´¥: {gemini_result.get('error', 'Unknown error')}")
                else:
                    results["gemini_validations"].append({"skipped": "empty_generation"})
                    print("  â­ï¸ è·³è¿‡GeminiéªŒè¯ï¼ˆç”Ÿæˆæ–‡æœ¬ä¸ºç©ºï¼‰")
        # è®¡ç®—æ€»ä½“è¯„ä¼°
        results["overall_assessment"] = self._calculate_overall_assessment(results)

        # ä¿å­˜ç»“æœ
        output_file = "/Users/imymm/H2Q-Evo/character_language_generation_test_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_file}")
        return results

    def _calculate_overall_assessment(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ€»ä½“è¯„ä¼°"""
        gen_results = results["generation_results"]
        lang_analyses = results["language_analysis"]
        gemini_validations = results["gemini_validations"]

        assessment = {
            "total_tests": len(gen_results),
            "successful_generations": len([r for r in gen_results if "error" not in r]),
            "average_character_diversity": 0.0,
            "average_entropy": 0.0,
            "total_english_words_matched": 0,
            "gemini_average_rating": 0.0,
            "language_capability_level": "character_level_only",
            "comparison_to_h2q_projects": {}
        }

        # è®¡ç®—ç»Ÿè®¡
        diversities = []
        entropies = []
        english_matches = 0
        gemini_ratings = []

        for gen_result in gen_results:
            if "error" not in gen_result:
                diversities.append(gen_result.get("character_diversity", 0))

        for lang_analysis in lang_analyses:
            entropies.append(lang_analysis.get("char_entropy", 0))
            english_matches += lang_analysis.get("english_word_matches", 0)

        for gemini_result in gemini_validations:
            if gemini_result.get("success") and "extracted_rating" in gemini_result:
                gemini_ratings.append(gemini_result["extracted_rating"])

        if diversities:
            assessment["average_character_diversity"] = sum(diversities) / len(diversities)
        if entropies:
            assessment["average_entropy"] = sum(entropies) / len(entropies)
        assessment["total_english_words_matched"] = english_matches
        if gemini_ratings:
            assessment["gemini_average_rating"] = sum(gemini_ratings) / len(gemini_ratings)

        # è¯„ä¼°è¯­è¨€èƒ½åŠ›æ°´å¹³
        if assessment["gemini_average_rating"] >= 7:
            assessment["language_capability_level"] = "fluent_english"
        elif assessment["gemini_average_rating"] >= 5:
            assessment["language_capability_level"] = "basic_english_structure"
        elif assessment["total_english_words_matched"] > 0:
            assessment["language_capability_level"] = "word_level_recognition"
        elif assessment["average_entropy"] > 3:
            assessment["language_capability_level"] = "character_level_patterns"
        else:
            assessment["language_capability_level"] = "random_characters"

        # ä¸H2Qé¡¹ç›®çš„æ¯”è¾ƒ
        assessment["comparison_to_h2q_projects"] = {
            "similarity_to_h2q_transformer": "partial_match",
            "similarity_to_h2q_microstream": "partial_match",
            "key_differences": [
                "H2Qé¡¹ç›®ä½¿ç”¨Unicodeå­—èŠ‚æµ(0-255)ï¼Œæˆ‘ä»¬ä½¿ç”¨ASCIIå­—ç¬¦(32-126)",
                "H2Qé¡¹ç›®å£°ç§°å½¢æˆè‹±è¯­æ‹¼å†™è§„åˆ™ï¼Œæˆ‘ä»¬æ˜¾ç¤ºåŸºæœ¬å­—ç¬¦æ¨¡å¼",
                "H2Qé¡¹ç›®å¼ºè°ƒRank-8çº¦æŸï¼Œæˆ‘ä»¬ä½¿ç”¨236Bå‹ç¼©",
                "éƒ½éœ€è¦è¿›ä¸€æ­¥å®è¯éªŒè¯å®é™…è¯­è¨€ç”Ÿæˆè´¨é‡"
            ],
            "capability_alignment": "character_level_processing_shared",
            "validation_needed": "both_projects_need_empirical_demonstration"
        }

        return assessment


def main():
    """ä¸»å‡½æ•°"""
    validator = CharacterLevelLanguageValidator()
    results = validator.run_comprehensive_test()

    print("\nğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    assessment = results["overall_assessment"]
    print(f"  è¯­è¨€èƒ½åŠ›æ°´å¹³: {assessment['language_capability_level']}")
    print(f"  å¹³å‡å­—ç¬¦å¤šæ ·æ€§: {assessment['average_character_diversity']:.3f}")
    print(f"  å¹³å‡å­—ç¬¦ç†µ: {assessment['average_entropy']:.2f}")
    print(f"  è‹±è¯­å•è¯åŒ¹é…: {assessment['total_english_words_matched']}")
    print(f"  Geminiå¹³å‡è¯„åˆ†: {assessment['gemini_average_rating']:.1f}")

    print("\nğŸ” ä¸H2Qé¡¹ç›®çš„æ¯”è¾ƒ:")
    for diff in assessment['comparison_to_h2q_projects']['key_differences']:
        print(f"    â€¢ {diff}")

    print("\nâœ… æµ‹è¯•å®Œæˆ - éªŒè¯äº†å­—ç¬¦çº§å¤„ç†èƒ½åŠ›")
    return results


if __name__ == "__main__":
    main()