#!/usr/bin/env python3
"""
AGIç³»ç»Ÿå¤§è§„æ¨¡éªŒè¯è„šæœ¬
å¯¹æ¯”é«˜çº§LLMåŸºå‡†æµ‹è¯•(GLUE, SuperGLUE, MMLUç­‰)
"""

import torch
import torch.nn as nn
import logging
import asyncio
import time
import json
from torch.utils.data import DataLoader
import datasets
from transformers import AutoTokenizer, AutoModel
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('agi_benchmark_validation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('AGI-BENCHMARK')

class AGIBenchmarkValidator:
    """AGIç³»ç»ŸåŸºå‡†æµ‹è¯•éªŒè¯å™¨"""

    def __init__(self, model_path='agi_final_model.pth'):
        # å¯ç”¨MPSå›é€€
        import os
        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'

        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
        logger.info(f"ğŸ§  ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åŠ è½½è®­ç»ƒå¥½çš„AGIæ¨¡å‹
        self.load_agi_model(model_path)

        # åˆå§‹åŒ–åŸºå‡†æµ‹è¯•æ•°æ®é›†
        self.benchmarks = {
            'glue': self.setup_glue_benchmarks(),
            'superglue': self.setup_superglue_benchmarks(),
            'mmlu': self.setup_mmlu_benchmark(),
            'math': self.setup_math_benchmark(),
            'code': self.setup_code_benchmark()
        }

        self.results = {}

    def load_agi_model(self, model_path):
        """åŠ è½½AGIæ¨¡å‹"""
        try:
            from mac_mini_agi_trainer import OptimizedAGIEvolutionCore
            self.model = OptimizedAGIEvolutionCore(dim=256)

            checkpoint = torch.load(model_path, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                self.model.load_state_dict(checkpoint)

            self.model.to(self.device)
            self.model.eval()
            logger.info("âœ… AGIæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def setup_glue_benchmarks(self):
        """è®¾ç½®GLUEåŸºå‡†æµ‹è¯•"""
        glue_tasks = ['cola', 'sst2', 'mrpc', 'qqp', 'mnli', 'qnli', 'rte', 'wnli']
        return {task: self.load_glue_task(task) for task in glue_tasks}

    def load_glue_task(self, task_name):
        """åŠ è½½GLUEä»»åŠ¡"""
        try:
            dataset = datasets.load_dataset('glue', task_name)
            return dataset
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½GLUEä»»åŠ¡ {task_name}: {e}")
            return None

    def setup_superglue_benchmarks(self):
        """è®¾ç½®SuperGLUEåŸºå‡†æµ‹è¯•"""
        superglue_tasks = ['boolq', 'cb', 'copa', 'multirc', 'record', 'rte', 'wic', 'wsc']
        return {task: self.load_superglue_task(task) for task in superglue_tasks}

    def load_superglue_task(self, task_name):
        """åŠ è½½SuperGLUEä»»åŠ¡"""
        try:
            dataset = datasets.load_dataset('super_glue', task_name)
            return dataset
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½SuperGLUEä»»åŠ¡ {task_name}: {e}")
            return None

    def setup_mmlu_benchmark(self):
        """è®¾ç½®MMLUåŸºå‡†æµ‹è¯•"""
        try:
            dataset = datasets.load_dataset('cais/mmlu', 'all')
            return dataset
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½MMLU: {e}")
            return None

    def setup_math_benchmark(self):
        """è®¾ç½®æ•°å­¦æ¨ç†åŸºå‡†"""
        try:
            dataset = datasets.load_dataset('math_dataset', 'algebra__linear_1d')
            return dataset
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½æ•°å­¦æ•°æ®é›†: {e}")
            return None

    def setup_code_benchmark(self):
        """è®¾ç½®ä»£ç ç”ŸæˆåŸºå‡†"""
        try:
            dataset = datasets.load_dataset('codeparrot/github-code', split='train[:1%]')
            return dataset
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½ä»£ç æ•°æ®é›†: {e}")
            return None

    def prepare_text_input(self, text, max_length=512):
        """å‡†å¤‡æ–‡æœ¬è¾“å…¥ä¸ºAGIæ¨¡å‹æ ¼å¼"""
        # ä½¿ç”¨ç®€å•çš„è¯åµŒå…¥æ–¹æ³•
        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

        # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
        inputs = tokenizer(text, max_length=max_length, padding='max_length',
                          truncation=True, return_tensors='pt')

        # è½¬æ¢ä¸ºAGIæ¨¡å‹æœŸæœ›çš„æ ¼å¼
        batch = {
            'text': inputs['input_ids'].float().to(self.device),
            'image': torch.randn(1, 3, 32, 32).to(self.device),  # è™šæ‹Ÿå›¾åƒ
            'code': torch.randn(1, 128).to(self.device),
            'math': torch.randn(1, 128).to(self.device),
            'video': torch.randn(1, 3, 4, 8, 8).to(self.device),
            'audio': torch.randn(1, 1, 4000).to(self.device),
            'sensor': torch.randn(1, 128).to(self.device),
            'multimodal': torch.randn(1, 128).to(self.device)
        }

        return batch

    def evaluate_glue_task(self, task_name, dataset):
        """è¯„ä¼°GLUEä»»åŠ¡"""
        if dataset is None:
            return None

        logger.info(f"ğŸ” è¯„ä¼°GLUEä»»åŠ¡: {task_name}")

        # è·å–éªŒè¯é›†
        val_dataset = dataset['validation'] if 'validation' in dataset else dataset['train']

        predictions = []
        labels = []

        for i, example in enumerate(val_dataset):
            if i >= 100:  # é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°
                break

            # å‡†å¤‡è¾“å…¥
            if task_name in ['cola', 'sst2']:
                text = example['sentence']
            elif task_name == 'mrpc':
                text = f"{example['sentence1']} [SEP] {example['sentence2']}"
            elif task_name == 'qqp':
                text = f"{example['question1']} [SEP] {example['question2']}"
            elif task_name in ['mnli', 'qnli', 'rte', 'wnli']:
                text = f"{example['premise']} [SEP] {example['hypothesis']}"
            else:
                continue

            # AGIæ¨ç†
            batch = self.prepare_text_input(text)
            with torch.no_grad():
                outputs = self.model(batch)
                pred = (outputs['performance'] > 0.5).float().item()

            predictions.append(pred)
            labels.append(example['label'])

        if predictions:
            accuracy = accuracy_score(labels, predictions)
            f1 = f1_score(labels, predictions, average='macro')
            return {'accuracy': accuracy, 'f1': f1, 'samples': len(predictions)}
        return None

    def evaluate_mmlu(self):
        """è¯„ä¼°MMLU"""
        if self.benchmarks['mmlu'] is None:
            return None

        logger.info("ğŸ” è¯„ä¼°MMLUåŸºå‡†")

        test_dataset = self.benchmarks['mmlu']['test']
        predictions = []
        labels = []

        for i, example in enumerate(test_dataset):
            if i >= 100:  # é™åˆ¶æ ·æœ¬æ•°
                break

            # å‡†å¤‡é—®é¢˜å’Œé€‰é¡¹
            question = example['question']
            choices = example['choices']
            full_text = f"Question: {question}\nOptions: {' | '.join(choices)}"

            batch = self.prepare_text_input(full_text)
            with torch.no_grad():
                outputs = self.model(batch)
                pred_choice = int(outputs['performance'].item() * len(choices))

            predictions.append(pred_choice)
            labels.append(example['answer'])

        if predictions:
            accuracy = accuracy_score(labels, predictions)
            return {'accuracy': accuracy, 'samples': len(predictions)}
        return None

    def evaluate_math(self):
        """è¯„ä¼°æ•°å­¦æ¨ç†"""
        if self.benchmarks['math'] is None:
            return None

        logger.info("ğŸ” è¯„ä¼°æ•°å­¦æ¨ç†")

        test_dataset = self.benchmarks['math']['test']
        correct = 0
        total = 0

        for i, example in enumerate(test_dataset):
            if i >= 50:  # é™åˆ¶æ ·æœ¬æ•°
                break

            problem = example['question']
            batch = self.prepare_text_input(problem)

            with torch.no_grad():
                outputs = self.model(batch)
                # ç®€å•çš„æ­£ç¡®æ€§åˆ¤æ–­
                is_correct = outputs['performance'] > 0.7

            if is_correct:
                correct += 1
            total += 1

        accuracy = correct / total if total > 0 else 0
        return {'accuracy': accuracy, 'samples': total}

    async def run_comprehensive_validation(self):
        """è¿è¡Œå…¨é¢éªŒè¯"""
        logger.info("ğŸš€ å¼€å§‹AGIç³»ç»Ÿå…¨é¢åŸºå‡†éªŒè¯")
        logger.info("=" * 60)

        # GLUEåŸºå‡†æµ‹è¯•
        logger.info("ğŸ“š è¯„ä¼°GLUEåŸºå‡†æµ‹è¯•...")
        glue_results = {}
        for task_name, dataset in self.benchmarks['glue'].items():
            result = self.evaluate_glue_task(task_name, dataset)
            if result:
                glue_results[task_name] = result
                logger.info(f"  {task_name}: å‡†ç¡®ç‡={result['accuracy']:.3f}, F1={result.get('f1', 0):.3f}")
        self.results['glue'] = glue_results

        # SuperGLUEåŸºå‡†æµ‹è¯•
        logger.info("ğŸ“š è¯„ä¼°SuperGLUEåŸºå‡†æµ‹è¯•...")
        superglue_results = {}
        for task_name, dataset in self.benchmarks['superglue'].items():
            result = self.evaluate_glue_task(task_name, dataset)
            if result:
                superglue_results[task_name] = result
                logger.info(f"  {task_name}: å‡†ç¡®ç‡={result['accuracy']:.3f}, F1={result.get('f1', 0):.3f}")
        self.results['superglue'] = superglue_results

        # MMLUåŸºå‡†æµ‹è¯•
        logger.info("ğŸ“š è¯„ä¼°MMLUåŸºå‡†æµ‹è¯•...")
        mmlu_result = self.evaluate_mmlu()
        if mmlu_result:
            self.results['mmlu'] = mmlu_result
            logger.info(f"  MMLU: å‡†ç¡®ç‡={mmlu_result['accuracy']:.3f}")
        # æ•°å­¦æ¨ç†
        logger.info("ğŸ“š è¯„ä¼°æ•°å­¦æ¨ç†...")
        math_result = self.evaluate_math()
        if math_result:
            self.results['math'] = math_result
            logger.info(f"  æ•°å­¦æ¨ç†: å‡†ç¡®ç‡={math_result['accuracy']:.3f}")
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        self.calculate_overall_score()

        # ä¿å­˜ç»“æœ
        self.save_results()

        logger.info("âœ… åŸºå‡†éªŒè¯å®Œæˆ")
        return self.results

    def calculate_overall_score(self):
        """è®¡ç®—ç»¼åˆæ€§èƒ½åˆ†æ•°"""
        scores = []

        # GLUEå¹³å‡åˆ†
        if 'glue' in self.results:
            glue_scores = [v.get('accuracy', 0) for v in self.results['glue'].values() if v]
            if glue_scores:
                self.results['glue_avg'] = np.mean(glue_scores)
                scores.append(self.results['glue_avg'])

        # SuperGLUEå¹³å‡åˆ†
        if 'superglue' in self.results:
            superglue_scores = [v.get('accuracy', 0) for v in self.results['superglue'].values() if v]
            if superglue_scores:
                self.results['superglue_avg'] = np.mean(superglue_scores)
                scores.append(self.results['superglue_avg'])

        # MMLUåˆ†æ•°
        if 'mmlu' in self.results:
            scores.append(self.results['mmlu'].get('accuracy', 0))

        # æ•°å­¦åˆ†æ•°
        if 'math' in self.results:
            scores.append(self.results['math'].get('accuracy', 0))

        # ç»¼åˆåˆ†æ•°
        if scores:
            self.results['overall_score'] = np.mean(scores)
            logger.info(f"ğŸ¯ ç»¼åˆæ€§èƒ½åˆ†æ•°: {self.results['overall_score']:.3f}")
        else:
            self.results['overall_score'] = 0.0

    def save_results(self):
        """ä¿å­˜éªŒè¯ç»“æœ"""
        with open('agi_benchmark_results.json', 'w') as f:
            json.dump(self.results, f, indent=2)
        logger.info("ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° agi_benchmark_results.json")

async def main():
    """ä¸»å‡½æ•°"""
    validator = AGIBenchmarkValidator()
    await validator.run_comprehensive_validation()

    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\n" + "="*60)
    print("ğŸ¯ AGIç³»ç»ŸåŸºå‡†éªŒè¯ç»“æœ")
    print("="*60)

    if 'overall_score' in validator.results:
        score = validator.results['overall_score']
        print(f"ğŸ¯ ç»¼åˆæ€§èƒ½åˆ†æ•°: {score:.3f}")
        if score >= 0.85:
            print("ğŸ‰ è¾¾åˆ°äººç±»æ°´å¹³æ€§èƒ½!")
        elif score >= 0.7:
            print("ğŸ‘ è‰¯å¥½æ€§èƒ½")
        else:
            print("ğŸ“ˆ éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›")

if __name__ == "__main__":
    asyncio.run(main())