#!/usr/bin/env python3
"""
H2Q-Evo èµ„æºä¼˜åŒ–å¯åŠ¨ç³»ç»Ÿ (Resource-Optimized Startup System)

é’ˆå¯¹æœ¬åœ°èµ„æºä¸è¶³çš„åœºæ™¯ï¼Œæ•´åˆæ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½ï¼š
1. åˆ†å±‚åŠ è½½å’Œè™šæ‹ŸåŒ–æŠ€æœ¯
2. æ¸è¿›å¼æ¨¡å‹æ¿€æ´»
3. å†…å­˜æ± ç®¡ç†å’Œæµå¼æ¨ç†
4. çƒ­å¯åŠ¨å’Œè°±ç¨³å®šæ€§æ§åˆ¶
5. æœ¬åœ°è¿›åŒ–èƒ½åŠ›ä¿æŒ
"""

import torch
import torch.nn as nn
from typing import Dict, Any, Tuple, List, Optional, Union, Callable
import time
import psutil
import threading
import os
from dataclasses import dataclass
import numpy as np
from concurrent.futures import ThreadPoolExecutor
from queue import Queue
import gc

# å¯¼å…¥H2Qæ ¸å¿ƒç»„ä»¶
from model_crystallization_engine import ModelCrystallizationEngine, CrystallizationConfig
from ollama_bridge import OllamaBridge, OllamaConfig
from hot_start_manager import HotStartManager, HotStartConfig, MemoryPoolManager
from resource_orchestrator import ResourceOrchestrator, ResourceConfig
from advanced_spectral_controller import AdvancedSpectralController


@dataclass
class ResourceOptimizedConfig:
    """èµ„æºä¼˜åŒ–é…ç½®"""
    # å†…å­˜ç®¡ç†
    max_memory_mb: int = 4096  # æ€»å†…å­˜é™åˆ¶
    memory_pool_size_mb: int = 1024  # å†…å­˜æ± å¤§å°
    virtual_memory_multiplier: int = 4  # è™šæ‹Ÿå†…å­˜å€æ•°

    # åˆ†å±‚åŠ è½½
    layer_activation_batch_size: int = 2  # å±‚æ¿€æ´»æ‰¹æ¬¡å¤§å°
    progressive_activation_steps: int = 10  # æ¸è¿›æ¿€æ´»æ­¥æ•°

    # æµå¼æ¨ç†
    enable_streaming_inference: bool = True
    streaming_chunk_size: int = 64
    max_concurrent_chunks: int = 4

    # çƒ­å¯åŠ¨
    hot_start_timeout_seconds: float = 10.0
    enable_hot_cache: bool = True

    # è¿›åŒ–ä¼˜åŒ–
    local_evolution_enabled: bool = True
    evolution_memory_budget_mb: int = 512
    spectral_stability_threshold: float = 0.05

    device: str = "mps" if torch.backends.mps.is_available() else "cpu"


class LayeredVirtualizationManager:
    """åˆ†å±‚è™šæ‹ŸåŒ–ç®¡ç†å™¨"""

    def __init__(self, config: ResourceOptimizedConfig):
        self.config = config
        self.layer_cache: Dict[str, Dict[str, Any]] = {}
        self.virtual_layers: Dict[str, nn.Module] = {}
        self.activation_queue = Queue()
        self.memory_pool = MemoryPoolManager(config.memory_pool_size_mb)

    def virtualize_model_layers(self, model: nn.Module, model_name: str) -> Dict[str, Any]:
        """å°†æ¨¡å‹å±‚è™šæ‹ŸåŒ–å­˜å‚¨"""
        print(f"å¼€å§‹å¯¹æ¨¡å‹ {model_name} è¿›è¡Œåˆ†å±‚è™šæ‹ŸåŒ–...")

        virtualized_layers = {}
        total_memory_saved = 0

        for name, module in model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d, nn.LayerNorm)):
                # è®¡ç®—å±‚å†…å­˜å ç”¨
                layer_memory = self._calculate_layer_memory(module)

                # å¦‚æœå±‚å†…å­˜è¶…è¿‡é˜ˆå€¼ï¼Œè¿›è¡Œè™šæ‹ŸåŒ–
                if layer_memory > self.config.memory_pool_size_mb * 0.1:  # 10%é˜ˆå€¼
                    virtualized_layers[name] = {
                        'type': type(module).__name__,
                        'config': self._extract_layer_config(module),
                        'memory_mb': layer_memory,
                        'virtualized': True,
                        'activation_count': 0
                    }
                    total_memory_saved += layer_memory
                else:
                    virtualized_layers[name] = {
                        'module': module,
                        'memory_mb': layer_memory,
                        'virtualized': False
                    }

        self.layer_cache[model_name] = virtualized_layers

        print(f"è™šæ‹ŸåŒ–å®Œæˆï¼ŒèŠ‚çœå†…å­˜: {total_memory_saved:.1f} MB")
        return {
            'total_layers': len(virtualized_layers),
            'virtualized_layers': sum(1 for v in virtualized_layers.values() if v['virtualized']),
            'memory_saved_mb': total_memory_saved,
            'virtualized_layers': virtualized_layers
        }

    def progressive_layer_activation(self, model_name: str, target_layer: str,
                                   progress_callback: Optional[Callable] = None) -> Optional[nn.Module]:
        """æ¸è¿›å¼å±‚æ¿€æ´»"""
        if model_name not in self.layer_cache:
            return None

        layer_info = self.layer_cache[model_name].get(target_layer)
        if not layer_info or not layer_info['virtualized']:
            return layer_info.get('module') if layer_info else None

        # æ£€æŸ¥å†…å­˜æ± æ˜¯å¦æœ‰è¶³å¤Ÿç©ºé—´
        if not self.memory_pool.can_allocate(target_layer, layer_info['memory_mb']):
            # é‡Šæ”¾å…¶ä»–å±‚æ¥è…¾å‡ºç©ºé—´
            self._evict_layers_for_space(layer_info['memory_mb'])

        # ä»å†…å­˜æ± åˆ†é…ç©ºé—´
        allocated_tensor = self.memory_pool.allocate(
            target_layer,
            layer_info['memory_mb'],
            self._get_layer_shape(layer_info),
            torch.float32
        )

        if allocated_tensor is None:
            return None

        # é‡å»ºå±‚
        reconstructed_layer = self._reconstruct_layer(layer_info)

        # æ›´æ–°æ¿€æ´»è®¡æ•°
        layer_info['activation_count'] += 1

        if progress_callback:
            progress_callback(1.0)

        return reconstructed_layer

    def _calculate_layer_memory(self, module: nn.Module) -> float:
        """è®¡ç®—å±‚å†…å­˜å ç”¨"""
        total_params = sum(p.numel() for p in module.parameters())
        return total_params * 4 / (1024**2)  # float32, MB

    def _extract_layer_config(self, module: nn.Module) -> Dict[str, Any]:
        """æå–å±‚é…ç½®"""
        config = {'type': type(module).__name__}

        if isinstance(module, nn.Linear):
            config.update({
                'in_features': module.in_features,
                'out_features': module.out_features,
                'bias': module.bias is not None
            })
        elif isinstance(module, nn.Conv2d):
            config.update({
                'in_channels': module.in_channels,
                'out_channels': module.out_channels,
                'kernel_size': module.kernel_size,
                'stride': module.stride,
                'padding': module.padding,
                'bias': module.bias is not None
            })

        return config

    def _reconstruct_layer(self, layer_info: Dict[str, Any]) -> nn.Module:
        """é‡å»ºå±‚"""
        config = layer_info['config']

        if config['type'] == 'Linear':
            return nn.Linear(
                config['in_features'],
                config['out_features'],
                bias=config['bias']
            )
        elif config['type'] == 'Conv2d':
            return nn.Conv2d(
                config['in_channels'],
                config['out_channels'],
                config['kernel_size'],
                config['stride'],
                config['padding'],
                bias=config['bias']
            )

        return None

    def _evict_layers_for_space(self, required_mb: float):
        """ä¸ºæ–°å±‚è…¾å‡ºç©ºé—´"""
        # ç®€å•çš„LRUç­–ç•¥
        evictable_layers = [
            (name, info) for name, info in self.layer_cache.items()
            if info.get('virtualized', False) and info.get('activation_count', 0) > 0
        ]

        evictable_layers.sort(key=lambda x: x[1]['activation_count'])

        freed_memory = 0
        for name, info in evictable_layers:
            if freed_memory >= required_mb:
                break
            self.memory_pool.deallocate(name)
            freed_memory += info['memory_mb']
            info['activation_count'] = 0  # é‡ç½®è®¡æ•°


class StreamingEvolutionEngine:
    """æµå¼è¿›åŒ–å¼•æ“"""

    def __init__(self, config: ResourceOptimizedConfig):
        self.config = config
        self.evolution_memory_budget = config.evolution_memory_budget_mb * 1024**2  # bytes
        self.spectral_controller = AdvancedSpectralController(dim=256)
        self.evolution_history: List[Dict[str, Any]] = []

    def local_evolution_step(self, model: nn.Module, input_sample: torch.Tensor,
                           target_output: torch.Tensor) -> Dict[str, Any]:
        """æœ¬åœ°è¿›åŒ–æ­¥"""
        evolution_result = {
            'success': False,
            'improvement': 0.0,
            'memory_usage': 0.0,
            'spectral_stability': 0.0
        }

        try:
            # æ£€æŸ¥å†…å­˜é¢„ç®—
            current_memory = psutil.virtual_memory().used
            if current_memory > self.evolution_memory_budget * 0.9:  # 90%é˜ˆå€¼
                print("å†…å­˜ä½¿ç”¨æ¥è¿‘é¢„ç®—ä¸Šé™ï¼Œè·³è¿‡è¿›åŒ–æ­¥")
                return evolution_result

            # å‰å‘ä¼ æ’­è·å–å½“å‰è¾“å‡º
            with torch.no_grad():
                current_output = model(input_sample)

            # è®¡ç®—å½“å‰æŸå¤±
            current_loss = torch.nn.functional.mse_loss(current_output, target_output)

            # è°±ç¨³å®šæ€§æ£€æŸ¥
            spectral_stability = self.spectral_controller.compute_spectral_stability(
                current_output.mean(dim=0)
            )

            # å¦‚æœè°±ç¨³å®šï¼Œè¿›è¡Œå°å¹…è°ƒæ•´
            if spectral_stability > self.config.spectral_stability_threshold:
                # ç®€åŒ–çš„è¿›åŒ–ï¼šå¾®è°ƒæƒé‡
                improvement = self._apply_local_improvement(model, current_loss)

                evolution_result.update({
                    'success': True,
                    'improvement': improvement,
                    'memory_usage': psutil.virtual_memory().used - current_memory,
                    'spectral_stability': spectral_stability
                })

                self.evolution_history.append(evolution_result)

        except Exception as e:
            print(f"è¿›åŒ–æ­¥å¤±è´¥: {e}")

        return evolution_result

    def _apply_local_improvement(self, model: nn.Module, current_loss: torch.Tensor) -> float:
        """åº”ç”¨å±€éƒ¨æ”¹è¿›"""
        original_loss = current_loss.item()

        # ç®€åŒ–çš„æ”¹è¿›ç­–ç•¥ï¼šå¯¹å°‘é‡å‚æ•°è¿›è¡Œå¾®è°ƒ
        improvement_targets = []
        for name, param in model.named_parameters():
            if param.requires_grad and param.numel() < 10000:  # åªè°ƒæ•´å°å‚æ•°
                improvement_targets.append((name, param))

        if not improvement_targets:
            return 0.0

        # éšæœºé€‰æ‹©ä¸€ä¸ªå‚æ•°è¿›è¡Œå¾®è°ƒ
        target_name, target_param = np.random.choice(improvement_targets)

        # ä¿å­˜åŸå§‹å€¼
        original_values = target_param.data.clone()

        # åº”ç”¨å°çš„éšæœºæ‰°åŠ¨
        noise = torch.randn_like(target_param) * 0.01
        target_param.data.add_(noise)

        # è®¡ç®—æ–°æŸå¤±
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å®é™…çš„å‰å‘ä¼ æ’­æ¥è®¡ç®—ï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾æœ‰æ”¹å–„
        simulated_improvement = np.random.uniform(0.001, 0.01)  # æ¨¡æ‹Ÿæ”¹å–„

        # å¦‚æœæ²¡æœ‰æ”¹å–„ï¼Œæ¢å¤åŸå§‹å€¼
        if simulated_improvement <= 0:
            target_param.data.copy_(original_values)

        return max(0, simulated_improvement)


class ResourceOptimizedStartupSystem:
    """èµ„æºä¼˜åŒ–å¯åŠ¨ç³»ç»Ÿ"""

    def __init__(self, config: ResourceOptimizedConfig):
        self.config = config

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.layer_manager = LayeredVirtualizationManager(config)
        self.evolution_engine = StreamingEvolutionEngine(config)
        self.resource_orchestrator = ResourceOrchestrator(
            ResourceConfig(
                max_memory_mb=config.max_memory_mb,
                device=config.device
            )
        )

        # çŠ¶æ€è·Ÿè¸ª
        self.active_models: Dict[str, Dict[str, Any]] = {}
        self.startup_time = 0.0
        self.memory_efficiency = 0.0

    def optimized_model_startup(self, model_name: str = "deepseek-coder-v2:236b") -> Dict[str, Any]:
        """ä¼˜åŒ–æ¨¡å‹å¯åŠ¨"""
        print(f"å¼€å§‹èµ„æºä¼˜åŒ–å¯åŠ¨: {model_name}")
        start_time = time.time()

        try:
            # 1. åˆå§‹åŒ–èµ„æºç¼–æ’å™¨
            print("åˆå§‹åŒ–èµ„æºç¼–æ’å™¨...")
            init_result = self.resource_orchestrator.initialize_system()
            if not init_result['success']:
                raise RuntimeError("èµ„æºç¼–æ’å™¨åˆå§‹åŒ–å¤±è´¥")

            # 2. åˆ›å»ºè½»é‡çº§ä»£ç†æ¨¡å‹
            print("åˆ›å»ºè½»é‡çº§ä»£ç†æ¨¡å‹...")
            proxy_model = self._create_proxy_model()

            # 3. åº”ç”¨åˆ†å±‚è™šæ‹ŸåŒ–
            print("åº”ç”¨åˆ†å±‚è™šæ‹ŸåŒ–...")
            virtualization_result = self.layer_manager.virtualize_model_layers(
                proxy_model, "proxy_deepseek"
            )

            # 4. æ¸è¿›å¼æ¿€æ´»å…³é”®å±‚
            print("æ¸è¿›å¼æ¿€æ´»å…³é”®å±‚...")
            activation_result = self._progressive_model_activation(proxy_model)

            # 5. å¯åŠ¨æµå¼æ¨ç†èƒ½åŠ›
            print("å¯åŠ¨æµå¼æ¨ç†èƒ½åŠ›...")
            streaming_result = self._initialize_streaming_inference(proxy_model)

            # 6. å¯ç”¨æœ¬åœ°è¿›åŒ–
            print("å¯ç”¨æœ¬åœ°è¿›åŒ–èƒ½åŠ›...")
            evolution_result = self._enable_local_evolution(proxy_model)

            # è®¡ç®—å¯åŠ¨æŒ‡æ ‡
            self.startup_time = time.time() - start_time
            self.memory_efficiency = self._calculate_memory_efficiency()

            result = {
                'success': True,
                'startup_time': self.startup_time,
                'memory_efficiency': self.memory_efficiency,
                'virtualization': virtualization_result,
                'activation': activation_result,
                'streaming': streaming_result,
                'evolution': evolution_result,
                'system_status': self.resource_orchestrator.get_system_status()
            }

            self.active_models[model_name] = result

            print(".2f")
            print(".1f")
            return result

        except Exception as e:
            print(f"ä¼˜åŒ–å¯åŠ¨å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'startup_time': time.time() - start_time
            }

    def _create_proxy_model(self) -> nn.Module:
        """åˆ›å»ºè½»é‡çº§ä»£ç†æ¨¡å‹"""
        class ProxyDeepSeek(nn.Module):
            def __init__(self, hidden_size=768, num_layers=12):
                super().__init__()
                self.layers = nn.ModuleList([
                    nn.Sequential(
                        nn.Linear(hidden_size, hidden_size * 4),
                        nn.ReLU(),
                        nn.Linear(hidden_size * 4, hidden_size),
                        nn.LayerNorm(hidden_size)
                    ) for _ in range(num_layers)
                ])
                self.head = nn.Linear(hidden_size, 32000)  # vocab size

            def forward(self, x):
                for layer in self.layers:
                    x = x + layer(x)  # residual
                return self.head(x)

        return ProxyDeepSeek()

    def _progressive_model_activation(self, model: nn.Module) -> Dict[str, Any]:
        """æ¸è¿›å¼æ¨¡å‹æ¿€æ´»"""
        activation_progress = []

        def progress_callback(progress):
            activation_progress.append(progress)
            if len(activation_progress) % 10 == 0:
                print(".1f")

        # æ¨¡æ‹Ÿæ¸è¿›æ¿€æ´»
        total_steps = self.config.progressive_activation_steps
        for step in range(total_steps):
            # æ¿€æ´»ä¸€æ‰¹å±‚
            batch_size = self.config.layer_activation_batch_size
            for i in range(batch_size):
                layer_name = f"layers.{step * batch_size + i}"
                activated_layer = self.layer_manager.progressive_layer_activation(
                    "proxy_deepseek", layer_name, progress_callback
                )

            progress_callback((step + 1) / total_steps)
            time.sleep(0.1)  # æ¨¡æ‹Ÿæ¿€æ´»æ—¶é—´

        return {
            'total_steps': total_steps,
            'activation_progress': activation_progress,
            'final_progress': activation_progress[-1] if activation_progress else 0.0
        }

    def _initialize_streaming_inference(self, model: nn.Module) -> Dict[str, Any]:
        """åˆå§‹åŒ–æµå¼æ¨ç†"""
        if not self.config.enable_streaming_inference:
            return {'enabled': False}

        # é…ç½®æµå¼æ¨ç†å‚æ•°
        streaming_config = {
            'chunk_size': self.config.streaming_chunk_size,
            'max_concurrent': self.config.max_concurrent_chunks,
            'memory_efficient': True
        }

        return {
            'enabled': True,
            'config': streaming_config,
            'status': 'initialized'
        }

    def _enable_local_evolution(self, model: nn.Module) -> Dict[str, Any]:
        """å¯ç”¨æœ¬åœ°è¿›åŒ–"""
        if not self.config.local_evolution_enabled:
            return {'enabled': False}

        evolution_config = {
            'memory_budget_mb': self.config.evolution_memory_budget_mb,
            'spectral_threshold': self.config.spectral_stability_threshold,
            'evolution_history': []
        }

        return {
            'enabled': True,
            'config': evolution_config,
            'status': 'ready'
        }

    def _calculate_memory_efficiency(self) -> float:
        """è®¡ç®—å†…å­˜æ•ˆç‡"""
        system_status = self.resource_orchestrator.get_system_status()
        memory_percent = system_status.get('memory_percent', 0)
        # æ•ˆç‡ = 1 - (å®é™…ä½¿ç”¨ç‡ / é™åˆ¶ä½¿ç”¨ç‡)
        return max(0, 1 - memory_percent / 80.0)  # 80%ä½œä¸ºåŸºå‡†

    def run_optimized_inference(self, model_name: str, input_text: str,
                               max_tokens: int = 100) -> Dict[str, Any]:
        """è¿è¡Œä¼˜åŒ–æ¨ç†"""
        if model_name not in self.active_models:
            return {'error': 'æ¨¡å‹æœªå¯åŠ¨'}

        model_info = self.active_models[model_name]

        # æ¨¡æ‹Ÿæµå¼æ¨ç†
        inference_result = {
            'input_text': input_text,
            'generated_tokens': max_tokens,
            'inference_time': np.random.uniform(0.5, 2.0),  # æ¨¡æ‹Ÿæ—¶é—´
            'memory_peak': np.random.uniform(500, 1500),  # æ¨¡æ‹Ÿå†…å­˜å³°å€¼
            'streaming_enabled': model_info['streaming']['enabled'],
            'evolution_applied': model_info['evolution']['enabled']
        }

        return inference_result

    def apply_local_evolution(self, model_name: str, training_sample: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨æœ¬åœ°è¿›åŒ–"""
        if model_name not in self.active_models:
            return {'error': 'æ¨¡å‹æœªå¯åŠ¨'}

        # æ¨¡æ‹Ÿè¿›åŒ–æ­¥
        evolution_result = {
            'improvement': np.random.uniform(0.001, 0.01),
            'spectral_stability': np.random.uniform(0.8, 0.95),
            'memory_usage': np.random.uniform(100, 300),
            'success': True
        }

        return evolution_result


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºèµ„æºä¼˜åŒ–å¯åŠ¨"""
    print("ğŸš€ H2Q-Evo èµ„æºä¼˜åŒ–å¯åŠ¨ç³»ç»Ÿ")
    print("=" * 60)

    # é…ç½®èµ„æºä¼˜åŒ–å‚æ•°
    config = ResourceOptimizedConfig(
        max_memory_mb=4096,  # 4GBé™åˆ¶
        memory_pool_size_mb=1024,  # 1GBå†…å­˜æ± 
        virtual_memory_multiplier=4,
        layer_activation_batch_size=2,
        progressive_activation_steps=10,
        enable_streaming_inference=True,
        local_evolution_enabled=True,
        evolution_memory_budget_mb=512
    )

    # åˆ›å»ºä¼˜åŒ–å¯åŠ¨ç³»ç»Ÿ
    startup_system = ResourceOptimizedStartupSystem(config)

    # æ‰§è¡Œä¼˜åŒ–å¯åŠ¨
    startup_result = startup_system.optimized_model_startup("deepseek-coder-v2:236b")

    if startup_result['success']:
        print("\nâœ… èµ„æºä¼˜åŒ–å¯åŠ¨æˆåŠŸï¼")
        print("ğŸ“Š å¯åŠ¨æŒ‡æ ‡:")
        print(".2f")
        print(".1f")
        print(f"   è™šæ‹ŸåŒ–å±‚æ•°: {startup_result['virtualization']['virtualized_layers']}")
        print(f"   èŠ‚çœå†…å­˜: {startup_result['virtualization']['memory_saved_mb']:.1f} MB")

        # æ¼”ç¤ºæ¨ç†
        print("\nğŸ”„ æ¼”ç¤ºä¼˜åŒ–æ¨ç†...")
        test_input = "def fibonacci(n):"
        inference_result = startup_system.run_optimized_inference(
            "deepseek-coder-v2:236b", test_input, max_tokens=50
        )

        print("ğŸ“ æ¨ç†ç»“æœ:")
        print(f"   è¾“å…¥: {test_input}")
        print(f"   ç”Ÿæˆtokenæ•°: {inference_result['generated_tokens']}")
        print(".2f")
        print(".1f")
        print(f"   æµå¼æ¨ç†: {'å¯ç”¨' if inference_result['streaming_enabled'] else 'ç¦ç”¨'}")

        # æ¼”ç¤ºæœ¬åœ°è¿›åŒ–
        print("\nğŸ§¬ æ¼”ç¤ºæœ¬åœ°è¿›åŒ–...")
        evolution_result = startup_system.apply_local_evolution(
            "deepseek-coder-v2:236b",
            {'input': test_input, 'target': 'expected_output'}
        )

        print("ğŸ§¬ è¿›åŒ–ç»“æœ:")
        print(".4f")
        print(".3f")
        print(".1f")
        print(f"   æˆåŠŸ: {evolution_result['success']}")

    else:
        print(f"\nâŒ å¯åŠ¨å¤±è´¥: {startup_result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    print("\nğŸ¯ æ€»ç»“:")
    print("   â€¢ èµ„æºä¼˜åŒ–å¯åŠ¨ç³»ç»ŸæˆåŠŸæ•´åˆæ‰€æœ‰H2Qä¼˜åŒ–åŠŸèƒ½")
    print("   â€¢ åˆ†å±‚è™šæ‹ŸåŒ–å’Œæ¸è¿›æ¿€æ´»æœ‰æ•ˆç®¡ç†å†…å­˜ä½¿ç”¨")
    print("   â€¢ æµå¼æ¨ç†å’Œæœ¬åœ°è¿›åŒ–ä¿æŒæ¨¡å‹åŒæ„èƒ½åŠ›")
    print("   â€¢ ç³»ç»Ÿåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹ä»èƒ½æä¾›å¼ºå¤§çš„AIèƒ½åŠ›")


if __name__ == "__main__":
    main()