#!/usr/bin/env python3
"""
H2Q-Evo æ•°å­¦æ ¸å¿ƒæ¶æ„çœŸå®æ€§éªŒè¯

éªŒè¯æˆ‘ä»¬çš„æ•°å­¦æ ¸å¿ƒæ¶æ„æ˜¯å¦çœŸå®è¿è¡Œï¼Œå¹¶å°è¯•å°†DeepSeekæ¨¡å‹é›†æˆ
"""

import torch
import torch.nn as nn
import json
import time
import requests
from typing import Dict, Any
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.append('/Users/imymm/H2Q-Evo')
sys.path.append('/Users/imymm/H2Q-Evo/h2q_project/src')

def test_mathematical_core():
    """æµ‹è¯•æ•°å­¦æ ¸å¿ƒæ¶æ„"""
    print("ğŸ”¬ æµ‹è¯•H2Qæ•°å­¦æ ¸å¿ƒæ¶æ„")
    print("=" * 50)

    try:
        from h2q_project.src.h2q.core.unified_architecture import (
            UnifiedH2QMathematicalArchitecture,
            UnifiedMathematicalArchitectureConfig
        )

        print("âœ… å¯¼å…¥æ•°å­¦æ¶æ„æˆåŠŸ")

        # åˆ›å»ºé…ç½®
        config = UnifiedMathematicalArchitectureConfig(
            dim=128,
            action_dim=32,
            device="cpu"  # ä½¿ç”¨CPUé¿å…MPSé—®é¢˜
        )

        # åˆå§‹åŒ–æ¶æ„
        start_time = time.time()
        math_core = UnifiedH2QMathematicalArchitecture(config)
        init_time = time.time() - start_time

        print(f"âœ… æ•°å­¦æ¶æ„åˆå§‹åŒ–æˆåŠŸ: {init_time:.3f} ç§’")
        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        batch_size, seq_len = 2, 10
        dummy_input = torch.randn(batch_size, seq_len, config.dim)

        start_time = time.time()
        output = math_core(dummy_input)
        forward_time = time.time() - start_time

        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ: è¾“å…¥{batch_size}x{seq_len}x{config.dim} -> è¾“å‡º{output.shape}")
        print(f"âœ… å‰å‘ä¼ æ’­è€—æ—¶: {forward_time:.3f} ç§’")
        return {
            "success": True,
            "init_time": init_time,
            "forward_time": forward_time,
            "output_shape": output.shape
        }

    except Exception as e:
        print(f"âŒ æ•°å­¦æ ¸å¿ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}


def test_streaming_middleware():
    """æµ‹è¯•æµå¼æ¨ç†ä¸­é—´ä»¶"""
    print("\nğŸŒŠ æµ‹è¯•æµå¼æ¨ç†ä¸­é—´ä»¶")
    print("=" * 50)

    try:
        from h2q_project.src.h2q.core.guards.holomorphic_streaming_middleware import HolomorphicStreamingMiddleware

        print("âœ… å¯¼å…¥æµå¼ä¸­é—´ä»¶æˆåŠŸ")

        # åˆ›å»ºä¸­é—´ä»¶
        middleware = HolomorphicStreamingMiddleware(threshold=0.1, max_history=8)

        # æµ‹è¯•å››å…ƒæ•°çŠ¶æ€å¤„ç†
        q_state = torch.randn(4)  # å››å…ƒæ•°çŠ¶æ€

        start_time = time.time()
        curvature = middleware.calculate_fueter_laplace(q_state)
        curvature_time = time.time() - start_time

        print(f"âœ… æ›²ç‡è®¡ç®—æˆåŠŸ: {curvature.item():.6f}")
        print(f"âœ… æ›²ç‡è®¡ç®—è€—æ—¶: {curvature_time:.3f} ç§’")
        return {
            "success": True,
            "curvature": curvature.item(),
            "computation_time": curvature_time
        }

    except Exception as e:
        print(f"âŒ æµå¼ä¸­é—´ä»¶æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}


def test_model_integration():
    """æµ‹è¯•æ¨¡å‹é›†æˆèƒ½åŠ›"""
    print("\nğŸ”— æµ‹è¯•æ¨¡å‹é›†æˆèƒ½åŠ›")
    print("=" * 50)

    try:
        from model_crystallization_engine import ModelCrystallizationEngine, CrystallizationConfig

        # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„DeepSeeké£æ ¼çš„æ¨¡å‹
        class MockDeepSeekModel(nn.Module):
            def __init__(self, vocab_size=32000, hidden_size=1024, num_layers=12):
                super().__init__()
                self.embeddings = nn.Embedding(vocab_size, hidden_size)
                self.layers = nn.ModuleList([
                    nn.TransformerDecoderLayer(
                        d_model=hidden_size, nhead=16, dim_feedforward=4096,
                        batch_first=True
                    ) for _ in range(num_layers)
                ])
                self.ln_f = nn.LayerNorm(hidden_size)
                self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)

            def forward(self, input_ids):
                x = self.embeddings(input_ids)
                for layer in self.layers:
                    x = layer(x, x)  # è‡ªæ³¨æ„åŠ›
                x = self.ln_f(x)
                logits = self.lm_head(x)
                return logits

        # åˆ›å»ºå°å‹æ¨¡å‹ç”¨äºæµ‹è¯•
        model = MockDeepSeekModel(vocab_size=1000, hidden_size=256, num_layers=4)
        original_params = sum(p.numel() for p in model.parameters())

        print(f"âœ… åˆ›å»ºæ¨¡æ‹ŸDeepSeekæ¨¡å‹: {original_params:,} å‚æ•°")

        # é…ç½®ç»“æ™¶åŒ–
        config = CrystallizationConfig(
            target_compression_ratio=8.0,
            max_memory_mb=1024,
            device="cpu"
        )

        engine = ModelCrystallizationEngine(config)

        # æ‰§è¡Œç»“æ™¶åŒ–
        start_time = time.time()
        report = engine.crystallize_model(model, "mock_deepseek_integration")
        crystallization_time = time.time() - start_time

        print("âœ… æ¨¡å‹ç»“æ™¶åŒ–æˆåŠŸ!")
        print(f"   å‹ç¼©æ¯”: {report.get('compression_ratio', 1.0):.1f}x")
        print(f"   è´¨é‡åˆ†æ•°: {report.get('quality_score', 0.0):.3f}")
        print(f"   ç»“æ™¶åŒ–æ—¶é—´: {crystallization_time:.2f} ç§’")
        return {
            "success": True,
            "original_params": original_params,
            "compression_ratio": report.get('compression_ratio', 1.0),
            "crystallization_time": crystallization_time
        }

    except Exception as e:
        print(f"âŒ æ¨¡å‹é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}


def test_streaming_with_ollama():
    """æµ‹è¯•ä¸Ollamaçš„æµå¼æ¨ç†"""
    print("\nğŸ“¡ æµ‹è¯•Ollamaæµå¼æ¨ç†")
    print("=" * 50)

    try:
        # æµ‹è¯•æµå¼API
        payload = {
            "model": "deepseek-coder:6.7b",
            "prompt": "Write a Python function to sort a list",
            "stream": True,  # å¯ç”¨æµå¼
            "options": {
                "num_predict": 100,
                "temperature": 0.1
            }
        }

        print("ğŸš€ å‘é€æµå¼æ¨ç†è¯·æ±‚...")

        start_time = time.time()
        response = requests.post(
            "http://localhost:11434/api/generate",
            json=payload,
            stream=True,
            timeout=120
        )

        if response.status_code == 200:
            print("âœ… æµå¼å“åº”å¼€å§‹æ¥æ”¶")

            total_content = ""
            chunk_count = 0
            first_chunk_time = None

            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        try:
                            data = json.loads(line[6:])
                            if 'response' in data:
                                chunk_count += 1
                                if first_chunk_time is None:
                                    first_chunk_time = time.time() - start_time
                                total_content += data['response']
                                if chunk_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªchunk
                                    print(f"   Chunk {chunk_count}: {data['response'][:50]}...")
                            if data.get('done', False):
                                break
                        except json.JSONDecodeError:
                            continue

            total_time = time.time() - start_time

            print("âœ… æµå¼æ¨ç†å®Œæˆ!")
            print(f"   æ€»æ¥æ”¶å—æ•°: {chunk_count}")
            print(f"   æ€»æ—¶é—´: {total_time:.3f} ç§’")
            print(f"   é¦–å—æ—¶é—´: {first_chunk_time:.3f} ç§’")
            print(f"   æ€»å†…å®¹é•¿åº¦: {len(total_content)} å­—ç¬¦")

            return {
                "success": True,
                "total_time": total_time,
                "first_chunk_time": first_chunk_time,
                "chunk_count": chunk_count,
                "content_length": len(total_content)
            }
        else:
            print(f"âŒ æµå¼è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
            return {"success": False, "error": f"HTTP {response.status_code}"}

    except Exception as e:
        print(f"âŒ æµå¼æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


def analyze_236b_model_issue():
    """åˆ†æ236Bæ¨¡å‹é—®é¢˜"""
    print("\nğŸ” åˆ†æ236Bæ¨¡å‹é—®é¢˜")
    print("=" * 50)

    try:
        # æ£€æŸ¥æ¨¡å‹ä¿¡æ¯
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        if response.status_code == 200:
            data = response.json()
            for model in data.get('models', []):
                if '236b' in model['name']:
                    print(f"ğŸ“Š 236Bæ¨¡å‹ä¿¡æ¯:")
                    print(f"   åç§°: {model['name']}")
                    print(f"   å¤§å°: {model['size'] / (1024**3):.1f} GB")
                    print(f"   ä¿®æ”¹æ—¶é—´: {model['modified_at']}")

                    # åˆ†æå¯èƒ½çš„å†…å­˜é—®é¢˜
                    import psutil
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / (1024**3)

                    print("\nğŸ’¾ ç³»ç»Ÿå†…å­˜åˆ†æ:")
                    print(f"   æ€»å†…å­˜: {memory.total / (1024**3):.1f} GB")
                    print(f"   å¯ç”¨å†…å­˜: {available_gb:.1f} GB")
                    print(f"   ä½¿ç”¨ç‡: {memory.percent:.1f}%")
                    if available_gb < 8:
                        print("   âš ï¸ å¯ç”¨å†…å­˜ä¸è¶³ï¼Œ236Bæ¨¡å‹éœ€è¦å¤§é‡å†…å­˜")
                        print("   ğŸ’¡ å»ºè®®: å¢åŠ ç³»ç»Ÿå†…å­˜æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
                    else:
                        print("   âœ… ç³»ç»Ÿå†…å­˜å……è¶³")

                    # åˆ†æå¯èƒ½çš„æµå¼é—®é¢˜
                    print("\nğŸŒŠ æµå¼æ¨ç†åˆ†æ:")
                    print("   â€¢ 236Bæ¨¡å‹å‚æ•°é‡æå¤§ (~2360äº¿)")
                    print("   â€¢ é¦–æ¬¡åŠ è½½éœ€è¦é•¿æ—¶é—´é¢„çƒ­")
                    print("   â€¢ å†…å­˜å ç”¨å¯èƒ½è¶…è¿‡16GB")
                    print("   â€¢ å»ºè®®ä½¿ç”¨åˆ†å—åŠ è½½æˆ–å†…å­˜æ˜ å°„")
                    break

        return {"analysis_complete": True}

    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ†æå¤±è´¥: {e}")
        return {"analysis_complete": False, "error": str(e)}


def audit_achievements():
    """å®¡è®¡ç°æœ‰æˆæœä»·å€¼"""
    print("\nğŸ“‹ å®¡è®¡ç°æœ‰æˆæœä»·å€¼")
    print("=" * 50)

    achievements = {
        "mathematical_core": False,
        "streaming_middleware": False,
        "model_integration": False,
        "crystallization_technology": False,
        "real_model_testing": False
    }

    # æ£€æŸ¥æ•°å­¦æ ¸å¿ƒ
    try:
        from h2q_project.src.h2q.core.unified_architecture import UnifiedH2QMathematicalArchitecture
        achievements["mathematical_core"] = True
        print("âœ… æ•°å­¦æ ¸å¿ƒæ¶æ„: å­˜åœ¨ä¸”å¯å¯¼å…¥")
    except ImportError:
        print("âŒ æ•°å­¦æ ¸å¿ƒæ¶æ„: å¯¼å…¥å¤±è´¥")

    # æ£€æŸ¥æµå¼ä¸­é—´ä»¶
    try:
        from h2q_project.src.h2q.core.guards.holomorphic_streaming_middleware import HolomorphicStreamingMiddleware
        achievements["streaming_middleware"] = True
        print("âœ… æµå¼æ¨ç†ä¸­é—´ä»¶: å­˜åœ¨ä¸”å¯å¯¼å…¥")
    except ImportError:
        print("âŒ æµå¼æ¨ç†ä¸­é—´ä»¶: å¯¼å…¥å¤±è´¥")

    # æ£€æŸ¥æ¨¡å‹é›†æˆ
    try:
        from model_crystallization_engine import ModelCrystallizationEngine
        achievements["model_integration"] = True
        print("âœ… æ¨¡å‹é›†æˆå¼•æ“: å­˜åœ¨ä¸”å¯å¯¼å…¥")
    except ImportError:
        print("âŒ æ¨¡å‹é›†æˆå¼•æ“: å¯¼å…¥å¤±è´¥")

    # æ£€æŸ¥ç»“æ™¶åŒ–æŠ€æœ¯
    if os.path.exists('/Users/imymm/H2Q-Evo/real_deepseek_benchmark_results.json'):
        achievements["crystallization_technology"] = True
        print("âœ… ç»“æ™¶åŒ–æŠ€æœ¯: å·²æœ‰å®é™…æµ‹è¯•ç»“æœ")
    else:
        print("âŒ ç»“æ™¶åŒ–æŠ€æœ¯: æ— å®é™…æµ‹è¯•ç»“æœ")

    # æ£€æŸ¥çœŸå®æ¨¡å‹æµ‹è¯•
    try:
        with open('/Users/imymm/H2Q-Evo/real_deepseek_benchmark_results.json', 'r') as f:
            data = json.load(f)
            if data.get('model') == 'deepseek-coder:6.7b':
                achievements["real_model_testing"] = True
                print("âœ… çœŸå®æ¨¡å‹æµ‹è¯•: å·²éªŒè¯DeepSeekæ¨¡å‹")
            else:
                print("âŒ çœŸå®æ¨¡å‹æµ‹è¯•: æµ‹è¯•æ•°æ®æ— æ•ˆ")
    except:
        print("âŒ çœŸå®æ¨¡å‹æµ‹è¯•: æ— æµ‹è¯•æ•°æ®")

    # è®¡ç®—ä»·å€¼åˆ†æ•°
    value_score = sum(achievements.values()) / len(achievements) * 100

    print("\nğŸ“Š æˆæœä»·å€¼è¯„ä¼°:")
    print(f"   ä»·å€¼åˆ†æ•°: {value_score:.1f}%")
    print(f"   å®Œæˆé¡¹ç›®: {sum(achievements.values())}/{len(achievements)}")

    if value_score >= 80:
        print("ğŸ¯ ç»“è®º: ç°æœ‰æˆæœå…·æœ‰æ˜¾è‘—ä»·å€¼ï¼Œå·²å®ç°æ ¸å¿ƒæŠ€æœ¯çªç ´")
    elif value_score >= 60:
        print("ğŸ¯ ç»“è®º: ç°æœ‰æˆæœå…·æœ‰ä¸€å®šä»·å€¼ï¼Œéœ€è¦è¿›ä¸€æ­¥å®Œå–„")
    else:
        print("ğŸ¯ ç»“è®º: ç°æœ‰æˆæœä»·å€¼æœ‰é™ï¼Œéœ€è¦é‡æ–°è¯„ä¼°æ–¹å‘")

    return achievements


def run_comprehensive_audit():
    """è¿è¡Œç»¼åˆå®¡è®¡"""
    print("ğŸš€ H2Q-Evo ç»¼åˆçœŸå®æ€§å®¡è®¡")
    print("=" * 60)

    results = {
        "timestamp": time.time(),
        "tests": {},
        "analysis": {},
        "achievements": {}
    }

    # 1. æµ‹è¯•æ•°å­¦æ ¸å¿ƒ
    results["tests"]["mathematical_core"] = test_mathematical_core()

    # 2. æµ‹è¯•æµå¼ä¸­é—´ä»¶
    results["tests"]["streaming_middleware"] = test_streaming_middleware()

    # 3. æµ‹è¯•æ¨¡å‹é›†æˆ
    results["tests"]["model_integration"] = test_model_integration()

    # 4. æµ‹è¯•æµå¼æ¨ç†
    results["tests"]["streaming_inference"] = test_streaming_with_ollama()

    # 5. åˆ†æ236Bæ¨¡å‹é—®é¢˜
    results["analysis"]["236b_model_issue"] = analyze_236b_model_issue()

    # 6. å®¡è®¡æˆæœä»·å€¼
    results["achievements"] = audit_achievements()

    # ä¿å­˜å®Œæ•´å®¡è®¡ç»“æœ
    with open("comprehensive_audit_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print("\nğŸ“„ å®Œæ•´å®¡è®¡ç»“æœå·²ä¿å­˜: comprehensive_audit_results.json")
    return results


if __name__ == "__main__":
    run_comprehensive_audit()