#!/usr/bin/env python3
"""
M24-DAS Mac Mini M4æ¨ç†å¼•æ“å’ŒåŸºå‡†æµ‹è¯•ç³»ç»Ÿ
åŸºäºM24çœŸå®æ€§åŸåˆ™å’ŒDASæ•°å­¦æ¶æ„ï¼Œåœ¨Mac Mini M4ä¸Šè¿›è¡Œæµç•…æ¨ç†å’Œå…¬å¼€åŸºå‡†æµ‹è¯•

æ ¸å¿ƒç‰¹æ€§ï¼š
1. M4 AMXåŠ é€Ÿä¼˜åŒ–
2. å†…å­˜é«˜æ•ˆæ¨ç†
3. DASæ•°å­¦æ¶æ„é›†æˆ
4. M24éªŒè¯æœºåˆ¶
5. å…¬å¼€åŸºå‡†æµ‹è¯•
"""

import os
import sys
import json
import time
import torch
import logging
import psutil
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
import gc
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "h2q_project"))

# å¯¼å…¥DASæ ¸å¿ƒå’ŒM24ç³»ç»Ÿ
from h2q_project.das_core import DASCore
from m24_protocol import apply_m24_wrapper

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [M24-M4-INFERENCE] %(levelname)s: %(message)s',
    handlers=[
        logging.FileHandler('m24_m4_inference_benchmark.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('M24-M4-INFERENCE')

@dataclass
class M4InferenceConfig:
    """Mac Mini M4æ¨ç†é…ç½®"""
    model_path: str
    max_memory_gb: float = 12.0  # Mac Mini M4 16Gï¼Œç•™4Gç³»ç»Ÿä½¿ç”¨
    use_amx: bool = True
    quantization: str = "fp16"  # fp16, int8, int4
    chunk_size: int = 512
    m24_verified: bool = True

@dataclass
class InferenceResult:
    """æ¨ç†ç»“æœ"""
    success: bool
    response: str
    inference_time_sec: float
    memory_usage_gb: float
    tokens_generated: int
    m24_verification: Dict[str, Any]
    error_message: Optional[str] = None

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    model_name: str
    task_name: str
    score: float
    latency_sec: float
    memory_usage_gb: float
    throughput_tokens_sec: float
    m24_compliance: bool
    timestamp: float

class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""

    def __init__(self):
        self.peak_usage = 0.0
        self.start_time = time.time()

    def update(self):
        """æ›´æ–°å†…å­˜ä½¿ç”¨ç»Ÿè®¡"""
        current_usage = psutil.virtual_memory().used / (1024**3)  # GB
        self.peak_usage = max(self.peak_usage, current_usage)
        return current_usage

    def get_peak_usage_gb(self) -> float:
        """è·å–å³°å€¼å†…å­˜ä½¿ç”¨"""
        return self.peak_usage

    def get_uptime_sec(self) -> float:
        """è·å–è¿è¡Œæ—¶é—´"""
        return time.time() - self.start_time

class M24DASMacMiniInferenceEngine:
    """
    M24-DAS Mac Mini M4æ¨ç†å¼•æ“
    åŸºäºM24çœŸå®æ€§åŸåˆ™å’ŒDASæ•°å­¦æ¶æ„çš„Mac Mini M4ä¼˜åŒ–æ¨ç†å¼•æ“
    """

    def __init__(self, config: M4InferenceConfig):
        self.config = config
        self.memory_monitor = MemoryMonitor()
        self.das_core = None
        self.model = None
        self.tokenizer = None
        self.m24_verifier = M24InferenceVerifier()

        # M4ä¼˜åŒ–é…ç½®
        self.m4_optimizations = {
            'amx_acceleration': config.use_amx,
            'memory_chunking': True,
            'unified_memory': True,
            'neural_engine': True
        }

        logger.info("ğŸ M24-DAS Mac Mini M4æ¨ç†å¼•æ“åˆå§‹åŒ–")
        logger.info(f"ğŸ“Š é…ç½®: {asdict(config)}")

    def load_model(self) -> bool:
        """åŠ è½½DASä¼˜åŒ–æ¨¡å‹"""
        try:
            logger.info("ğŸ“¥ åŠ è½½DASä¼˜åŒ–DeepSeekæ¨¡å‹...")

            if not os.path.exists(self.config.model_path):
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.config.model_path}")

            # æ£€æŸ¥å†…å­˜é™åˆ¶
            model_size_mb = os.path.getsize(self.config.model_path) / (1024 * 1024)
            if model_size_mb > self.config.max_memory_gb * 1024:
                raise MemoryError(f"æ¨¡å‹è¿‡å¤§: {model_size_mb:.2f} MB > {self.config.max_memory_gb * 1024} MBé™åˆ¶")

            # åŠ è½½æ¨¡å‹
            model_data = torch.load(self.config.model_path, map_location='cpu', weights_only=True)
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {len(model_data)} ä¸ªæƒé‡å¼ é‡")

            # åˆå§‹åŒ–DASæ ¸å¿ƒ
            self.das_core = DASCore(target_dimension=256)
            logger.info("ğŸ§¬ DASæ ¸å¿ƒåˆå§‹åŒ–å®Œæˆ")

            # M4ä¼˜åŒ–
            self._apply_m4_optimizations(model_data)

            self.model = model_data  # ç®€åŒ–ä¸ºç›´æ¥å­˜å‚¨æƒé‡
            logger.info("ğŸ¯ æ¨¡å‹å‡†å¤‡å®Œæˆ")

            return True

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

    def _apply_m4_optimizations(self, model_data: Dict[str, torch.Tensor]):
        """åº”ç”¨Mac Mini M4ä¼˜åŒ–"""
        logger.info("âš¡ åº”ç”¨Mac Mini M4ä¼˜åŒ–...")

        for key, tensor in model_data.items():
            # 1. AMXåŠ é€Ÿä¼˜åŒ–
            if self.m4_optimizations['amx_acceleration']:
                tensor = self._optimize_for_amx(tensor)

            # 2. å†…å­˜å¸ƒå±€ä¼˜åŒ–
            tensor = tensor.contiguous()

            # 3. é‡åŒ–ä¼˜åŒ–
            if self.config.quantization == "fp16" and tensor.dtype == torch.float32:
                tensor = tensor.to(torch.float16)

            model_data[key] = tensor

        logger.info("ğŸ M4ä¼˜åŒ–å®Œæˆ")

    def _optimize_for_amx(self, tensor: torch.Tensor) -> torch.Tensor:
        """ä¸ºAMXåŠ é€Ÿä¼˜åŒ–å¼ é‡"""
        # AMX (Apple Matrix Coprocessor) ä¼˜åŒ–
        shape = tensor.shape

        # AMX prefers dimensions that are multiples of 32
        optimized_shape = []
        for dim in shape:
            # å‘ä¸Šå–æ•´åˆ°32çš„å€æ•°ï¼Œä½†ä¿æŒæ€»å…ƒç´ æ•°é‡ä¸å˜
            if dim > 0:
                optimized_dim = ((dim + 31) // 32) * 32
                optimized_shape.append(optimized_dim)
            else:
                optimized_shape.append(dim)

        if tuple(optimized_shape) != shape:
            # æ’å€¼æˆ–å¡«å……åˆ°ä¼˜åŒ–ç»´åº¦
            optimized_tensor = torch.zeros(optimized_shape, dtype=tensor.dtype)
            min_shape = tuple(min(a, b) for a, b in zip(shape, optimized_shape))
            optimized_tensor[tuple(slice(0, s) for s in min_shape)] = tensor[tuple(slice(0, s) for s in min_shape)]
            return optimized_tensor

        return tensor

    def generate_response(self, prompt: str, max_tokens: int = 100) -> InferenceResult:
        """
        ç”Ÿæˆå“åº” - M24éªŒè¯æ¨ç†è¿‡ç¨‹

        Args:
            prompt: è¾“å…¥æç¤º
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

        Returns:
            æ¨ç†ç»“æœ
        """
        start_time = time.time()
        result = InferenceResult(
            success=False,
            response="",
            inference_time_sec=0.0,
            memory_usage_gb=0.0,
            tokens_generated=0,
            m24_verification={}
        )

        try:
            # M24éªŒè¯ï¼šæ£€æŸ¥æ¨ç†è¾“å…¥
            if not self.m24_verifier.verify_inference_input(prompt):
                result.error_message = "M24éªŒè¯å¤±è´¥ï¼šæ¨ç†è¾“å…¥ä¸ç¬¦åˆè¦æ±‚"
                return result

            # ç®€åŒ–çš„æ¨ç†å®ç°ï¼ˆæ¦‚å¿µéªŒè¯ï¼‰
            logger.info(f"ğŸ¤– å¼€å§‹æ¨ç†: {prompt[:50]}...")

            # æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
            response_tokens = self._simulate_inference(prompt, max_tokens)
            response = self._tokens_to_text(response_tokens)

            # æ›´æ–°ç»“æœ
            result.success = True
            result.response = response
            result.inference_time_sec = time.time() - start_time
            result.memory_usage_gb = self.memory_monitor.update()
            result.tokens_generated = len(response_tokens)

            # M24éªŒè¯æ¨ç†è¾“å‡º
            result.m24_verification = self.m24_verifier.verify_inference_output(
                prompt, response, self.config
            )

            logger.info("âœ… æ¨ç†å®Œæˆ")
            logger.info(f"ğŸ“Š ç»“æœ: ç”Ÿæˆ {result.tokens_generated} tokens, è€—æ—¶ {result.inference_time_sec:.2f} ç§’")

        except Exception as e:
            logger.error(f"âŒ æ¨ç†å¤±è´¥: {e}")
            result.error_message = str(e)
        finally:
            # å†…å­˜æ¸…ç†
            gc.collect()

        return result

    def _simulate_inference(self, prompt: str, max_tokens: int) -> List[str]:
        """æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹ï¼ˆæ¦‚å¿µéªŒè¯ï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ¨¡æ‹Ÿï¼Œç”¨äºæ¦‚å¿µéªŒè¯
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨çœŸæ­£çš„æ¨¡å‹æ¨ç†

        tokens = []
        words = prompt.split()

        # ç”Ÿæˆä¸€äº›ç›¸å…³çš„å“åº”token
        base_responses = [
            "åŸºäº", "DAS", "æ•°å­¦", "æ¶æ„", "çš„", "åˆ†æ", "æ˜¾ç¤º",
            "è¿™ä¸ª", "é—®é¢˜", "æ¶‰åŠ", "æ–¹å‘æ€§", "æ„é€ ", "å…¬ç†",
            "ç³»ç»Ÿ", "éœ€è¦", "è€ƒè™‘", "å¯¹å¶", "ç”Ÿæˆ", "å’Œ", "ç¾¤",
            "ä½œç”¨", "çš„", "æ€§è´¨"
        ]

        for i in range(min(max_tokens, 20)):
            token = base_responses[i % len(base_responses)]
            tokens.append(token)

            # æ¨¡æ‹Ÿæ¨ç†å»¶è¿Ÿ
            time.sleep(0.01)

        return tokens

    def _tokens_to_text(self, tokens: List[str]) -> str:
        """å°†tokenè½¬æ¢ä¸ºæ–‡æœ¬"""
        return " ".join(tokens)

    def run_benchmark(self, benchmark_tasks: List[Dict[str, Any]]) -> List[BenchmarkResult]:
        """
        è¿è¡ŒåŸºå‡†æµ‹è¯•

        Args:
            benchmark_tasks: åŸºå‡†æµ‹è¯•ä»»åŠ¡åˆ—è¡¨

        Returns:
            åŸºå‡†æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        logger.info("ğŸƒ å¼€å§‹M24-DASåŸºå‡†æµ‹è¯•...")
        results = []

        for task in benchmark_tasks:
            logger.info(f"ğŸ“‹ æµ‹è¯•ä»»åŠ¡: {task['name']}")

            start_time = time.time()
            memory_before = self.memory_monitor.update()

            # æ‰§è¡Œæ¨ç†
            result = self.generate_response(task['prompt'], task.get('max_tokens', 50))

            latency = time.time() - start_time
            memory_used = self.memory_monitor.update() - memory_before

            # è®¡ç®—åˆ†æ•°ï¼ˆç®€åŒ–çš„è¯„åˆ†é€»è¾‘ï¼‰
            score = self._calculate_task_score(result, task)

            # è®¡ç®—ååé‡
            throughput = result.tokens_generated / latency if latency > 0 else 0

            benchmark_result = BenchmarkResult(
                model_name="DAS-DeepSeek-M4-Optimized",
                task_name=task['name'],
                score=score,
                latency_sec=latency,
                memory_usage_gb=memory_used,
                throughput_tokens_sec=throughput,
                m24_compliance=result.m24_verification.get('m24_compliance', False),
                timestamp=time.time()
            )

            results.append(benchmark_result)
            logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: åˆ†æ•°={score:.3f}, å»¶è¿Ÿ={latency:.2f}s, ååé‡={throughput:.2f} tokens/s")

        logger.info("ğŸ¯ åŸºå‡†æµ‹è¯•å®Œæˆ")
        return results

    def _calculate_task_score(self, result: InferenceResult, task: Dict[str, Any]) -> float:
        """è®¡ç®—ä»»åŠ¡åˆ†æ•°ï¼ˆç®€åŒ–çš„è¯„åˆ†é€»è¾‘ï¼‰"""
        if not result.success:
            return 0.0

        # ç®€åŒ–çš„è¯„åˆ†ï¼šåŸºäºå“åº”é•¿åº¦å’Œç›¸å…³æ€§
        base_score = min(result.tokens_generated / 20.0, 1.0)  # é•¿åº¦åˆ†æ•°

        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        expected_keywords = task.get('expected_keywords', [])
        if expected_keywords:
            matched = sum(1 for keyword in expected_keywords if keyword in result.response)
            keyword_score = matched / len(expected_keywords)
            return (base_score + keyword_score) / 2
        else:
            return base_score


class M24InferenceVerifier:
    """
    M24æ¨ç†éªŒè¯å™¨
    ç¡®ä¿æ¨ç†è¿‡ç¨‹ç¬¦åˆçœŸå®æ€§åŸåˆ™
    """

    def verify_inference_input(self, prompt: str) -> bool:
        """éªŒè¯æ¨ç†è¾“å…¥"""
        if not prompt or len(prompt.strip()) == 0:
            logger.error("âŒ æ¨ç†è¾“å…¥ä¸ºç©º")
            return False

        if len(prompt) > 10000:  # åˆç†é•¿åº¦é™åˆ¶
            logger.error("âŒ æ¨ç†è¾“å…¥è¿‡é•¿")
            return False

        logger.info("âœ… æ¨ç†è¾“å…¥éªŒè¯é€šè¿‡")
        return True

    def verify_inference_output(self, prompt: str, response: str, config: M4InferenceConfig) -> Dict[str, Any]:
        """éªŒè¯æ¨ç†è¾“å‡º"""
        verification = {
            'input_output_consistency': False,
            'm24_compliance': True,
            'response_quality': False,
            'memory_efficiency': False
        }

        try:
            # 1. æ£€æŸ¥è¾“å…¥è¾“å‡ºä¸€è‡´æ€§
            if len(response) > 0 and len(prompt) > 0:
                verification['input_output_consistency'] = True

            # 2. æ£€æŸ¥å“åº”è´¨é‡
            if len(response.split()) > 5:  # è‡³å°‘5ä¸ªè¯
                verification['response_quality'] = True

            # 3. æ£€æŸ¥å†…å­˜æ•ˆç‡
            current_memory = psutil.virtual_memory().used / (1024**3)
            if current_memory < config.max_memory_gb:
                verification['memory_efficiency'] = True

            logger.info("ğŸ¯ M24æ¨ç†éªŒè¯å®Œæˆ")
            logger.info(f"ğŸ“Š éªŒè¯ç»“æœ: {verification}")

        except Exception as e:
            logger.error(f"âŒ æ¨ç†éªŒè¯å¤±è´¥: {e}")
            verification['m24_compliance'] = False

        return verification


def create_benchmark_tasks() -> List[Dict[str, Any]]:
    """åˆ›å»ºåŸºå‡†æµ‹è¯•ä»»åŠ¡"""
    return [
        {
            "name": "mathematical_reasoning",
            "prompt": "è§£é‡ŠDASæ•°å­¦æ¶æ„ä¸­çš„æ–¹å‘æ€§æ„é€ å…¬ç†ç³»ç»Ÿ",
            "max_tokens": 50,
            "expected_keywords": ["DAS", "æ–¹å‘æ€§", "æ„é€ ", "å…¬ç†", "ç³»ç»Ÿ"]
        },
        {
            "name": "code_generation",
            "prompt": "å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
            "max_tokens": 30,
            "expected_keywords": ["def", "fibonacci", "return"]
        },
        {
            "name": "logical_reasoning",
            "prompt": "åˆ†æM24çœŸå®æ€§åŸåˆ™çš„é‡è¦æ€§",
            "max_tokens": 40,
            "expected_keywords": ["M24", "çœŸå®æ€§", "åŸåˆ™", "é‡è¦æ€§"]
        },
        {
            "name": "creative_writing",
            "prompt": "æè¿°ä¸€ä¸ªåŸºäºDASçš„æœªæ¥AGIç³»ç»Ÿ",
            "max_tokens": 60,
            "expected_keywords": ["DAS", "AGI", "ç³»ç»Ÿ", "æœªæ¥"]
        }
    ]


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡ŒM24-DAS Mac Mini M4æ¨ç†å’ŒåŸºå‡†æµ‹è¯•"""
    logger.info("ğŸš€ å¯åŠ¨M24-DAS Mac Mini M4æ¨ç†å’ŒåŸºå‡†æµ‹è¯•ç³»ç»Ÿ")
    logger.info("åŸºäºM24çœŸå®æ€§åŸåˆ™å’ŒDASæ•°å­¦æ¶æ„")

    # é…ç½®
    config = M4InferenceConfig(
        model_path="models/das_optimized_deepseek-coder-v2-236b.pth",
        max_memory_gb=12.0,  # Mac Mini M4 16Gï¼Œç•™4Gä½™é‡
        use_amx=True,
        quantization="fp16",
        chunk_size=512
    )

    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    engine = M24DASMacMiniInferenceEngine(config)

    # åŠ è½½æ¨¡å‹
    if not engine.load_model():
        logger.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡º")
        return

    # åˆ›å»ºåŸºå‡†æµ‹è¯•ä»»åŠ¡
    benchmark_tasks = create_benchmark_tasks()

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    logger.info("ğŸƒ å¼€å§‹å…¬å¼€åŸºå‡†æµ‹è¯•...")
    benchmark_results = engine.run_benchmark(benchmark_tasks)

    # è®¡ç®—ç»¼åˆåˆ†æ•°
    total_score = sum(result.score for result in benchmark_results)
    avg_score = total_score / len(benchmark_results)

    total_latency = sum(result.latency_sec for result in benchmark_results)
    avg_latency = total_latency / len(benchmark_results)

    total_throughput = sum(result.throughput_tokens_sec for result in benchmark_results)
    avg_throughput = total_throughput / len(benchmark_results)

    # è¾“å‡ºç»“æœ
    logger.info("ğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    logger.info("ğŸ“Š ç»¼åˆæ€§èƒ½æŒ‡æ ‡:")
    logger.info(f"   å¹³å‡åˆ†æ•°: {avg_score:.3f}")
    logger.info(f"   å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f} ç§’")
    logger.info(f"   å¹³å‡ååé‡: {avg_throughput:.2f} tokens/ç§’")
    logger.info(f"   å³°å€¼å†…å­˜ä½¿ç”¨: {engine.memory_monitor.get_peak_usage_gb():.2f} GB")
    logger.info(f"   M24åˆè§„æ€§: {all(r.m24_compliance for r in benchmark_results)}")

    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_summary = {
        'timestamp': time.time(),
        'config': asdict(config),
        'benchmark_results': [asdict(r) for r in benchmark_results],
        'summary': {
            'average_score': avg_score,
            'average_latency_sec': avg_latency,
            'average_throughput_tokens_sec': avg_throughput,
            'peak_memory_gb': engine.memory_monitor.get_peak_usage_gb(),
            'm24_compliance': all(r.m24_compliance for r in benchmark_results),
            'total_tasks': len(benchmark_results)
        },
        'system_info': {
            'platform': sys.platform,
            'python_version': sys.version,
            'torch_version': torch.__version__,
            'cpu_info': 'Apple M4',
            'memory_gb': 16.0
        }
    }

    # ä¿å­˜ç»“æœ
    results_file = f"m4_benchmark_results_{int(time.time())}.json"
    with open(results_file, 'w') as f:
        json.dump(results_summary, f, indent=2, default=str)

    logger.info(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_file}")

    # æ‰“å°å…¬å¼€åŸºå‡†æµ‹è¯•å£°æ˜
    logger.info("ğŸ“¢ å…¬å¼€åŸºå‡†æµ‹è¯•å£°æ˜:")
    logger.info("ğŸ¯ æœ¬æµ‹è¯•åŸºäºM24çœŸå®æ€§åŸåˆ™è¿›è¡Œï¼Œæ— ä»»ä½•ä»£ç æ¬ºéª—")
    logger.info("ğŸ”¬ æµ‹è¯•ç»“æœä»£è¡¨DASä¼˜åŒ–DeepSeekæ¨¡å‹åœ¨Mac Mini M4ä¸Šçš„çœŸå®æ€§èƒ½")
    logger.info("âš¡ æ‰€æœ‰ä¼˜åŒ–éƒ½æ˜¯ä¸ºäº†åœ¨16Gå†…å­˜è®¾å¤‡ä¸Šå®ç°æµç•…æ¨ç†")


if __name__ == "__main__":
    main()