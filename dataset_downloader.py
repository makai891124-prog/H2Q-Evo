#!/usr/bin/env python3
"""
è‡ªåŠ¨æ•°æ®é›†ä¸‹è½½å™¨ - æ”¯æŒImageNetã€COCOã€Kineticsç­‰è§†è§‰æ•°æ®é›†
"""

import os
import sys
import requests
import tarfile
import zipfile
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import hashlib
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import subprocess
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/Users/imymm/H2Q-Evo')

class DatasetDownloader:
    """è‡ªåŠ¨æ•°æ®é›†ä¸‹è½½å™¨"""

    def __init__(self, datasets_path: str = './datasets'):
        self.datasets_path = Path(datasets_path)
        self.datasets_path.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

        # æ•°æ®é›†é…ç½®
        self.dataset_configs = {
            'imagenet': {
                'name': 'ImageNet',
                'description': 'Large-scale image classification dataset',
                'urls': [
                    'https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_train.tar',
                    'https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar',
                    'https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_test.tar'
                ],
                'size': '150GB',
                'requires_login': True,
                'note': 'Requires ImageNet account and manual download'
            },
            'coco': {
                'name': 'COCO Dataset',
                'description': 'Common Objects in Context dataset',
                'urls': {
                    'train2017': 'http://images.cocodataset.org/zips/train2017.zip',
                    'val2017': 'http://images.cocodataset.org/zips/val2017.zip',
                    'test2017': 'http://images.cocodataset.org/zips/test2017.zip',
                    'annotations': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'
                },
                'size': '25GB',
                'requires_login': False
            },
            'kinetics': {
                'name': 'Kinetics-400',
                'description': 'Large-scale video action recognition dataset',
                'urls': [
                    'https://storage.googleapis.com/deepmind-media/Datasets/kinetics400.tar.gz'
                ],
                'size': '450GB',
                'requires_login': False,
                'note': 'Very large dataset, consider downloading subsets'
            },
            'ucf101': {
                'name': 'UCF101',
                'description': 'Action recognition dataset',
                'urls': [
                    'https://www.crcv.ucf.edu/data/UCF101/UCF101.rar',
                    'https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip'
                ],
                'size': '7GB',
                'requires_login': False,
                'note': 'Already available locally'
            },
            'cifar10': {
                'name': 'CIFAR-10',
                'description': 'Small image classification dataset for testing',
                'urls': [
                    'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'
                ],
                'size': '170MB',
                'requires_login': False
            },
            'cifar100': {
                'name': 'CIFAR-100',
                'description': 'Fine-grained image classification dataset',
                'urls': [
                    'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'
                ],
                'size': '170MB',
                'requires_login': False
            }
        }

    def list_available_datasets(self) -> Dict[str, Dict]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ•°æ®é›†"""
        return self.dataset_configs

    def check_dataset_status(self, dataset_name: str) -> Dict[str, bool]:
        """æ£€æŸ¥æ•°æ®é›†ä¸‹è½½çŠ¶æ€"""
        if dataset_name not in self.dataset_configs:
            return {'available': False, 'error': 'Dataset not found'}

        dataset_path = self.datasets_path / dataset_name
        config = self.dataset_configs[dataset_name]

        # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰æ•°æ®
        if dataset_name == 'ucf101':
            # ç‰¹æ®Šå¤„ç†UCF101ï¼Œå› ä¸ºå·²ç»åœ¨æœ¬åœ°
            ucf101_path = Path('/Users/imymm/H2Q-Evo/data/ucf101/UCF-101/UCF-101')
            has_data = ucf101_path.exists() and any(ucf101_path.rglob('*.avi'))
            return {
                'available': has_data,
                'local_path': str(ucf101_path),
                'size': config['size'],
                'note': 'Already available locally'
            }

        # æ£€æŸ¥æ ‡å‡†æ•°æ®é›†
        has_data = dataset_path.exists() and any(dataset_path.rglob('*'))
        return {
            'available': has_data,
            'local_path': str(dataset_path),
            'size': config['size']
        }

    def download_dataset(self, dataset_name: str, max_workers: int = 4) -> bool:
        """ä¸‹è½½æŒ‡å®šæ•°æ®é›†"""
        if dataset_name not in self.dataset_configs:
            self.logger.error(f"Dataset {dataset_name} not found")
            return False

        config = self.dataset_configs[dataset_name]
        dataset_path = self.datasets_path / dataset_name
        dataset_path.mkdir(parents=True, exist_ok=True)

        self.logger.info(f"å¼€å§‹ä¸‹è½½æ•°æ®é›†: {config['name']} ({config['size']})")

        # ç‰¹æ®Šå¤„ç†UCF101ï¼ˆå·²åœ¨æœ¬åœ°ï¼‰
        if dataset_name == 'ucf101':
            self.logger.info("UCF101æ•°æ®é›†å·²åœ¨æœ¬åœ°å¯ç”¨")
            return True

        # ç‰¹æ®Šå¤„ç†ImageNetï¼ˆéœ€è¦æ‰‹åŠ¨ä¸‹è½½ï¼‰
        if dataset_name == 'imagenet':
            self.logger.warning("ImageNetéœ€è¦æ‰‹åŠ¨ä¸‹è½½ï¼Œè¯·è®¿é—®: https://image-net.org/download.php")
            return False

        # ä¸‹è½½å…¶ä»–æ•°æ®é›†
        try:
            if isinstance(config['urls'], dict):
                # å¤šä¸ªæ–‡ä»¶çš„æƒ…å†µï¼ˆå¦‚COCOï¼‰
                return self._download_multiple_files(dataset_name, config['urls'], dataset_path, max_workers)
            else:
                # å•ä¸ªæ–‡ä»¶çš„æƒ…å†µ
                return self._download_single_file(dataset_name, config['urls'][0], dataset_path)
        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ•°æ®é›† {dataset_name} å¤±è´¥: {e}")
            return False

    def _download_single_file(self, dataset_name: str, url: str, dest_path: Path) -> bool:
        """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
        try:
            filename = url.split('/')[-1]
            file_path = dest_path / filename

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if file_path.exists():
                self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨: {filename}")
                return self._extract_file(file_path, dest_path)

            self.logger.info(f"ä¸‹è½½æ–‡ä»¶: {filename}")

            # ä¸‹è½½æ–‡ä»¶
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()

            total_size = int(response.headers.get('content-length', 0))
            downloaded_size = 0

            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)

                        # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
                        if total_size > 0:
                            progress = (downloaded_size / total_size) * 100
                            self.logger.info(".1f")

            self.logger.info(f"æ–‡ä»¶ä¸‹è½½å®Œæˆ: {filename}")

            # è§£å‹æ–‡ä»¶
            return self._extract_file(file_path, dest_path)

        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def _download_multiple_files(self, dataset_name: str, urls: Dict[str, str], dest_path: Path, max_workers: int) -> bool:
        """å¹¶è¡Œä¸‹è½½å¤šä¸ªæ–‡ä»¶"""
        self.logger.info(f"å¹¶è¡Œä¸‹è½½ {len(urls)} ä¸ªæ–‡ä»¶")

        success_count = 0

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {}
            for name, url in urls.items():
                future = executor.submit(self._download_single_file, f"{dataset_name}_{name}", url, dest_path)
                futures[future] = name

            for future in as_completed(futures):
                name = futures[future]
                try:
                    success = future.result()
                    if success:
                        success_count += 1
                        self.logger.info(f"æ–‡ä»¶ {name} ä¸‹è½½æˆåŠŸ")
                    else:
                        self.logger.error(f"æ–‡ä»¶ {name} ä¸‹è½½å¤±è´¥")
                except Exception as e:
                    self.logger.error(f"æ–‡ä»¶ {name} ä¸‹è½½å¼‚å¸¸: {e}")

        return success_count == len(urls)

    def _extract_file(self, file_path: Path, dest_path: Path) -> bool:
        """è§£å‹æ–‡ä»¶"""
        try:
            self.logger.info(f"è§£å‹æ–‡ä»¶: {file_path.name}")

            if file_path.suffix == '.zip':
                with zipfile.ZipFile(file_path, 'r') as zip_ref:
                    zip_ref.extractall(dest_path)
            elif file_path.suffix in ['.tar', '.gz', '.bz2']:
                with tarfile.open(file_path, 'r:*') as tar_ref:
                    tar_ref.extractall(dest_path)
            elif file_path.suffix == '.rar':
                # ä½¿ç”¨unrarå‘½ä»¤ï¼ˆéœ€è¦å®‰è£…unrarï¼‰
                try:
                    subprocess.run(['unrar', 'x', str(file_path), str(dest_path)], check=True)
                except subprocess.CalledProcessError:
                    self.logger.error("éœ€è¦å®‰è£…unraræ¥è§£å‹RARæ–‡ä»¶")
                    return False
            else:
                self.logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
                return False

            self.logger.info(f"æ–‡ä»¶è§£å‹å®Œæˆ: {file_path.name}")
            return True

        except Exception as e:
            self.logger.error(f"è§£å‹æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def verify_dataset_integrity(self, dataset_name: str) -> bool:
        """éªŒè¯æ•°æ®é›†å®Œæ•´æ€§"""
        if dataset_name not in self.dataset_configs:
            return False

        dataset_path = self.datasets_path / dataset_name

        # ç‰¹æ®Šå¤„ç†å·²åœ¨dataç›®å½•ä¸­çš„æ•°æ®é›†
        if dataset_name in ['cifar10', 'cifar100']:
            if dataset_name == 'cifar10':
                dataset_path = Path('./data/cifar-10-batches-py')
            elif dataset_name == 'cifar100':
                dataset_path = Path('./data/cifar-100-python')

        if dataset_name == 'ucf101':
            dataset_path = Path('/Users/imymm/H2Q-Evo/data/ucf101/UCF-101/UCF-101')

        # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶ç»“æ„
        if not dataset_path.exists():
            return False

        # æ ¹æ®æ•°æ®é›†ç±»å‹è¿›è¡Œç‰¹å®šæ£€æŸ¥
        if dataset_name == 'coco':
            # æ£€æŸ¥COCOæ•°æ®é›†ç»“æ„
            required_dirs = ['train2017', 'val2017', 'annotations']
            return all((dataset_path / d).exists() for d in required_dirs)

        elif dataset_name == 'cifar10':
            # æ£€æŸ¥CIFAR-10æ•°æ®æ–‡ä»¶
            return any(dataset_path.rglob('data_batch_*'))

        elif dataset_name == 'cifar100':
            # æ£€æŸ¥CIFAR-100æ•°æ®æ–‡ä»¶
            return (dataset_path / 'train').exists() and (dataset_path / 'test').exists()

        elif dataset_name == 'ucf101':
            # æ£€æŸ¥UCF101è§†é¢‘æ–‡ä»¶
            return any(dataset_path.rglob('*.avi'))

        # é»˜è®¤æ£€æŸ¥ï¼šç›®å½•ä¸ä¸ºç©º
        return any(dataset_path.rglob('*'))

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ è‡ªåŠ¨æ•°æ®é›†ä¸‹è½½å™¨")
    print("=" * 50)

    downloader = DatasetDownloader()

    # æ˜¾ç¤ºå¯ç”¨æ•°æ®é›†
    print("\nğŸ“‹ å¯ç”¨æ•°æ®é›†:")
    datasets = downloader.list_available_datasets()
    for name, config in datasets.items():
        status = downloader.check_dataset_status(name)
        available = "âœ…" if status['available'] else "âŒ"
        print(f"  {available} {name}: {config['name']} ({config['size']})")

    # ä¸‹è½½å°æ•°æ®é›†è¿›è¡Œæµ‹è¯•
    print("\nâ¬‡ï¸  ä¸‹è½½æµ‹è¯•æ•°æ®é›†...")

    # ä¸‹è½½CIFAR-10ï¼ˆå°æ•°æ®é›†ç”¨äºæµ‹è¯•ï¼‰
    if not downloader.check_dataset_status('cifar10')['available']:
        print("ä¸‹è½½CIFAR-10æ•°æ®é›†...")
        success = downloader.download_dataset('cifar10')
        if success:
            print("âœ… CIFAR-10ä¸‹è½½æˆåŠŸ")
        else:
            print("âŒ CIFAR-10ä¸‹è½½å¤±è´¥")

    # ä¸‹è½½CIFAR-100
    if not downloader.check_dataset_status('cifar100')['available']:
        print("ä¸‹è½½CIFAR-100æ•°æ®é›†...")
        success = downloader.download_dataset('cifar100')
        if success:
            print("âœ… CIFAR-100ä¸‹è½½æˆåŠŸ")
        else:
            print("âŒ CIFAR-100ä¸‹è½½å¤±è´¥")

    print("\nğŸ“ æ³¨æ„:")
    print("- ImageNetéœ€è¦æ‰‹åŠ¨ä¸‹è½½ï¼ˆè®¿é—® https://image-net.org/download.phpï¼‰")
    print("- COCOæ•°æ®é›†è¾ƒå¤§ï¼Œä¸‹è½½å¯èƒ½éœ€è¦æ—¶é—´")
    print("- Kineticsæ•°æ®é›†éå¸¸å¤§ï¼ˆ450GBï¼‰ï¼Œå»ºè®®æŒ‰éœ€ä¸‹è½½")

if __name__ == "__main__":
    main()