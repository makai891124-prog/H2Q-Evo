#!/usr/bin/env python3
"""
H2Q-Evo AGIè¿›åŒ–ç³»ç»Ÿå¯åŠ¨å™¨
å¯åŠ¨çœŸå®çš„AGIè¿›åŒ–ç³»ç»Ÿï¼ŒåŒ…å«å®Œæ•´çš„è¿›åŒ–æŸå¤±æŒ‡æ ‡è®¡ç®—
"""

import sys
import os
import argparse
import asyncio
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from evolution_system import H2QNexus, Config

def print_banner():
    """æ‰“å°å¯åŠ¨æ¨ªå¹…"""
    print("ğŸš€ H2Q-Evo AGIè¿›åŒ–ç³»ç»Ÿå¯åŠ¨å™¨")
    print("=" * 80)
    print("ğŸ¤– çœŸå®çš„AGIè¿›åŒ–ç³»ç»Ÿ - åŸºäºæ•°å­¦æ¶æ„å’Œè¿›åŒ–æŸå¤±æŒ‡æ ‡")
    print("ğŸ§¬ åŒ…å«å››ä¸ªæ ¸å¿ƒæŸå¤±æŒ‡æ ‡ï¼š")
    print("   â€¢ èƒ½åŠ›æå‡æŸå¤± - é‡åŒ–å„èƒ½åŠ›ç»´åº¦çš„æ”¹è¿›ç¨‹åº¦")
    print("   â€¢ çŸ¥è¯†æ•´åˆæŸå¤± - è¡¡é‡æ–°çŸ¥è¯†ä¸ç°æœ‰çŸ¥è¯†çš„æ•´åˆæ•ˆç‡")
    print("   â€¢ æ¶Œç°èƒ½åŠ›æŸå¤± - æ£€æµ‹æ–°èƒ½åŠ›çš„æ¶Œç°å’Œå·©å›ºç¨‹åº¦")
    print("   â€¢ ç¨³å®šæ€§æŸå¤± - ç¡®ä¿è¿›åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§")
    print("=" * 80)

def print_system_status(nexus):
    """æ‰“å°ç³»ç»ŸçŠ¶æ€"""
    print("\nğŸ“Š ç³»ç»ŸçŠ¶æ€æ£€æŸ¥:")

    # æ£€æŸ¥æ•°å­¦æ¶æ„é›†æˆ
    math_status = "âœ… å·²é›†æˆ" if nexus.math_bridge is not None else "âŒ æœªé›†æˆ"
    print(f"  æ•°å­¦æ¶æ„é›†æˆ: {math_status}")

    # æ£€æŸ¥AGIè¿›åŒ–æŸå¤±ç³»ç»Ÿ
    loss_status = "âœ… å·²é›†æˆ" if nexus.loss_system is not None else "âŒ æœªé›†æˆ"
    print(f"  AGIè¿›åŒ–æŸå¤±ç³»ç»Ÿ: {loss_status}")

    # æ£€æŸ¥Docker
    docker_status = "âœ… å¯ç”¨" if nexus.docker_available else "âŒ ä¸å¯ç”¨"
    print(f"  Dockerç¯å¢ƒ: {docker_status}")

    # æ£€æŸ¥æ¨ç†æ¨¡å¼
    mode = "LOCAL (Docker)" if Config.INFERENCE_MODE == 'local' else "API (Gemini)"
    print(f"  æ¨ç†æ¨¡å¼: {mode}")

    # æ£€æŸ¥APIå¯†é’¥
    api_status = "âœ… å·²é…ç½®" if Config.API_KEY else "âŒ æœªé…ç½®"
    print(f"  Gemini APIå¯†é’¥: {api_status}")

    print()

def setup_environment():
    """è®¾ç½®ç¯å¢ƒå˜é‡"""
    # è®¾ç½®é»˜è®¤çš„ç¯å¢ƒå˜é‡
    if not os.getenv("PROJECT_ROOT"):
        os.environ["PROJECT_ROOT"] = str(Path.cwd() / "h2q_project")

    if not os.getenv("INFERENCE_MODE"):
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å¼ï¼Œå¦‚æœDockerå¯ç”¨çš„è¯
        os.environ["INFERENCE_MODE"] = "local"

    if not os.getenv("LOG_LEVEL"):
        os.environ["LOG_LEVEL"] = "INFO"

async def start_evolution_system(continuous=True):
    """å¯åŠ¨AGIè¿›åŒ–ç³»ç»Ÿ"""
    print_banner()

    try:
        # è®¾ç½®ç¯å¢ƒ
        setup_environment()

        # åˆå§‹åŒ–ç³»ç»Ÿ
        print("ğŸ”§ åˆå§‹åŒ–H2Q-Evoç³»ç»Ÿ...")
        nexus = H2QNexus()

        # æ‰“å°ç³»ç»ŸçŠ¶æ€
        print_system_status(nexus)

        # æ£€æŸ¥å¿…è¦ç»„ä»¶
        if nexus.math_bridge is None:
            print("âš ï¸  è­¦å‘Š: æ•°å­¦æ¶æ„æœªé›†æˆï¼Œè¿›åŒ–åŠŸèƒ½å°†å—é™")
        else:
            print("âœ… æ•°å­¦æ¶æ„å·²é›†æˆï¼ŒAGIè¿›åŒ–åŠŸèƒ½å®Œæ•´")

        if nexus.loss_system is None:
            print("âš ï¸  è­¦å‘Š: AGIè¿›åŒ–æŸå¤±ç³»ç»Ÿæœªé›†æˆï¼ŒæŸå¤±æŒ‡æ ‡è®¡ç®—å°†ä¸å¯ç”¨")
        else:
            print("âœ… AGIè¿›åŒ–æŸå¤±ç³»ç»Ÿå·²é›†æˆï¼ŒæŸå¤±æŒ‡æ ‡è®¡ç®—å¯ç”¨")

        if not nexus.docker_available and Config.INFERENCE_MODE == 'local':
            print("âš ï¸  Dockerä¸å¯ç”¨ï¼Œå°†è‡ªåŠ¨åˆ‡æ¢åˆ°APIæ¨¡å¼")
            os.environ["INFERENCE_MODE"] = "api"
            Config.INFERENCE_MODE = "api"

        # å¯åŠ¨ç³»ç»Ÿ
        print("\nğŸš€ å¯åŠ¨AGIè¿›åŒ–ç³»ç»Ÿ...")
        if continuous:
            print("ğŸ”„ ç³»ç»Ÿå°†æŒç»­è¿è¡Œï¼Œæ‰§è¡Œ7*24å°æ—¶AGIè¿›åŒ–")
            print("ğŸ“Š æ¯60ç§’è®¡ç®—ä¸€æ¬¡è¿›åŒ–æŸå¤±æŒ‡æ ‡")
            print("ğŸ’¾ æŸå¤±æŒ‡æ ‡å°†ä¿å­˜åˆ°evo_state.json")
            print("ğŸ›‘ æŒ‰Ctrl+Cåœæ­¢ç³»ç»Ÿ")
            print("-" * 80)

            await nexus.run()
        else:
            print("ğŸ”„ æ‰§è¡Œå•æ¬¡æµ‹è¯•è¿›åŒ–å‘¨æœŸ...")

            # æ‰§è¡Œä¸€æ¬¡æµ‹è¯•å‘¨æœŸ
            if nexus.math_bridge is not None:
                import torch
                state = torch.randn(1, 256)
                learning_signal = torch.tensor([0.1])
                results = nexus.math_bridge(state, learning_signal)

                if nexus.loss_system is not None:
                    # è®¡ç®—AGIè¿›åŒ–æŸå¤±æŒ‡æ ‡
                    capability_embeddings = {
                        'mathematical_reasoning': torch.randn(256),
                        'creative_problem_solving': torch.randn(256),
                        'knowledge_integration': torch.randn(256),
                        'emergent_capabilities': torch.randn(256)
                    }

                    current_performance = {
                        'mathematical_reasoning': 0.75,
                        'creative_problem_solving': 0.68,
                        'knowledge_integration': 0.82,
                        'emergent_capabilities': 0.58
                    }

                    from agi_evolution_loss_metrics import MathematicalCoreMetrics
                    math_metrics = MathematicalCoreMetrics(
                        lie_automorphism_coherence=results.get('evolution_metrics', {}).get('state_change', 0.85),
                        noncommutative_geometry_consistency=0.78,
                        knot_invariant_stability=0.88,
                        dde_decision_quality=0.92,
                        constraint_violation=0.08,
                        fueter_violation=0.03
                    )

                    loss_components = nexus.loss_system(
                        capability_embeddings=capability_embeddings,
                        current_performance=current_performance,
                        mathematical_metrics=math_metrics
                    )

                    print("ğŸ“Š å•æ¬¡è¿›åŒ–å‘¨æœŸç»“æœ:")
                    print(f"  æ€»è¿›åŒ–æŸå¤±: {loss_components.total_loss:.4f}")
                    print(f"  è¿›åŒ–æ•ˆç‡è¯„åˆ†: {getattr(loss_components, 'evolution_efficiency_score', 0.0):.4f}")
                    print("âœ… å•æ¬¡æµ‹è¯•å®Œæˆ")

    except KeyboardInterrupt:
        print("\n\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­AGIè¿›åŒ–ç³»ç»Ÿ...")
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    print("\nâœ… AGIè¿›åŒ–ç³»ç»Ÿå·²åœæ­¢")
    return True

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="H2Q-Evo AGIè¿›åŒ–ç³»ç»Ÿå¯åŠ¨å™¨")
    parser.add_argument("--test", action="store_true",
                       help="æ‰§è¡Œå•æ¬¡æµ‹è¯•è€Œä¸æ˜¯æŒç»­è¿è¡Œ")
    parser.add_argument("--mode", choices=['local', 'api'],
                       help="æ¨ç†æ¨¡å¼ (local=æœ¬åœ°Docker, api=Gemini API)")
    parser.add_argument("--log-level", choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       default='INFO', help="æ—¥å¿—çº§åˆ«")

    args = parser.parse_args()

    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°
    if args.mode:
        os.environ["INFERENCE_MODE"] = args.mode
        Config.INFERENCE_MODE = args.mode

    if args.log_level:
        os.environ["LOG_LEVEL"] = args.log_level

    # å¯åŠ¨ç³»ç»Ÿ
    continuous = not args.test
    success = asyncio.run(start_evolution_system(continuous))

    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()