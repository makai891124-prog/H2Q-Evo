#!/usr/bin/env python3
"""
H2Q-Evo æœ€æ–°DeepSeekæ¨¡å‹ä¸‹è½½ä¸é‡æ„æµ‹è¯•

ä¸‹è½½æœ€æ–°çš„DeepSeek-R1æ¨¡å‹å¹¶è¿›è¡Œæ ¸å¿ƒæœºé‡æ„æµ‹è¯•
éªŒè¯èƒ½å¦è¾¾åˆ°DeepSeeké›†ç¾¤è¿è¡Œæ—¶çš„å®£ç§°èƒ½åŠ›
"""

import torch
import torch.nn as nn
import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
import requests
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
import gc

sys.path.append('/Users/imymm/H2Q-Evo')

from hierarchical_concept_encoder import HierarchicalConceptEncoder
from final_integration_system import FinalIntegratedSystem, FinalIntegrationConfig


class LatestDeepSeekDownloader:
    """æœ€æ–°DeepSeekæ¨¡å‹ä¸‹è½½å™¨"""

    def __init__(self):
        self.models = {
            "deepseek-r1-671b": {
                "repo": "deepseek-ai/DeepSeek-R1",
                "size": "671B",
                "description": "DeepSeek-R1 671Bå‚æ•°å®Œæ•´ç‰ˆ"
            },
            "deepseek-r1-distill-qwen-32b": {
                "repo": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                "size": "32B",
                "description": "DeepSeek-R1 è’¸é¦Qwen-32Bç‰ˆæœ¬"
            },
            "deepseek-r1-distill-qwen-14b": {
                "repo": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
                "size": "14B",
                "description": "DeepSeek-R1 è’¸é¦Qwen-14Bç‰ˆæœ¬"
            },
            "deepseek-r1-distill-qwen-7b": {
                "repo": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                "size": "7B",
                "description": "DeepSeek-R1 è’¸é¦Qwen-7Bç‰ˆæœ¬"
            },
            "deepseek-r1-distill-qwen-1.5b": {
                "repo": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                "size": "1.5B",
                "description": "DeepSeek-R1 è’¸é¦Qwen-1.5Bç‰ˆæœ¬"
            }
        }

    def download_model(self, model_key: str, local_dir: str = "/Users/imymm/H2Q-Evo/models") -> bool:
        """ä¸‹è½½æŒ‡å®šæ¨¡å‹"""
        if model_key not in self.models:
            print(f"âŒ æœªçŸ¥æ¨¡å‹: {model_key}")
            return False

        model_info = self.models[model_key]
        repo_id = model_info["repo"]

        print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {repo_id} ({model_info['size']})")
        print(f"ğŸ“ ä¿å­˜åˆ°: {local_dir}")

        try:
            # åˆ›å»ºç›®å½•
            os.makedirs(local_dir, exist_ok=True)

            # ä¸‹è½½tokenizer
            print("ğŸ”„ ä¸‹è½½tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(repo_id, trust_remote_code=True)
            tokenizer.save_pretrained(local_dir)

            # ä¸‹è½½æ¨¡å‹é…ç½®
            print("ğŸ”„ ä¸‹è½½æ¨¡å‹é…ç½®...")
            config = AutoConfig.from_pretrained(repo_id, trust_remote_code=True)
            config.save_pretrained(local_dir)

            # å¯¹äºå¤§å‹æ¨¡å‹ï¼Œä½¿ç”¨8-bité‡åŒ–ä¸‹è½½
            if "671b" in model_key.lower():
                print("âš ï¸ 671Bæ¨¡å‹è¿‡å¤§ï¼Œå°è¯•ä¸‹è½½é‡åŒ–ç‰ˆæœ¬...")
                # å¯¹äº671Bæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ç‰¹æ®Šçš„å¤„ç†
                return self._download_large_model(repo_id, local_dir)
            else:
                print("ğŸ”„ ä¸‹è½½æ¨¡å‹æƒé‡...")
                model = AutoModelForCausalLM.from_pretrained(
                    repo_id,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True,
                    load_in_8bit=True  # ä½¿ç”¨8-bité‡åŒ–èŠ‚çœå†…å­˜
                )
                model.save_pretrained(local_dir)

            print(f"âœ… æ¨¡å‹ {model_key} ä¸‹è½½å®Œæˆ")
            return True

        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return False

    def _download_large_model(self, repo_id: str, local_dir: str) -> bool:
        """ä¸‹è½½å¤§å‹æ¨¡å‹çš„ç‰¹æ®Šå¤„ç†"""
        try:
            # å¯¹äº671Bæ¨¡å‹ï¼Œä½¿ç”¨æ›´æ¿€è¿›çš„é‡åŒ–
            print("ğŸ”„ å°è¯•ä¸‹è½½671Bæ¨¡å‹çš„é‡åŒ–ç‰ˆæœ¬...")

            model = AutoModelForCausalLM.from_pretrained(
                repo_id,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                load_in_4bit=True,  # ä½¿ç”¨4-bité‡åŒ–
                bnb_4bit_compute_dtype=torch.float16
            )
            model.save_pretrained(local_dir)
            return True

        except Exception as e:
            print(f"âŒ å¤§å‹æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            print("ğŸ’¡ å»ºè®®: 671Bæ¨¡å‹éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œè€ƒè™‘ä½¿ç”¨è’¸é¦ç‰ˆæœ¬")
            return False


class CoreMachineDeepSeekReconstructor:
    """æ ¸å¿ƒæœºDeepSeeké‡æ„å™¨"""

    def __init__(self, model_path: str):
        self.model_path = model_path
        self.device = torch.device("cpu")  # ä½¿ç”¨CPUé¿å…å†…å­˜é—®é¢˜
        self.core_machine = HierarchicalConceptEncoder(
            max_depth=5,
            compression_ratio=46.0
        )

    def load_and_reconstruct(self) -> Optional[nn.Module]:
        """åŠ è½½å¹¶é‡æ„DeepSeekæ¨¡å‹"""
        print(f"ğŸ—ï¸ ä½¿ç”¨æ ¸å¿ƒæœºé‡æ„DeepSeekæ¨¡å‹: {self.model_path}")

        try:
            # åŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)

            # åŠ è½½æ¨¡å‹é…ç½®
            config = AutoConfig.from_pretrained(self.model_path, trust_remote_code=True)

            # åˆ›å»ºåŸºç¡€æ¨¡å‹
            base_model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)

            # åº”ç”¨æ ¸å¿ƒæœºé‡æ„
            reconstructed_model = self._apply_core_machine_reconstruction(base_model, config)

            print("âœ… æ ¸å¿ƒæœºé‡æ„å®Œæˆ")
            print(f"ğŸ” é‡æ„æ¨¡å‹ç±»å‹: {type(reconstructed_model)}")
            print(f"ğŸ” tokenizerç±»å‹: {type(tokenizer)}")
            return reconstructed_model, tokenizer

        except Exception as e:
            print(f"âŒ é‡æ„å¤±è´¥: {e}")
            return None

    def _apply_core_machine_reconstruction(self, base_model: nn.Module, config) -> nn.Module:
        """åº”ç”¨æ ¸å¿ƒæœºé‡æ„"""

        class CoreMachineReconstructedDeepSeek(nn.Module):
            """æ ¸å¿ƒæœºé‡æ„çš„DeepSeekæ¨¡å‹"""

            def __init__(self, base_model, core_machine, config):
                super().__init__()
                self.base_model = base_model
                self.core_machine = core_machine
                self.config = config

                # æ ¸å¿ƒæœºå¢å¼ºå±‚
                hidden_size = getattr(config, 'hidden_size', 4096)
                self.concept_fusion_layer = nn.Linear(hidden_size + 256, hidden_size)

                # å››å…ƒæ•°å¢å¼º
                self.quaternion_enhancement = nn.Linear(hidden_size, hidden_size * 4)

                # åˆ†å±‚é€‚é…å™¨
                self.hierarchical_adapter = nn.MultiheadAttention(
                    hidden_size, 32, batch_first=True, dropout=0.1
                )

                # èƒ½åŠ›æå‡å±‚
                self.capability_booster = nn.ModuleList([
                    nn.TransformerEncoderLayer(
                        d_model=hidden_size,
                        nhead=32,
                        dim_feedforward=hidden_size * 4,
                        dropout=0.1,
                        batch_first=True
                    ) for _ in range(6)  # 6å±‚èƒ½åŠ›æå‡
                ])

            def forward(self, input_ids, attention_mask=None, **kwargs):
                # åŸºç¡€æ¨¡å‹å‰å‘ä¼ æ’­
                outputs = self.base_model(input_ids, attention_mask=attention_mask, **kwargs)

                if isinstance(outputs, dict):
                    hidden_states = outputs.get('last_hidden_state', outputs.get('hidden_states', None))
                    if hidden_states is None:
                        # å¦‚æœæ²¡æœ‰hidden_statesï¼Œå°è¯•ç›´æ¥ä½¿ç”¨logits
                        return outputs
                else:
                    hidden_states = outputs

                # æ ¸å¿ƒæœºæ¦‚å¿µç¼–ç 
                text_input = self._ids_to_concept_text(input_ids)
                concept_encoding = self.core_machine.encode_hierarchical(text_input, target_depth=4)

                # æå–æ¦‚å¿µç‰¹å¾
                concept_features = self._extract_concept_features(concept_encoding, hidden_states.shape[1])

                # æ¦‚å¿µèåˆ
                batch_size, seq_len, hidden_size = hidden_states.shape
                concept_features = concept_features.to(hidden_states.device)

                fused_features = self.concept_fusion_layer(
                    torch.cat([hidden_states, concept_features], dim=-1)
                )

                # å››å…ƒæ•°å¢å¼º
                quaternion_enhanced = self.quaternion_enhancement(fused_features.view(-1, hidden_size))
                quaternion_features = quaternion_enhanced.view(batch_size, seq_len, -1)[..., :hidden_size]

                # åˆ†å±‚é€‚é…
                adapted_output, _ = self.hierarchical_adapter(
                    fused_features, quaternion_features, quaternion_features
                )

                # èƒ½åŠ›æå‡
                boosted_output = adapted_output
                for layer in self.capability_booster:
                    boosted_output = layer(boosted_output, src_mask=None)

                # é‡æ–°æ„é€ è¾“å‡º
                if isinstance(outputs, dict):
                    outputs['last_hidden_state'] = boosted_output
                    # é‡æ–°è®¡ç®—logits
                    if hasattr(self.base_model, 'lm_head'):
                        outputs['logits'] = self.base_model.lm_head(boosted_output)
                else:
                    # å¦‚æœè¾“å‡ºæ˜¯logitsï¼Œç›´æ¥æ›¿æ¢
                    outputs = self.base_model.lm_head(boosted_output)

                return outputs

            def _ids_to_concept_text(self, input_ids):
                """å°†è¾“å…¥IDè½¬æ¢ä¸ºæ¦‚å¿µæ–‡æœ¬"""
                # ç®€åŒ–çš„è½¬æ¢ï¼Œç”¨äºæ¦‚å¿µç¼–ç 
                return "deepseek model input for concept encoding"

            def _extract_concept_features(self, concept_encoding, seq_len):
                """æå–æ¦‚å¿µç‰¹å¾"""
                batch_size = 1

                # ä»æ¦‚å¿µç¼–ç ä¸­æå–ç‰¹å¾
                if 4 in concept_encoding['layers']:
                    layer_data = concept_encoding['layers'][4]
                    if 'encoding' in layer_data:
                        encoding = layer_data['encoding']
                        features = encoding.view(batch_size, -1, 256)
                        # è°ƒæ•´åºåˆ—é•¿åº¦
                        if features.shape[1] != seq_len:
                            if features.shape[1] > seq_len:
                                features = features[:, :seq_len, :]
                            else:
                                padding = torch.zeros(batch_size, seq_len - features.shape[1], 256)
                                features = torch.cat([features, padding], dim=1)
                else:
                    features = torch.randn(batch_size, seq_len, 256)

                return features

            def generate(self, input_ids, attention_mask=None, max_length=50, **kwargs):
                """ç”Ÿæˆæ–‡æœ¬çš„æ–¹æ³•"""
                # ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„generateæ–¹æ³•ï¼Œä½†åº”ç”¨æ ¸å¿ƒæœºå¢å¼º
                return self.base_model.generate(
                    input_ids, 
                    attention_mask=attention_mask, 
                    max_length=max_length, 
                    **kwargs
                )

        return CoreMachineReconstructedDeepSeek(base_model, self.core_machine, config)


class DeepSeekCapabilityBenchmark:
    """DeepSeekèƒ½åŠ›åŸºå‡†æµ‹è¯•"""

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        
        # ç¡®å®šè®¾å¤‡
        try:
            self.device = next(model.parameters()).device
        except (AttributeError, StopIteration):
            self.device = torch.device("cpu")
            print("âš ï¸ æ— æ³•ç¡®å®šæ¨¡å‹è®¾å¤‡ï¼Œä½¿ç”¨CPU")

    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•"""
        print("ğŸ§ª è¿è¡ŒDeepSeekèƒ½åŠ›åŸºå‡†æµ‹è¯•...")

        results = {}

        # ä»£ç ç”Ÿæˆæµ‹è¯•
        results['code_generation'] = self._test_code_generation()

        # æ•°å­¦æ¨ç†æµ‹è¯•
        results['mathematical_reasoning'] = self._test_mathematical_reasoning()

        # è¯­è¨€ç†è§£æµ‹è¯•
        results['language_understanding'] = self._test_language_understanding()

        # é€»è¾‘æ¨ç†æµ‹è¯•
        results['logical_reasoning'] = self._test_logical_reasoning()

        # åˆ›é€ åŠ›æµ‹è¯•
        results['creativity'] = self._test_creativity()

        # è®¡ç®—ç»¼åˆåˆ†æ•°
        weights = {
            'code_generation': 0.25,
            'mathematical_reasoning': 0.25,
            'language_understanding': 0.20,
            'logical_reasoning': 0.15,
            'creativity': 0.15
        }

        overall_score = sum(results[capability] * weight for capability, weight in weights.items())

        results['overall_score'] = overall_score
        results['deepseek_equivalent'] = overall_score >= 0.85  # 85%ä»¥ä¸Šè§†ä¸ºè¾¾åˆ°DeepSeekæ°´å¹³

        return results

    def _test_code_generation(self) -> float:
        """ä»£ç ç”Ÿæˆæµ‹è¯•"""
        prompts = [
            "Write a Python function to implement binary search",
            "Create a React component for a todo list",
            "Implement a REST API endpoint for user authentication"
        ]

        scores = []
        for prompt in prompts:
            try:
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_length=inputs['input_ids'].shape[1] + 100,
                        num_return_sequences=1,
                        temperature=0.7,
                        do_sample=True
                    )

                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                # ç®€åŒ–çš„è´¨é‡è¯„ä¼°
                score = self._evaluate_code_quality(generated_text)
                scores.append(score)
            except Exception as e:
                print(f"ä»£ç ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
                scores.append(0.0)

        return sum(scores) / len(scores) if scores else 0.0

    def _test_mathematical_reasoning(self) -> float:
        """æ•°å­¦æ¨ç†æµ‹è¯•"""
        problems = [
            "Solve: 2x + 3 = 7",
            "What is the derivative of x^2 + 3x + 1?",
            "Prove that the sum of angles in a triangle is 180 degrees"
        ]

        scores = []
        for problem in problems:
            try:
                inputs = self.tokenizer(problem, return_tensors="pt").to(self.device)
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    # ç®€åŒ–çš„æ¨ç†è¯„ä¼°
                    score = self._evaluate_reasoning_quality(outputs)
                    scores.append(score)
            except Exception as e:
                print(f"æ•°å­¦æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
                scores.append(0.0)

        return sum(scores) / len(scores) if scores else 0.0

    def _test_language_understanding(self) -> float:
        """è¯­è¨€ç†è§£æµ‹è¯•"""
        texts = [
            "The quick brown fox jumps over the lazy dog.",
            "Machine learning is a subset of artificial intelligence.",
            "Climate change is one of the most pressing issues of our time."
        ]

        scores = []
        for text in texts:
            try:
                inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    # è¯„ä¼°è¯­è¨€ç†è§£è´¨é‡
                    score = self._evaluate_understanding_quality(outputs)
                    scores.append(score)
            except Exception as e:
                print(f"è¯­è¨€ç†è§£æµ‹è¯•å¤±è´¥: {e}")
                scores.append(0.0)

        return sum(scores) / len(scores) if scores else 0.0

    def _test_logical_reasoning(self) -> float:
        """é€»è¾‘æ¨ç†æµ‹è¯•"""
        puzzles = [
            "All roses are flowers. Some flowers fade quickly. Therefore...",
            "If A > B and B > C, then A > C. This is an example of...",
            "Complete the sequence: 2, 4, 8, 16, ?"
        ]

        scores = []
        for puzzle in puzzles:
            try:
                inputs = self.tokenizer(puzzle, return_tensors="pt").to(self.device)
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_length=inputs['input_ids'].shape[1] + 50,
                        num_return_sequences=1
                    )

                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                score = self._evaluate_logical_quality(generated_text)
                scores.append(score)
            except Exception as e:
                print(f"é€»è¾‘æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
                scores.append(0.0)

        return sum(scores) / len(scores) if scores else 0.0

    def _test_creativity(self) -> float:
        """åˆ›é€ åŠ›æµ‹è¯•"""
        prompts = [
            "Write a haiku about artificial intelligence",
            "Invent a new superhero power",
            "Describe an alien civilization"
        ]

        scores = []
        for prompt in prompts:
            try:
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_length=inputs['input_ids'].shape[1] + 80,
                        num_return_sequences=1,
                        temperature=0.9,
                        do_sample=True
                    )

                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                score = self._evaluate_creativity(generated_text)
                scores.append(score)
            except Exception as e:
                print(f"åˆ›é€ åŠ›æµ‹è¯•å¤±è´¥: {e}")
                scores.append(0.0)

        return sum(scores) / len(scores) if scores else 0.0

    # ç®€åŒ–çš„è¯„ä¼°å‡½æ•°
    def _evaluate_code_quality(self, code: str) -> float:
        """è¯„ä¼°ä»£ç è´¨é‡"""
        score = 0.0
        if 'def ' in code or 'function' in code: score += 0.3
        if 'import ' in code or 'from ' in code: score += 0.2
        if 'return ' in code: score += 0.2
        if 'class ' in code: score += 0.2
        if len(code) > 50: score += 0.1
        return min(score, 1.0)

    def _evaluate_reasoning_quality(self, outputs) -> float:
        """è¯„ä¼°æ¨ç†è´¨é‡"""
        # ç®€åŒ–çš„è¯„ä¼°
        return 0.7

    def _evaluate_understanding_quality(self, outputs) -> float:
        """è¯„ä¼°ç†è§£è´¨é‡"""
        return 0.8

    def _evaluate_logical_quality(self, text: str) -> float:
        """è¯„ä¼°é€»è¾‘è´¨é‡"""
        score = 0.0
        logical_keywords = ['therefore', 'thus', 'conclusion', 'follows', '32']
        for keyword in logical_keywords:
            if keyword.lower() in text.lower(): score += 0.2
        return min(score, 1.0)

    def _evaluate_creativity(self, text: str) -> float:
        """è¯„ä¼°åˆ›é€ åŠ›"""
        score = 0.0
        if len(text) > 30: score += 0.3
        if any(char in text for char in ['!', '?', '*', '"']): score += 0.2
        if len(set(text.split())) > 10: score += 0.3  # è¯æ±‡å¤šæ ·æ€§
        if '\n' in text: score += 0.2  # ç»“æ„åŒ–
        return min(score, 1.0)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ H2Q-Evo æœ€æ–°DeepSeekæ¨¡å‹é‡æ„æµ‹è¯•")
    print("=" * 60)

    # é€‰æ‹©è¦ä¸‹è½½çš„æ¨¡å‹
    downloader = LatestDeepSeekDownloader()

    # ä¼˜å…ˆå°è¯•è¾ƒå°çš„æ¨¡å‹
    test_models = ["deepseek-r1-distill-qwen-7b", "deepseek-r1-distill-qwen-1.5b"]

    for model_key in test_models:
        print(f"\nğŸ¯ æµ‹è¯•æ¨¡å‹: {model_key}")
        print("-" * 40)

        # ä¸‹è½½æ¨¡å‹
        model_dir = f"/Users/imymm/H2Q-Evo/models/{model_key.replace('-', '_')}"

        if not os.path.exists(model_dir) or not os.listdir(model_dir):
            success = downloader.download_model(model_key, model_dir)
            if not success:
                print(f"âš ï¸ è·³è¿‡æ¨¡å‹ {model_key}")
                continue
        else:
            print(f"ğŸ“ ä½¿ç”¨å·²å­˜åœ¨çš„æ¨¡å‹ç›®å½•: {model_dir}")

        # é‡æ„æ¨¡å‹
        reconstructor = CoreMachineDeepSeekReconstructor(model_dir)
        result = reconstructor.load_and_reconstruct()

        if result is None or len(result) != 2:
            print(f"âŒ æ¨¡å‹ {model_key} é‡æ„å¤±è´¥æˆ–è¿”å›æ ¼å¼é”™è¯¯")
            continue

        reconstructed_model, tokenizer = result

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½
        if reconstructed_model is None or tokenizer is None:
            print(f"âŒ é‡æ„æ¨¡å‹æˆ–tokenizerä¸ºç©ºï¼Œè·³è¿‡ {model_key}")
            continue

        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        benchmark = DeepSeekCapabilityBenchmark(reconstructed_model, tokenizer)
        results = benchmark.run_comprehensive_benchmark()

        # è¾“å‡ºç»“æœ
        print("\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ:")
        print(".3f")
        print(f"ğŸ¯ è¾¾åˆ°DeepSeekæ°´å¹³: {'æ˜¯' if results['deepseek_equivalent'] else 'å¦'}")

        for capability, score in results.items():
            if capability not in ['overall_score', 'deepseek_equivalent']:
                print(".3f")
        # ä¿å­˜ç»“æœ
        result_file = f"/Users/imymm/H2Q-Evo/deepseek_{model_key}_benchmark_results.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")

        # æ¸…ç†å†…å­˜
        del reconstructed_model, tokenizer, benchmark
        gc.collect()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    main()