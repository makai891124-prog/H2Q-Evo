#!/usr/bin/env python3
"""
236Bæ¨¡å‹ç›´æ¥æ¨ç†æµ‹è¯• - éªŒè¯åŸºç¡€åŠŸèƒ½
"""

import torch
import torch.nn as nn
import json
import time
import psutil
from typing import Dict, Any
import sys
import os

sys.path.append('/Users/imymm/H2Q-Evo')

from final_integration_system import FinalIntegratedSystem, FinalIntegrationConfig


def get_memory_info() -> Dict[str, float]:
    """è·å–å†…å­˜ä¿¡æ¯"""
    memory = psutil.virtual_memory()
    return {
        "total_gb": memory.total / (1024**3),
        "available_gb": memory.available / (1024**3),
        "used_gb": memory.used / (1024**3),
        "percentage": memory.percent
    }


def test_basic_inference():
    """æµ‹è¯•åŸºç¡€æ¨ç†åŠŸèƒ½"""
    print("ğŸ§ª 236Bæ¨¡å‹åŸºç¡€æ¨ç†æµ‹è¯•")
    print("=" * 60)

    # åˆå§‹åŒ–é…ç½®
    config = FinalIntegrationConfig(
        model_compression_ratio=100.0,
        enable_mathematical_core=False,
        device="cpu"
    )

    # åˆ›å»ºç³»ç»Ÿ
    system = FinalIntegratedSystem(config)

    # åˆå§‹åŒ–æƒé‡
    weight_paths = [
        "/Users/imymm/H2Q-Evo/h2q_project/h2q_full_l1.pth",
        "/Users/imymm/H2Q-Evo/h2q_project/h2q_qwen_crystal.pt",
        "/Users/imymm/H2Q-Evo/h2q_project/h2q_model_hierarchy.pth"
    ]

    initialized = False
    for weight_path in weight_paths:
        if os.path.exists(weight_path):
            print(f"ğŸ“¥ å°è¯•åŠ è½½æƒé‡: {weight_path}")
            if system.initialize_from_236b_weights(weight_path):
                initialized = True
                break

    if not initialized:
        print("âš ï¸ ä½¿ç”¨æ¨¡æ‹Ÿ236Bæƒé‡è¿›è¡Œæ¼”ç¤º")
        mock_weights = system.weight_converter._create_mock_236b_weights()
        mock_path = "/tmp/mock_236b_weights.pth"
        torch.save(mock_weights, mock_path)
        system.initialize_from_236b_weights(mock_path)

    print("\nğŸ” æµ‹è¯•åŸºç¡€æ¨ç†")

    # åˆ›å»ºéšæœºè¾“å…¥
    test_inputs = [
        torch.randint(0, 10000, (1, 5)).to(system.device),
        torch.randint(0, 10000, (1, 10)).to(system.device),
        torch.randint(0, 10000, (1, 20)).to(system.device)
    ]

    results = {
        "timestamp": time.time(),
        "memory_before": get_memory_info(),
        "inference_tests": [],
        "streaming_test": {},
        "performance_summary": {}
    }

    print("\nğŸ“Š æ ‡å‡†æ¨ç†æµ‹è¯•")
    inference_times = []

    for i, test_input in enumerate(test_inputs):
        print(f"  æµ‹è¯• {i+1}: è¾“å…¥å½¢çŠ¶ {test_input.shape}")

        start_time = time.time()
        try:
            output = system.perform_local_inference(test_input)
            inference_time = time.time() - start_time

            print(f"    è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(".4f")
            print(f"    è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")

            # æ£€æŸ¥è¾“å‡ºæ˜¯å¦æœ‰æ„ä¹‰ï¼ˆä¸æ˜¯å…¨é›¶æˆ–NaNï¼‰
            is_valid = not (torch.isnan(output).any() or torch.isinf(output).any())
            has_variance = output.var() > 1e-6

            test_result = {
                "input_shape": list(test_input.shape),
                "output_shape": list(output.shape),
                "inference_time": inference_time,
                "is_valid": is_valid,
                "has_variance": has_variance,
                "output_stats": {
                    "mean": float(output.mean().item()),
                    "std": float(output.std().item()),
                    "min": float(output.min().item()),
                    "max": float(output.max().item())
                }
            }

            results["inference_tests"].append(test_result)
            inference_times.append(inference_time)

        except Exception as e:
            print(f"    âŒ æ¨ç†å¤±è´¥: {e}")
            results["inference_tests"].append({
                "input_shape": list(test_input.shape),
                "error": str(e)
            })

    print("\nğŸŒŠ æµå¼æ¨ç†æµ‹è¯•")
    streaming_tokens = []
    start_time = time.time()

    try:
        for token in system.stream_inference(test_inputs[0], max_length=10):
            streaming_tokens.append(token)

        streaming_time = time.time() - start_time

        print(f"  ç”Ÿæˆtokenæ•°é‡: {len(streaming_tokens)}")
        print(".4f")
        print(f"  ç”Ÿæˆçš„tokens: {streaming_tokens}")

        results["streaming_test"] = {
            "tokens_generated": len(streaming_tokens),
            "total_time": streaming_time,
            "tokens_per_second": len(streaming_tokens) / streaming_time if streaming_time > 0 else 0,
            "tokens": streaming_tokens
        }

    except Exception as e:
        print(f"  âŒ æµå¼æ¨ç†å¤±è´¥: {e}")
        results["streaming_test"] = {"error": str(e)}

    # æ€§èƒ½æ€»ç»“
    results["memory_after"] = get_memory_info()

    if inference_times:
        results["performance_summary"] = {
            "avg_inference_time": sum(inference_times) / len(inference_times),
            "total_inference_time": sum(inference_times),
            "memory_delta_mb": (results["memory_after"]["used_gb"] - results["memory_before"]["used_gb"]) * 1024,
            "model_loaded": initialized
        }

    # ä¿å­˜ç»“æœï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…tensoråºåˆ—åŒ–é—®é¢˜ï¼‰
    simplified_results = {
        "timestamp": results["timestamp"],
        "memory_before": results["memory_before"],
        "memory_after": results["memory_after"],
        "inference_tests_count": len(results["inference_tests"]),
        "streaming_tokens_generated": results["streaming_test"].get("tokens_generated", 0),
        "streaming_time": results["streaming_test"].get("total_time", 0),
        "performance_summary": results.get("performance_summary", {})
    }

    with open("236b_basic_inference_test.json", "w", encoding="utf-8") as f:
        json.dump(simplified_results, f, indent=2, ensure_ascii=False)

    print("\nğŸ“‹ æµ‹è¯•æ€»ç»“")
    print("=" * 60)

    valid_tests = sum(1 for t in results["inference_tests"] if t.get("is_valid", False))
    total_tests = len(results["inference_tests"])

    print(f"âœ… æœ‰æ•ˆæ¨ç†æµ‹è¯•: {valid_tests}/{total_tests}")

    if results["performance_summary"]:
        perf = results["performance_summary"]
        print(f"    å¹³å‡æ¨ç†æ—¶é—´: {perf['avg_inference_time']:.4f} ç§’")
        print(f"    å†…å­˜å¢é‡: {perf['memory_delta_mb']:.1f} MB")
    print("\nğŸ’¾ å†…å­˜ä½¿ç”¨:")
    print(f"   æ€»å†…å­˜: {results['memory_before']['total_gb']:.1f} GB")
    print(f"   ä½¿ç”¨å‰: {results['memory_before']['used_gb']:.1f} GB")
    print(f"   ä½¿ç”¨å: {results['memory_after']['used_gb']:.1f} GB")

    print("\nğŸ† ç»“è®º:")
    if valid_tests > 0:
        print("   âœ… 236Bæ¨¡å‹åŸºç¡€æ¨ç†åŠŸèƒ½æ­£å¸¸")
        print("   âœ… æ¨¡å‹å¯ä»¥ç”Ÿæˆæœ‰æ•ˆè¾“å‡º")
        if results["streaming_test"].get("tokens_generated", 0) > 0:
            print("   âœ… æµå¼æ¨ç†åŠŸèƒ½æ­£å¸¸")
        else:
            print("   âš ï¸ æµå¼æ¨ç†éœ€è¦ä¼˜åŒ–")
    else:
        print("   âŒ 236Bæ¨¡å‹æ¨ç†åŠŸèƒ½å¼‚å¸¸")
        print("   ğŸ”§ éœ€è¦æ£€æŸ¥æƒé‡è½¬æ¢å’Œæ¨¡å‹åˆå§‹åŒ–")

    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜: 236b_basic_inference_test.json")

    return results


if __name__ == "__main__":
    test_basic_inference()