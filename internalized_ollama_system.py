#!/usr/bin/env python3
"""
H2Q-Evo å†…åŒ–Ollamaç³»ç»Ÿ (Internalized Ollama System)

å°†Ollamaé¡¹ç›®å®Œå…¨å†…åŒ–åˆ°H2Q-Evoä¸­ï¼Œå®ç°ï¼š
1. è‡ªåŒ…å«çš„æ¨¡å‹è¿è¡Œæ—¶
2. å†…å­˜ä¼˜åŒ–çš„å¤šæ¨¡å‹æ”¯æŒ
3. è‡ªåŠ¨æ¨¡å‹ä¸‹è½½å’Œç®¡ç†
4. H2Qç»“æ™¶åŒ–å‹ç¼©
5. è¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–
"""

import torch
import torch.nn as nn
from typing import Dict, Any, Tuple, List, Optional, Union, Callable
import json
import time
import os
import psutil
import threading
import requests
import hashlib
import gzip
import shutil
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import weakref
import gc
from pathlib import Path
import subprocess
import sys

# å¯¼å…¥H2Qæ ¸å¿ƒç»„ä»¶
from model_crystallization_engine import ModelCrystallizationEngine, CrystallizationConfig
from memory_safe_startup import MemorySafeStartupSystem, MemorySafeConfig, MemoryGuardian
from advanced_spectral_controller import AdvancedSpectralController


@dataclass
class InternalizedOllamaConfig:
    """å†…åŒ–Ollamaé…ç½®"""
    # æ¨¡å‹å­˜å‚¨
    model_cache_dir: str = "./models"
    crystallized_cache_dir: str = "./crystallized_models"
    temp_dir: str = "./temp"

    # å†…å­˜é…ç½®
    max_memory_mb: int = 4096  # 4GBæ€»å†…å­˜é™åˆ¶
    model_memory_limit_mb: int = 2048  # å•ä¸ªæ¨¡å‹2GB
    working_memory_mb: int = 1024  # å·¥ä½œå†…å­˜1GB

    # æ¨¡å‹é…ç½®
    supported_formats: List[str] = None  # æ”¯æŒçš„æ¨¡å‹æ ¼å¼
    auto_download: bool = True  # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹
    enable_crystallization: bool = True  # å¯ç”¨ç»“æ™¶åŒ–
    compression_ratio: float = 8.0  # å‹ç¼©ç‡

    # è¿è¡Œæ—¶é…ç½®
    max_concurrent_models: int = 2  # æœ€å¤§å¹¶å‘æ¨¡å‹æ•°
    inference_threads: int = 4  # æ¨ç†çº¿ç¨‹æ•°
    enable_streaming: bool = True  # å¯ç”¨æµå¼æ¨ç†

    # è¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–
    enable_quantization: bool = True  # å¯ç”¨é‡åŒ–
    target_device: str = "auto"  # ç›®æ ‡è®¾å¤‡ (auto/cpu/cuda/mps)
    optimize_for_edge: bool = True  # è¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–

    def __post_init__(self):
        if self.supported_formats is None:
            self.supported_formats = ["gguf", "safetensors", "pkl", "pth", "bin"]

        # æ ¹æ®è®¾å¤‡è‡ªåŠ¨é…ç½®
        if self.target_device == "auto":
            if torch.cuda.is_available():
                self.target_device = "cuda"
            elif torch.backends.mps.is_available():
                self.target_device = "mps"
            else:
                self.target_device = "cpu"


class ModelRegistry:
    """æ¨¡å‹æ³¨å†Œè¡¨"""

    def __init__(self, config: InternalizedOllamaConfig):
        self.config = config
        self.models: Dict[str, Dict[str, Any]] = {}
        self.loaded_models: Dict[str, weakref.ReferenceType] = {}

        # åˆ›å»ºç›®å½•
        os.makedirs(config.model_cache_dir, exist_ok=True)
        os.makedirs(config.crystallized_cache_dir, exist_ok=True)
        os.makedirs(config.temp_dir, exist_ok=True)

        # åŠ è½½æ¨¡å‹æ³¨å†Œè¡¨
        self._load_registry()

    def register_model(self, name: str, metadata: Dict[str, Any]):
        """æ³¨å†Œæ¨¡å‹"""
        self.models[name] = metadata
        self._save_registry()

    def get_model_info(self, name: str) -> Optional[Dict[str, Any]]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.models.get(name)

    def list_available_models(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        return list(self.models.keys())

    def _load_registry(self):
        """åŠ è½½æ³¨å†Œè¡¨"""
        registry_file = os.path.join(self.config.model_cache_dir, "registry.json")
        if os.path.exists(registry_file):
            try:
                with open(registry_file, 'r', encoding='utf-8') as f:
                    self.models = json.load(f)
            except Exception as e:
                print(f"åŠ è½½æ³¨å†Œè¡¨å¤±è´¥: {e}")

    def _save_registry(self):
        """ä¿å­˜æ³¨å†Œè¡¨"""
        registry_file = os.path.join(self.config.model_cache_dir, "registry.json")
        try:
            with open(registry_file, 'w', encoding='utf-8') as f:
                json.dump(self.models, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"ä¿å­˜æ³¨å†Œè¡¨å¤±è´¥: {e}")


class ModelDownloader:
    """æ¨¡å‹ä¸‹è½½å™¨"""

    def __init__(self, config: InternalizedOllamaConfig, registry: ModelRegistry):
        self.config = config
        self.registry = registry
        self.download_sessions: Dict[str, Dict[str, Any]] = {}

    def download_model(self, model_name: str, source_url: str = None,
                      progress_callback: Callable = None) -> bool:
        """ä¸‹è½½æ¨¡å‹"""
        try:
            print(f"å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")

            # è·å–æ¨¡å‹ä¿¡æ¯
            model_info = self.registry.get_model_info(model_name)
            if not model_info and not source_url:
                print(f"æœªæ‰¾åˆ°æ¨¡å‹ {model_name} çš„ä¿¡æ¯ï¼Œä¸”æœªæä¾›ä¸‹è½½æº")
                return False

            # ç¡®å®šä¸‹è½½URL
            download_url = source_url or model_info.get('download_url')
            if not download_url:
                print(f"æ¨¡å‹ {model_name} æ²¡æœ‰ä¸‹è½½URL")
                return False

            # åˆ›å»ºä¸‹è½½ä¼šè¯
            session_id = f"{model_name}_{int(time.time())}"
            self.download_sessions[session_id] = {
                'model_name': model_name,
                'status': 'downloading',
                'progress': 0.0,
                'start_time': time.time()
            }

            # æ‰§è¡Œä¸‹è½½
            success = self._download_file(download_url, model_name, progress_callback)

            # æ›´æ–°çŠ¶æ€
            self.download_sessions[session_id]['status'] = 'completed' if success else 'failed'
            self.download_sessions[session_id]['end_time'] = time.time()

            if success:
                print(f"æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ")
                # æ³¨å†Œæ¨¡å‹
                if not model_info:
                    self.registry.register_model(model_name, {
                        'name': model_name,
                        'format': self._guess_format(model_name),
                        'size_mb': self._get_file_size_mb(model_name),
                        'download_url': download_url,
                        'downloaded_at': time.time()
                    })
            else:
                print(f"æ¨¡å‹ {model_name} ä¸‹è½½å¤±è´¥")

            return success

        except Exception as e:
            print(f"ä¸‹è½½æ¨¡å‹å¤±è´¥: {e}")
            return False

    def _download_file(self, url: str, model_name: str,
                      progress_callback: Callable = None) -> bool:
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()

            # è·å–æ–‡ä»¶å¤§å°
            total_size = int(response.headers.get('content-length', 0))

            # ç¡®å®šæœ¬åœ°è·¯å¾„
            local_path = self._get_model_path(model_name)

            # ä¸‹è½½æ–‡ä»¶
            downloaded = 0
            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)

                        # æ›´æ–°è¿›åº¦
                        if total_size > 0 and progress_callback:
                            progress = downloaded / total_size
                            progress_callback(model_name, progress)

            return True

        except Exception as e:
            print(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {e}")
            return False

    def _get_model_path(self, model_name: str) -> str:
        """è·å–æ¨¡å‹æœ¬åœ°è·¯å¾„"""
        return os.path.join(self.config.model_cache_dir, f"{model_name}.gguf")

    def _guess_format(self, model_name: str) -> str:
        """çŒœæµ‹æ¨¡å‹æ ¼å¼"""
        if '.gguf' in model_name:
            return 'gguf'
        elif '.safetensors' in model_name:
            return 'safetensors'
        elif '.pkl' in model_name:
            return 'pkl'
        elif '.pth' in model_name:
            return 'pth'
        elif '.bin' in model_name:
            return 'bin'
        else:
            return 'unknown'

    def _get_file_size_mb(self, model_name: str) -> float:
        """è·å–æ–‡ä»¶å¤§å°(MB)"""
        path = self._get_model_path(model_name)
        if os.path.exists(path):
            return os.path.getsize(path) / (1024 * 1024)
        return 0.0


class ModelLoader:
    """æ¨¡å‹åŠ è½½å™¨"""

    def __init__(self, config: InternalizedOllamaConfig, registry: ModelRegistry,
                 memory_guardian: MemoryGuardian):
        self.config = config
        self.registry = registry
        self.memory_guardian = memory_guardian
        self.loaded_models: Dict[str, weakref.ReferenceType] = {}
        self.model_cache: Dict[str, Any] = {}

    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """åŠ è½½æ¨¡å‹"""
        try:
            print(f"å¼€å§‹åŠ è½½æ¨¡å‹: {model_name}")

            # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½
            if model_name in self.loaded_models:
                ref = self.loaded_models[model_name]
                model = ref() if ref() is not None else None
                if model is not None:
                    print(f"æ¨¡å‹ {model_name} å·²åŠ è½½ï¼Œä»ç¼“å­˜è¿”å›")
                    return model

            # è·å–æ¨¡å‹ä¿¡æ¯
            model_info = self.registry.get_model_info(model_name)
            if not model_info:
                print(f"æœªæ‰¾åˆ°æ¨¡å‹ {model_name} çš„ä¿¡æ¯")
                return None

            # æ£€æŸ¥å†…å­˜é¢„ç®—
            estimated_memory = self._estimate_model_memory(model_info)
            if not self.memory_guardian.allocate_memory('model', estimated_memory):
                print(f"å†…å­˜ä¸è¶³ï¼Œæ— æ³•åŠ è½½æ¨¡å‹ {model_name}")
                return None

            # ç¡®å®šæ¨¡å‹è·¯å¾„
            model_path = self._get_model_path(model_name)
            if not os.path.exists(model_path):
                print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return None

            # æ ¹æ®æ ¼å¼åŠ è½½æ¨¡å‹
            model_format = model_info.get('format', 'unknown')
            model = self._load_model_by_format(model_path, model_format, **kwargs)

            if model:
                # ä½¿ç”¨å¼±å¼•ç”¨è·Ÿè¸ªï¼ˆå¦‚æœæ”¯æŒçš„è¯ï¼‰
                try:
                    self.loaded_models[model_name] = weakref.ref(
                        model,
                        lambda ref: self._model_cleanup_callback(model_name, estimated_memory)
                    )
                except TypeError:
                    # å¯¹äºä¸æ”¯æŒå¼±å¼•ç”¨çš„å¯¹è±¡ï¼Œä½¿ç”¨æ™®é€šå¼•ç”¨å’Œæ‰‹åŠ¨æ¸…ç†
                    self.loaded_models[model_name] = model
                    # æ·»åŠ åˆ°æ¸…ç†åˆ—è¡¨
                    if not hasattr(self, '_manual_cleanup_models'):
                        self._manual_cleanup_models = {}
                    self._manual_cleanup_models[model_name] = estimated_memory

                print(f"æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
            else:
                self.memory_guardian.deallocate_memory('model', estimated_memory)

            return model

        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return None

    def _load_model_by_format(self, path: str, format: str, **kwargs) -> Optional[Any]:
        """æ ¹æ®æ ¼å¼åŠ è½½æ¨¡å‹"""
        try:
            if format == 'gguf':
                return self._load_gguf_model(path, **kwargs)
            elif format == 'safetensors':
                return self._load_safetensors_model(path, **kwargs)
            elif format == 'pkl':
                return self._load_pickle_model(path, **kwargs)
            elif format == 'pth':
                return self._load_pytorch_model(path, **kwargs)
            elif format == 'bin':
                return self._load_binary_model(path, **kwargs)
            else:
                print(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {format}")
                return None
        except Exception as e:
            print(f"åŠ è½½ {format} æ ¼å¼æ¨¡å‹å¤±è´¥: {e}")
            return None

    def _load_gguf_model(self, path: str, **kwargs) -> Optional[Any]:
        """åŠ è½½GGUFæ¨¡å‹"""
        # è¿™é‡Œå®ç°GGUFæ ¼å¼çš„åŠ è½½é€»è¾‘
        # ç”±äºGGUFæ˜¯äºŒè¿›åˆ¶æ ¼å¼ï¼Œéœ€è¦ä¸“é—¨çš„è§£æå™¨
        print(f"åŠ è½½GGUFæ¨¡å‹: {path}")
        # ç®€åŒ–çš„å®ç° - å®é™…éœ€è¦GGUFè§£æåº“
        return {"type": "gguf", "path": path, "loaded": True}

    def _load_safetensors_model(self, path: str, **kwargs) -> Optional[Any]:
        """åŠ è½½SafeTensorsæ¨¡å‹"""
        try:
            from safetensors import safe_open
            tensors = {}
            with safe_open(path, framework="pt", device=self.config.target_device) as f:
                for key in f.keys():
                    tensors[key] = f.get_tensor(key)
            return {"type": "safetensors", "tensors": tensors, "loaded": True}
        except ImportError:
            print("SafeTensorsåº“æœªå®‰è£…")
            return None

    def _load_pickle_model(self, path: str, **kwargs) -> Optional[Any]:
        """åŠ è½½Pickleæ¨¡å‹"""
        import pickle
        with open(path, 'rb') as f:
            model = pickle.load(f)
        return model

    def _load_pytorch_model(self, path: str, **kwargs) -> Optional[Any]:
        """åŠ è½½PyTorchæ¨¡å‹"""
        model = torch.load(path, map_location=self.config.target_device)
        return model

    def _load_binary_model(self, path: str, **kwargs) -> Optional[Any]:
        """åŠ è½½äºŒè¿›åˆ¶æ¨¡å‹"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ ¼å¼å®ç°
        print(f"åŠ è½½äºŒè¿›åˆ¶æ¨¡å‹: {path}")
        return {"type": "binary", "path": path, "loaded": True}

    def _estimate_model_memory(self, model_info: Dict[str, Any]) -> float:
        """ä¼°ç®—æ¨¡å‹å†…å­˜éœ€æ±‚"""
        size_mb = model_info.get('size_mb', 100)  # é»˜è®¤100MB
        # æ ¹æ®æ ¼å¼è°ƒæ•´ä¼°ç®—
        format_multiplier = {
            'gguf': 1.5,  # GGUFé€šå¸¸æ›´ç´§å‡‘
            'safetensors': 2.0,
            'pth': 2.5,
            'pkl': 2.0,
            'bin': 1.8
        }
        multiplier = format_multiplier.get(model_info.get('format', 'unknown'), 2.0)
        return size_mb * multiplier

    def _get_model_path(self, model_name: str) -> str:
        """è·å–æ¨¡å‹è·¯å¾„"""
        return os.path.join(self.config.model_cache_dir, f"{model_name}.gguf")

    def unload_model(self, model_name: str):
        """å¸è½½æ¨¡å‹"""
        if model_name in self.loaded_models:
            # æ‰‹åŠ¨æ¸…ç†å†…å­˜
            if hasattr(self, '_manual_cleanup_models') and model_name in self._manual_cleanup_models:
                memory_mb = self._manual_cleanup_models[model_name]
                self.memory_guardian.deallocate_memory('model', memory_mb)
                del self._manual_cleanup_models[model_name]

            del self.loaded_models[model_name]
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶

    def _model_cleanup_callback(self, model_name: str, memory_mb: float):
        """æ¨¡å‹æ¸…ç†å›è°ƒ"""
        print(f"æ¨¡å‹ {model_name} è¢«æ¸…ç†ï¼Œé‡Šæ”¾ {memory_mb:.1f} MB å†…å­˜")
        self.memory_guardian.deallocate_memory('model', memory_mb)


class H2QModelCrystallizer:
    """H2Qæ¨¡å‹ç»“æ™¶å™¨"""

    def __init__(self, config: InternalizedOllamaConfig, memory_guardian: MemoryGuardian):
        self.config = config
        self.memory_guardian = memory_guardian
        self.crystallization_engine = ModelCrystallizationEngine(
            CrystallizationConfig(
                target_compression_ratio=config.compression_ratio,
                max_memory_mb=config.model_memory_limit_mb,
                device=config.target_device
            )
        )

    def crystallize_model(self, model: Any, model_name: str) -> Optional[Any]:
        """ç»“æ™¶åŒ–æ¨¡å‹"""
        try:
            print(f"å¼€å§‹ç»“æ™¶åŒ–æ¨¡å‹: {model_name}")

            # æ£€æŸ¥å†…å­˜é¢„ç®—
            if not self.memory_guardian.allocate_memory('working', 500):  # 500MBå·¥ä½œå†…å­˜
                print("å†…å­˜ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç»“æ™¶åŒ–")
                return None

            # æ‰§è¡Œç»“æ™¶åŒ–
            if isinstance(model, nn.Module):
                # PyTorchæ¨¡å‹
                crystallized = self.crystallization_engine.crystallize_model(model, model_name)
            elif isinstance(model, dict) and 'tensors' in model:
                # SafeTensorsæ ¼å¼
                crystallized = self._crystallize_tensors(model['tensors'], model_name)
            else:
                # å…¶ä»–æ ¼å¼çš„ç®€åŒ–ä¸ºåŒ…è£…
                crystallized = self._crystallize_generic(model, model_name)

            self.memory_guardian.deallocate_memory('working', 500)

            if crystallized:
                print(f"æ¨¡å‹ {model_name} ç»“æ™¶åŒ–å®Œæˆ")
                # ä¿å­˜ç»“æ™¶åŒ–æ¨¡å‹
                self._save_crystallized_model(crystallized, model_name)

            return crystallized

        except Exception as e:
            print(f"ç»“æ™¶åŒ–å¤±è´¥: {e}")
            self.memory_guardian.deallocate_memory('working', 500)
            return None

    def _crystallize_tensors(self, tensors: Dict[str, torch.Tensor], model_name: str) -> Dict[str, Any]:
        """ç»“æ™¶åŒ–å¼ é‡"""
        # åˆ›å»ºè™šæ‹Ÿæ¨¡å‹è¿›è¡Œç»“æ™¶åŒ–
        class VirtualModel(nn.Module):
            def __init__(self, tensors):
                super().__init__()
                for name, tensor in tensors.items():
                    self.register_buffer(name.replace('.', '_'), tensor)

        virtual_model = VirtualModel(tensors)
        return self.crystallization_engine.crystallize_model(virtual_model, model_name)

    def _crystallize_generic(self, model: Any, model_name: str) -> Dict[str, Any]:
        """é€šç”¨ç»“æ™¶åŒ–"""
        # å¯¹äºä¸æ”¯æŒçš„æ ¼å¼ï¼Œè¿”å›åŒ…è£…ç‰ˆæœ¬
        return {
            'original_model': model,
            'crystallized': False,
            'compression_ratio': 1.0,
            'metadata': {
                'model_name': model_name,
                'crystallized_at': time.time(),
                'method': 'generic_wrapper'
            }
        }

    def _save_crystallized_model(self, crystallized: Any, model_name: str):
        """ä¿å­˜ç»“æ™¶åŒ–æ¨¡å‹"""
        try:
            path = os.path.join(self.config.crystallized_cache_dir, f"{model_name}_crystallized.pkl")
            import pickle
            with open(path, 'wb') as f:
                pickle.dump(crystallized, f)
            print(f"ç»“æ™¶åŒ–æ¨¡å‹å·²ä¿å­˜: {path}")
        except Exception as e:
            print(f"ä¿å­˜ç»“æ™¶åŒ–æ¨¡å‹å¤±è´¥: {e}")


class InferenceEngine:
    """æ¨ç†å¼•æ“"""

    def __init__(self, config: InternalizedOllamaConfig, memory_guardian: MemoryGuardian):
        self.config = config
        self.memory_guardian = memory_guardian
        self.executor = ThreadPoolExecutor(max_workers=config.inference_threads)

    def run_inference(self, model: Any, prompt: str, **kwargs) -> Dict[str, Any]:
        """è¿è¡Œæ¨ç†"""
        try:
            # æ£€æŸ¥å†…å­˜é¢„ç®—
            if not self.memory_guardian.allocate_memory('working', 200):  # 200MBæ¨ç†å†…å­˜
                return {'error': 'å†…å­˜ä¸è¶³'}

            # æ‰§è¡Œæ¨ç†
            if self.config.enable_streaming and kwargs.get('stream', False):
                result = self._run_streaming_inference(model, prompt, **kwargs)
            else:
                result = self._run_standard_inference(model, prompt, **kwargs)

            self.memory_guardian.deallocate_memory('working', 200)
            return result

        except Exception as e:
            self.memory_guardian.deallocate_memory('working', 200)
            return {'error': str(e)}

    def _run_standard_inference(self, model: Any, prompt: str, **kwargs) -> Dict[str, Any]:
        """æ ‡å‡†æ¨ç†"""
        # è¿™é‡Œå®ç°å…·ä½“çš„æ¨ç†é€»è¾‘
        # ç®€åŒ–çš„æ¨¡æ‹Ÿå®ç°
        time.sleep(0.1)  # æ¨¡æ‹Ÿæ¨ç†æ—¶é—´
        return {
            'response': f"Processed: {prompt[:50]}...",
            'model_type': type(model).__name__,
            'inference_time': 0.1,
            'tokens_generated': len(prompt.split()) * 2
        }

    def _run_streaming_inference(self, model: Any, prompt: str, **kwargs) -> Dict[str, Any]:
        """æµå¼æ¨ç†"""
        # æµå¼æ¨ç†å®ç°
        result = {'response': '', 'chunks': []}

        words = prompt.split()
        for i, word in enumerate(words):
            time.sleep(0.01)  # æ¨¡æ‹Ÿæµå¼å»¶è¿Ÿ
            chunk = f"{word} "
            result['response'] += chunk
            result['chunks'].append(chunk)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦å›è°ƒ
            if 'callback' in kwargs:
                kwargs['callback'](chunk)

        result.update({
            'model_type': type(model).__name__,
            'inference_time': len(words) * 0.01,
            'tokens_generated': len(words) * 2,
            'streaming': True
        })

        return result


class InternalizedOllamaSystem:
    """å†…åŒ–Ollamaç³»ç»Ÿ"""

    def __init__(self, config: InternalizedOllamaConfig):
        self.config = config

        # åˆå§‹åŒ–ç»„ä»¶
        self.memory_guardian = MemoryGuardian(MemorySafeConfig(
            max_memory_mb=config.max_memory_mb,
            model_memory_limit_mb=config.model_memory_limit_mb,
            working_memory_mb=config.working_memory_mb
        ))

        self.registry = ModelRegistry(config)
        self.downloader = ModelDownloader(config, self.registry)
        self.loader = ModelLoader(config, self.registry, self.memory_guardian)
        self.crystallizer = H2QModelCrystallizer(config, self.memory_guardian) if config.enable_crystallization else None
        self.inference_engine = InferenceEngine(config, self.memory_guardian)

        # ç³»ç»ŸçŠ¶æ€
        self.is_running = False
        self.loaded_models: Dict[str, Any] = {}

    def startup(self) -> bool:
        """å¯åŠ¨ç³»ç»Ÿ"""
        try:
            print("ğŸš€ å¯åŠ¨ H2Q-Evo å†…åŒ–Ollamaç³»ç»Ÿ")
            print("=" * 50)

            # å¯åŠ¨å†…å­˜å®ˆæŠ¤è€…
            if not self.memory_guardian.start_guardian():
                print("âŒ å†…å­˜å®ˆæŠ¤è€…å¯åŠ¨å¤±è´¥")
                return False

            # æ£€æŸ¥ç³»ç»Ÿèµ„æº
            if not self._check_system_resources():
                print("âŒ ç³»ç»Ÿèµ„æºæ£€æŸ¥å¤±è´¥")
                return False

            # åˆå§‹åŒ–ç»„ä»¶
            print("âœ… ç³»ç»Ÿå¯åŠ¨æˆåŠŸ")
            self.is_running = True
            return True

        except Exception as e:
            print(f"âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
            return False

    def shutdown(self):
        """å…³é—­ç³»ç»Ÿ"""
        print("ğŸ”„ å…³é—­å†…åŒ–Ollamaç³»ç»Ÿ...")
        self.is_running = False

        # æ¸…ç†æ¨¡å‹
        for model_name in list(self.loaded_models.keys()):
            self.unload_model(model_name)

        # åœæ­¢å†…å­˜å®ˆæŠ¤è€…
        self.memory_guardian.stop_guardian()

        print("âœ… ç³»ç»Ÿå…³é—­å®Œæˆ")

    def load_model(self, model_name: str, **kwargs) -> bool:
        """åŠ è½½æ¨¡å‹"""
        if not self.is_running:
            print("âŒ ç³»ç»Ÿæœªå¯åŠ¨")
            return False

        try:
            # æ£€æŸ¥å¹¶å‘é™åˆ¶
            if len(self.loaded_models) >= self.config.max_concurrent_models:
                print(f"å·²è¾¾åˆ°æœ€å¤§å¹¶å‘æ¨¡å‹æ•°: {self.config.max_concurrent_models}")
                return False

            # ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.config.auto_download and not self.registry.get_model_info(model_name):
                print(f"è‡ªåŠ¨ä¸‹è½½æ¨¡å‹: {model_name}")
                # è¿™é‡Œéœ€è¦å®ç°è‡ªåŠ¨ä¸‹è½½é€»è¾‘
                pass

            # åŠ è½½æ¨¡å‹
            model = self.loader.load_model(model_name, **kwargs)
            if not model:
                return False

            # ç»“æ™¶åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.crystallizer and self.config.enable_crystallization:
                crystallized = self.crystallizer.crystallize_model(model, model_name)
                if crystallized:
                    model = crystallized

            self.loaded_models[model_name] = model
            print(f"âœ… æ¨¡å‹ {model_name} åŠ è½½å®Œæˆ")
            return True

        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False

    def unload_model(self, model_name: str) -> bool:
        """å¸è½½æ¨¡å‹"""
        if model_name not in self.loaded_models:
            return False

        try:
            # æ¸…ç†æ¨¡å‹å¼•ç”¨
            del self.loaded_models[model_name]

            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()

            print(f"âœ… æ¨¡å‹ {model_name} å·²å¸è½½")
            return True

        except Exception as e:
            print(f"âŒ å¸è½½æ¨¡å‹å¤±è´¥: {e}")
            return False

    def run_inference(self, model_name: str, prompt: str, **kwargs) -> Dict[str, Any]:
        """è¿è¡Œæ¨ç†"""
        if not self.is_running:
            return {'error': 'ç³»ç»Ÿæœªå¯åŠ¨'}

        if model_name not in self.loaded_models:
            return {'error': f'æ¨¡å‹ {model_name} æœªåŠ è½½'}

        model = self.loaded_models[model_name]
        return self.inference_engine.run_inference(model, prompt, **kwargs)

    def list_models(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        return self.registry.list_available_models()

    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            'is_running': self.is_running,
            'loaded_models': list(self.loaded_models.keys()),
            'memory_usage': self.memory_guardian._get_memory_usage(),
            'config': {
                'max_memory_mb': self.config.max_memory_mb,
                'target_device': self.config.target_device,
                'enable_crystallization': self.config.enable_crystallization
            }
        }

    def _check_system_resources(self) -> bool:
        """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
        memory = psutil.virtual_memory()
        available_mb = memory.available / (1024 * 1024)

        if available_mb < self.config.working_memory_mb:
            print(f"å¯ç”¨å†…å­˜ä¸è¶³: {available_mb:.1f} MB < {self.config.working_memory_mb} MB")
            return False

        return True


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå†…åŒ–Ollamaç³»ç»Ÿ"""
    print("ğŸ§  H2Q-Evo å†…åŒ–Ollamaç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)

    # é…ç½®ç³»ç»Ÿ
    config = InternalizedOllamaConfig(
        max_memory_mb=6144,  # 6GBå†…å­˜é™åˆ¶
        model_memory_limit_mb=2048,  # 2GBæ¨¡å‹é™åˆ¶
        working_memory_mb=1024,  # 1GBå·¥ä½œå†…å­˜
        enable_crystallization=True,
        compression_ratio=8.0,
        target_device="cpu",  # è¾¹ç¼˜è®¾å¤‡ä½¿ç”¨CPU
        optimize_for_edge=True,
        enable_quantization=True
    )

    print("ğŸ“‹ ç³»ç»Ÿé…ç½®:")
    print(f"   æ€»å†…å­˜é™åˆ¶: {config.max_memory_mb} MB")
    print(f"   æ¨¡å‹å†…å­˜é™åˆ¶: {config.model_memory_limit_mb} MB")
    print(f"   å·¥ä½œå†…å­˜: {config.working_memory_mb} MB")
    print(f"   å‹ç¼©ç‡: {config.compression_ratio}x")
    print(f"   ç›®æ ‡è®¾å¤‡: {config.target_device}")
    print(f"   å¯ç”¨ç»“æ™¶åŒ–: {config.enable_crystallization}")
    print()

    # åˆ›å»ºå†…åŒ–Ollamaç³»ç»Ÿ
    ollama_system = InternalizedOllamaSystem(config)

    try:
        # å¯åŠ¨ç³»ç»Ÿ
        if not ollama_system.startup():
            print("âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥")
            return

        # æ¼”ç¤ºæ¨¡å‹ç®¡ç†
        print("ğŸ”„ æ¼”ç¤ºæ¨¡å‹ç®¡ç†...")

        # åˆ—å‡ºå¯ç”¨æ¨¡å‹
        available_models = ollama_system.list_models()
        print(f"å¯ç”¨æ¨¡å‹: {available_models}")

        # æ¨¡æ‹ŸåŠ è½½æ¨¡å‹
        test_model_name = "test_model"
        print(f"åŠ è½½æµ‹è¯•æ¨¡å‹: {test_model_name}")

        # ç”±äºæ²¡æœ‰çœŸå®çš„æ¨¡å‹æ–‡ä»¶ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„
        ollama_system.registry.register_model(test_model_name, {
            'name': test_model_name,
            'format': 'pkl',
            'size_mb': 100,
            'simulated': True
        })

        # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„æ¨¡å‹æ–‡ä»¶
        import pickle
        test_model_path = os.path.join(config.model_cache_dir, f"{test_model_name}.gguf")
        with open(test_model_path, 'wb') as f:
            pickle.dump({"type": "test", "data": "simulated model"}, f)

        # åŠ è½½æ¨¡å‹
        if ollama_system.load_model(test_model_name):
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

            # è¿è¡Œæ¨ç†
            print("ğŸ”„ è¿è¡Œæ¨ç†æµ‹è¯•...")
            test_prompts = [
                "Hello, how are you?",
                "Explain quantum computing",
                "Write a simple Python function"
            ]

            for i, prompt in enumerate(test_prompts, 1):
                print(f"æ¨ç† {i}: {prompt[:30]}...")
                result = ollama_system.run_inference(test_model_name, prompt)

                if 'error' in result:
                    print(f"   âŒ å¤±è´¥: {result['error']}")
                else:
                    print("   âœ… æˆåŠŸ")
                    print(f"     æ¨ç†æ—¶é—´: {result.get('inference_time', 0):.3f} ç§’")
                    print(f"     ç”Ÿæˆä»¤ç‰Œ: {result.get('tokens_generated', 0)}")

            # å¸è½½æ¨¡å‹
            ollama_system.unload_model(test_model_name)
            print("âœ… æ¨¡å‹å¸è½½å®Œæˆ")

        # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
        status = ollama_system.get_system_status()
        print("\nğŸ“Š æœ€ç»ˆç³»ç»ŸçŠ¶æ€:")
        print(f"   è¿è¡ŒçŠ¶æ€: {status['is_running']}")
        print(f"   å†…å­˜ä½¿ç”¨: {status['memory_usage']:.1f} MB")
        print(f"   åŠ è½½æ¨¡å‹: {status['loaded_models']}")

        print("\nğŸ¯ å†…åŒ–Ollamaç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
        print("âœ… æˆåŠŸå®ç°è‡ªåŒ…å«æ¨¡å‹è¿è¡Œæ—¶")
        print("âœ… å†…å­˜ä¼˜åŒ–å’Œèµ„æºæ§åˆ¶")
        print("âœ… H2Qç»“æ™¶åŒ–å‹ç¼©")
        print("âœ… è¾¹ç¼˜è®¾å¤‡å…¼å®¹æ€§")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¼”ç¤ºä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # ç¡®ä¿ç³»ç»Ÿæ­£ç¡®å…³é—­
        ollama_system.shutdown()


if __name__ == "__main__":
    main()