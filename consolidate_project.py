#!/usr/bin/env python3
"""
é¡¹ç›®ä»£ç åˆå¹¶å·¥å…· - å°†æ‰€æœ‰æºä»£ç åˆå¹¶åˆ°å•ä¸€Markdownæ–‡ä»¶
å°†æ’é™¤å¸¸è§çš„æ— ç”¨æ–‡ä»¶å’Œç›®å½•ï¼Œå¹¶æŒ‰ä»£ç å—è¿›è¡Œç»“æ„åŒ–æ ‡è®°
"""

import os
import json
from pathlib import Path
from datetime import datetime

# é…ç½®
PROJECT_ROOT = Path("/Users/imymm/H2Q-Evo")
OUTPUT_FILE = PROJECT_ROOT / "PROJECT_CODE_CONSOLIDATED.md"

# è¦å¿½ç•¥çš„æ–‡ä»¶å’Œç›®å½•
IGNORE_DIRS = {
    ".git",
    ".github",
    "__pycache__",
    ".pytest_cache",
    "node_modules",
    ".venv",
    "venv",
    ".env",
    "*.egg-info",
    ".coverage",
    "dist",
    "build",
    ".vscode",
    ".idea",
    ".DS_Store",
}

# è¦å¿½ç•¥çš„æ–‡ä»¶æ¨¡å¼
IGNORE_FILES = {
    ".pyc",
    ".pyo",
    ".pyd",
    ".so",
    ".dylib",
    ".dll",
    ".exe",
    ".lock",
    ".lockfile",
    ".package-lock.json",
    ".yarn.lock",
    ".pth",  # Python path files
    ".pt",  # PyTorch model files
    ".pth",  # PyTorch files
    ".h5",  # HDF5 files
    ".pb",  # Protocol buffer
    ".ckpt",  # Checkpoint files
    ".bin",  # Binary files
    ".so",  # Shared objects
    "evolution.log",  # Log files
    ".log",
}

# è¦åŒ…å«çš„ä»£ç æ–‡ä»¶æ‰©å±•å
CODE_EXTENSIONS = {
    ".py",
    ".js",
    ".ts",
    ".tsx",
    ".jsx",
    ".json",
    ".yaml",
    ".yml",
    ".toml",
    ".ini",
    ".cfg",
    ".conf",
    ".sh",
    ".bash",
    ".md",
    ".txt",
    ".dockerfile",
    ".sql",
    ".html",
    ".css",
    ".scss",
    ".sass",
}

def should_ignore(path: Path, is_dir: bool) -> bool:
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥å¿½ç•¥è¯¥è·¯å¾„"""
    # æ£€æŸ¥ç›®å½•å
    if is_dir:
        for ignore_pattern in IGNORE_DIRS:
            if ignore_pattern.startswith("*"):
                if path.name.endswith(ignore_pattern[1:]):
                    return True
            elif path.name == ignore_pattern:
                return True
    
    # æ£€æŸ¥æ–‡ä»¶åå’Œæ‰©å±•å
    if not is_dir:
        for ignore_pattern in IGNORE_FILES:
            if ignore_pattern.startswith("."):
                if path.suffix == ignore_pattern or path.name.endswith(ignore_pattern):
                    return True
            if path.name == ignore_pattern:
                return True
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå…è®¸çš„ä»£ç æ–‡ä»¶
        if path.suffix.lower() not in CODE_EXTENSIONS:
            if not (path.suffix == "" and path.name in {"Dockerfile", "Makefile", "LICENSE", "README"}):
                return True
    
    return False

def get_files_recursively(root: Path) -> list:
    """é€’å½’è·å–æ‰€æœ‰ä»£ç æ–‡ä»¶"""
    files = []
    
    try:
        for item in sorted(root.rglob("*")):
            if should_ignore(item, item.is_dir()):
                continue
            
            if item.is_file():
                files.append(item)
    except Exception as e:
        print(f"æ‰«æç›®å½•æ—¶å‡ºé”™ {root}: {e}")
    
    return sorted(files)

def read_file_safely(file_path: Path) -> str:
    """å®‰å…¨åœ°è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        # å°è¯•ä»¥UTF-8ç¼–ç è¯»å–
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    except UnicodeDecodeError:
        try:
            # å¦‚æœUTF-8å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
            with open(file_path, "r", encoding="latin-1") as f:
                return f.read()
        except Exception as e:
            return f"[æ— æ³•è¯»å–æ–‡ä»¶: {e}]"
    except Exception as e:
        return f"[è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}]"

def get_language_for_extension(file_path: Path) -> str:
    """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–ä»£ç å—è¯­è¨€æ ‡è®°"""
    ext = file_path.suffix.lower()
    
    language_map = {
        ".py": "python",
        ".js": "javascript",
        ".ts": "typescript",
        ".tsx": "typescript",
        ".jsx": "javascript",
        ".json": "json",
        ".yaml": "yaml",
        ".yml": "yaml",
        ".toml": "toml",
        ".ini": "ini",
        ".cfg": "ini",
        ".conf": "text",
        ".sh": "bash",
        ".bash": "bash",
        ".dockerfile": "dockerfile",
        ".sql": "sql",
        ".html": "html",
        ".css": "css",
        ".scss": "scss",
        ".sass": "sass",
    }
    
    return language_map.get(ext, "text")

def count_lines(content: str) -> int:
    """è®¡ç®—æ–‡ä»¶è¡Œæ•°"""
    return len(content.splitlines())

def calculate_size(content: str) -> str:
    """è®¡ç®—å†…å®¹å¤§å°"""
    size_bytes = len(content.encode("utf-8"))
    if size_bytes < 1024:
        return f"{size_bytes}B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.1f}KB"
    else:
        return f"{size_bytes / (1024 * 1024):.1f}MB"

def generate_markdown() -> None:
    """ç”Ÿæˆåˆå¹¶åçš„Markdownæ–‡ä»¶"""
    print(f"å¼€å§‹æ‰«æé¡¹ç›®: {PROJECT_ROOT}")
    
    files = get_files_recursively(PROJECT_ROOT)
    print(f"å‘ç° {len(files)} ä¸ªä»£ç æ–‡ä»¶")
    
    # æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„
    files_by_type = {}
    total_lines = 0
    total_size = 0
    
    content_parts = []
    
    # å¤´éƒ¨
    header = f"""# H2Q-Evo é¡¹ç›®ä»£ç æ•´ä½“æ±‡æ€»

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**é¡¹ç›®è·¯å¾„**: {PROJECT_ROOT}  

## ç›®å½•ç»“æ„æ¦‚è§ˆ

æ­¤æ–‡æ¡£åŒ…å«äº†é¡¹ç›®ä¸­æ‰€æœ‰æºä»£ç æ–‡ä»¶çš„å†…å®¹ï¼ŒæŒ‰ç…§é€»è¾‘åˆ†ç±»å’Œæ–‡ä»¶ç±»å‹ç»„ç»‡ã€‚

### ç»Ÿè®¡ä¿¡æ¯

- **æ€»æ–‡ä»¶æ•°**: {len(files)}
- **ç”Ÿæˆæ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d')}

---

## ğŸ“‘ æ–‡ä»¶æ¸…å•ä¸å¯¼èˆª

"""
    
    content_parts.append(header)
    
    # ç”Ÿæˆæ–‡ä»¶ç´¢å¼•
    file_index = []
    for idx, file_path in enumerate(files, 1):
        rel_path = file_path.relative_to(PROJECT_ROOT)
        try:
            content = read_file_safely(file_path)
            lines = count_lines(content)
            file_index.append({
                "idx": idx,
                "path": str(rel_path),
                "lines": lines,
                "ext": file_path.suffix,
            })
        except Exception as e:
            print(f"é”™è¯¯: {file_path}: {e}")
    
    # æŒ‰æ‰©å±•ååˆ†ç»„æ˜¾ç¤ºç´¢å¼•
    ext_groups = {}
    for item in file_index:
        ext = item["ext"] or "other"
        if ext not in ext_groups:
            ext_groups[ext] = []
        ext_groups[ext].append(item)
    
    for ext in sorted(ext_groups.keys()):
        items = ext_groups[ext]
        content_parts.append(f"\n### {ext} æ–‡ä»¶ ({len(items)} ä¸ª)\n")
        for item in sorted(items, key=lambda x: x["path"]):
            content_parts.append(f"- [{item['path']}](#{item['idx']}) - {item['lines']} è¡Œ\n")
    
    # æ–‡ä»¶å†…å®¹è¯¦æƒ…
    content_parts.append("\n\n---\n\n## ğŸ“„ è¯¦ç»†ä»£ç å†…å®¹\n\n")
    
    for idx, file_path in enumerate(files, 1):
        rel_path = file_path.relative_to(PROJECT_ROOT)
        
        try:
            content = read_file_safely(file_path)
            lines = count_lines(content)
            size = calculate_size(content)
            language = get_language_for_extension(file_path)
            
            total_lines += lines
            total_size += len(content.encode("utf-8"))
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            stat = file_path.stat()
            
            # æ„å»ºæ–‡ä»¶å¤´
            file_header = f"""### {idx}. {rel_path}

**å…ƒæ•°æ®**:
- **è·¯å¾„**: `{rel_path}`
- **å¤§å°**: {size}
- **è¡Œæ•°**: {lines}
- **ç±»å‹**: {language}
- **ä¿®æ”¹æ—¶é—´**: {datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')}

**ä»£ç å†…å®¹**:

```{language}
{content}
```

---

"""
            content_parts.append(file_header)
            
            # æ‰“å°è¿›åº¦
            if idx % 10 == 0:
                print(f"å·²å¤„ç† {idx}/{len(files)} ä¸ªæ–‡ä»¶...")
        
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶å‡ºé”™ {file_path}: {e}")
            error_content = f"""### {idx}. {rel_path}

**é”™è¯¯**: {e}

---

"""
            content_parts.append(error_content)
    
    # æ€»ç»“
    summary = f"""
---

## ğŸ“Š é¡¹ç›®æ€»ç»“ç»Ÿè®¡

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| æ€»æ–‡ä»¶æ•° | {len(files)} |
| æ€»ä»£ç è¡Œæ•° | {total_lines:,} |
| æ€»ä»£ç å¤§å° | {total_size / (1024*1024):.2f} MB |
| ç”Ÿæˆæ—¶é—´ | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} |

### ä»£ç ç±»å‹åˆ†å¸ƒ

"""
    
    ext_stats = {}
    for item in file_index:
        ext = item["ext"] or "other"
        if ext not in ext_stats:
            ext_stats[ext] = {"count": 0, "lines": 0}
        ext_stats[ext]["count"] += 1
        ext_stats[ext]["lines"] += item["lines"]
    
    summary += "| æ–‡ä»¶ç±»å‹ | æ•°é‡ | æ€»è¡Œæ•° |\n|---------|------|--------|\n"
    for ext in sorted(ext_stats.keys()):
        stats = ext_stats[ext]
        summary += f"| {ext or 'other'} | {stats['count']} | {stats['lines']:,} |\n"
    
    summary += f"""

---

**æ³¨æ„**: 
- æ­¤æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆï¼Œç”¨äºæ•´ä½“é€»è¾‘ä¸€è‡´æ€§åˆ†æ
- å·²æ’é™¤äºŒè¿›åˆ¶æ–‡ä»¶ã€ä¾èµ–ã€æ—¥å¿—ç­‰æ— å…³å†…å®¹
- å¯ç”¨äº AI å·¥å…·è¿›è¡Œå…¨å±€åˆ†æ

"""
    
    content_parts.append(summary)
    
    # å†™å…¥æ–‡ä»¶
    final_content = "".join(content_parts)
    
    print(f"\næ­£åœ¨å†™å…¥è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(final_content)
    
    output_size = len(final_content.encode("utf-8"))
    print(f"\nâœ… å®Œæˆ!")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print(f"   - è¾“å‡ºå¤§å°: {output_size / (1024*1024):.2f} MB")
    print(f"   - åŒ…å«æ–‡ä»¶æ•°: {len(files)}")
    print(f"   - æ€»ä»£ç è¡Œæ•°: {total_lines:,}")

if __name__ == "__main__":
    generate_markdown()
