#!/usr/bin/env python3
"""
H2Q-Evo æ¶æ„å¯¹æ¯”åˆ†æç³»ç»Ÿ
æ¯”è¾ƒæ ¸å¿ƒæœºèƒ½åŠ› vs ä¸€èˆ¬æ¶æ„ vs ç°æœ‰æ¨¡å‹çš„æ€§èƒ½å’Œå¼€é”€
"""

import torch
import torch.nn as nn
import time
import psutil
import os
import sys
from typing import Dict, Any, List, Optional
from pathlib import Path
import json
import requests

sys.path.append('/Users/imymm/H2Q-Evo')

from h2q_project.src.h2q.tokenizer_simple import default_tokenizer
from hierarchical_concept_encoder import HierarchicalConceptEncoder
from simple_hierarchical_encoder import SimpleHierarchicalEncoder
from real_code_completion_system import RealCodeCompletionSystem


class ArchitectureComparisonSystem:
    """æ¶æ„å¯¹æ¯”åˆ†æç³»ç»Ÿ"""

    def __init__(self):
        self.tokenizer = default_tokenizer
        self.results = {}

    def get_system_metrics(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)

        return {
            "memory_total_gb": memory.total / (1024**3),
            "memory_available_gb": memory.available / (1024**3),
            "memory_used_gb": memory.used / (1024**3),
            "memory_percentage": memory.percent,
            "cpu_percentage": cpu_percent,
            "gpu_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "gpu_memory_allocated": torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0,
            "gpu_memory_reserved": torch.cuda.memory_reserved() / (1024**3) if torch.cuda.is_available() else 0
        }

    def test_core_machine_architecture(self) -> Dict[str, Any]:
        """æµ‹è¯•æ ¸å¿ƒæœºæ¶æ„æ€§èƒ½"""
        print("ğŸ”¬ æµ‹è¯•æ ¸å¿ƒæœºæ¶æ„ (H2Q-Evo Hierarchical Concept Encoder)")
        print("-" * 60)

        start_time = time.time()
        start_metrics = self.get_system_metrics()

        try:
            # åˆå§‹åŒ–æ ¸å¿ƒæœºç³»ç»Ÿ
            encoder = HierarchicalConceptEncoder(max_depth=3, compression_ratio=46.0)

            # æµ‹è¯•ç¼–ç æ€§èƒ½
            test_texts = [
                "def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)",
                "class NeuralNetwork(nn.Module): def __init__(self): super().__init__()",
                "import torch; model = torch.nn.Linear(10, 1)"
            ]

            encoding_times = []
            for text in test_texts:
                encode_start = time.time()
                result = encoder.encode_hierarchical(text)
                encode_time = time.time() - encode_start
                encoding_times.append(encode_time)
                print(".4f")

            # è®¡ç®—å¹³å‡æ€§èƒ½
            avg_encoding_time = sum(encoding_times) / len(encoding_times)
            total_time = time.time() - start_time
            end_metrics = self.get_system_metrics()

            return {
                "architecture": "core_machine",
                "status": "success",
                "total_time": total_time,
                "avg_encoding_time": avg_encoding_time,
                "memory_delta_mb": (end_metrics["memory_used_gb"] - start_metrics["memory_used_gb"]) * 1024,
                "cpu_overhead": end_metrics["cpu_percentage"] - start_metrics["cpu_percentage"],
                "gpu_memory_delta_mb": (end_metrics["gpu_memory_allocated"] - start_metrics["gpu_memory_allocated"]) * 1024,
                "compression_ratio": encoder.compression_ratio,
                "max_depth": encoder.max_depth,
                "uses_quaternion_mapping": True,
                "uses_wordnet": True,
                "uses_fractal_structure": True
            }

        except Exception as e:
            return {
                "architecture": "core_machine",
                "status": "failed",
                "error": str(e),
                "total_time": time.time() - start_time
            }

    def test_general_architecture(self) -> Dict[str, Any]:
        """æµ‹è¯•ä¸€èˆ¬æ¶æ„æ€§èƒ½"""
        print("ğŸ”¬ æµ‹è¯•ä¸€èˆ¬æ¶æ„ (Standard Transformer)")
        print("-" * 60)

        start_time = time.time()
        start_metrics = self.get_system_metrics()

        try:
            # åˆå§‹åŒ–ä¸€èˆ¬æ¶æ„ç³»ç»Ÿ
            system = RealCodeCompletionSystem()

            # æµ‹è¯•ç”Ÿæˆæ€§èƒ½
            test_prompts = [
                "def calculate_fibonacci(n):",
                "class NeuralNetwork(nn.Module):",
                "import torch"
            ]

            generation_times = []
            for prompt in test_prompts:
                gen_start = time.time()
                result = system.generate_completion(prompt, max_length=20)
                gen_time = time.time() - gen_start
                generation_times.append(gen_time)
                print(".4f")

            # è®¡ç®—å¹³å‡æ€§èƒ½
            avg_generation_time = sum(generation_times) / len(generation_times)
            total_time = time.time() - start_time
            end_metrics = self.get_system_metrics()

            return {
                "architecture": "general_transformer",
                "status": "success",
                "total_time": total_time,
                "avg_generation_time": avg_generation_time,
                "memory_delta_mb": (end_metrics["memory_used_gb"] - start_metrics["memory_used_gb"]) * 1024,
                "cpu_overhead": end_metrics["cpu_percentage"] - start_metrics["cpu_percentage"],
                "gpu_memory_delta_mb": (end_metrics["gpu_memory_allocated"] - start_metrics["gpu_memory_allocated"]) * 1024,
                "model_size_mb": self._get_model_size(system.model),
                "uses_quaternion_mapping": False,
                "uses_wordnet": False,
                "uses_fractal_structure": False
            }

        except Exception as e:
            return {
                "architecture": "general_transformer",
                "status": "failed",
                "error": str(e),
                "total_time": time.time() - start_time
            }

    def test_deepseek_model(self) -> Dict[str, Any]:
        """æµ‹è¯•DeepSeekæ¨¡å‹æ€§èƒ½"""
        print("ğŸ”¬ æµ‹è¯•DeepSeekæ¨¡å‹")
        print("-" * 60)

        start_time = time.time()
        start_metrics = self.get_system_metrics()

        try:
            # æµ‹è¯•DeepSeek APIè¿æ¥
            api_results = self._test_deepseek_api()

            total_time = time.time() - start_time
            end_metrics = self.get_system_metrics()

            return {
                "architecture": "deepseek_api",
                "status": "success" if api_results["connected"] else "api_unavailable",
                "total_time": total_time,
                "api_response_time": api_results.get("response_time", 0),
                "memory_delta_mb": (end_metrics["memory_used_gb"] - start_metrics["memory_used_gb"]) * 1024,
                "cpu_overhead": end_metrics["cpu_percentage"] - start_metrics["cpu_percentage"],
                "model_hosted_remotely": True,
                "uses_quaternion_mapping": False,
                "uses_wordnet": False,
                "uses_fractal_structure": False,
                **api_results
            }

        except Exception as e:
            return {
                "architecture": "deepseek_api",
                "status": "failed",
                "error": str(e),
                "total_time": time.time() - start_time
            }

    def _test_deepseek_api(self) -> Dict[str, Any]:
        """æµ‹è¯•DeepSeek APIè¿æ¥"""
        try:
            # æµ‹è¯•åŸºæœ¬è¿æ¥
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = data.get('models', [])

                # æŸ¥æ‰¾DeepSeekç›¸å…³æ¨¡å‹
                deepseek_models = [m for m in models if 'deepseek' in m['name'].lower()]

                if deepseek_models:
                    # æµ‹è¯•æ¨ç†
                    test_payload = {
                        "model": deepseek_models[0]['name'],
                        "prompt": "def hello_world():",
                        "stream": False
                    }

                    infer_start = time.time()
                    response = requests.post("http://localhost:11434/api/generate",
                                           json=test_payload, timeout=30)
                    response_time = time.time() - infer_start

                    if response.status_code == 200:
                        result = response.json()
                        return {
                            "connected": True,
                            "response_time": response_time,
                            "model_name": deepseek_models[0]['name'],
                            "model_size_gb": deepseek_models[0]['size'] / (1024**3),
                            "generated_text": result.get('response', '')[:100]
                        }

            return {"connected": False, "reason": "No DeepSeek models found or API unavailable"}

        except Exception as e:
            return {"connected": False, "error": str(e)}

    def _get_model_size(self, model: nn.Module) -> float:
        """è®¡ç®—æ¨¡å‹å¤§å°ï¼ˆMBï¼‰"""
        param_size = 0
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        buffer_size = 0
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        return (param_size + buffer_size) / (1024**2)

    def run_comparison_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ¶æ„å¯¹æ¯”åˆ†æ"""
        print("ğŸš€ H2Q-Evo æ¶æ„å¯¹æ¯”åˆ†æ")
        print("=" * 80)

        # æµ‹è¯•å„ä¸ªæ¶æ„
        results = {}

        print("\n1. æµ‹è¯•æ ¸å¿ƒæœºæ¶æ„...")
        results["core_machine"] = self.test_core_machine_architecture()

        print("\n2. æµ‹è¯•ä¸€èˆ¬æ¶æ„...")
        results["general_transformer"] = self.test_general_architecture()

        print("\n3. æµ‹è¯•DeepSeekæ¨¡å‹...")
        results["deepseek"] = self.test_deepseek_model()

        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        analysis = self._generate_analysis_report(results)

        # ä¿å­˜ç»“æœ
        self._save_results(results, analysis)

        return {"results": results, "analysis": analysis}

    def _generate_analysis_report(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        analysis = {
            "summary": {},
            "performance_comparison": {},
            "resource_efficiency": {},
            "capability_analysis": {},
            "recommendations": []
        }

        # æˆåŠŸç‡åˆ†æ
        successful_architectures = [k for k, v in results.items() if v.get("status") == "success"]
        analysis["summary"]["successful_architectures"] = successful_architectures
        analysis["summary"]["success_rate"] = len(successful_architectures) / len(results)

        # æ€§èƒ½å¯¹æ¯”
        if "core_machine" in successful_architectures and "general_transformer" in successful_architectures:
            core_time = results["core_machine"]["avg_encoding_time"]
            general_time = results["general_transformer"]["avg_generation_time"]

            analysis["performance_comparison"] = {
                "core_machine_vs_general": {
                    "speed_ratio": core_time / general_time if general_time > 0 else float('inf'),
                    "core_machine_faster": core_time < general_time
                }
            }

        # èµ„æºæ•ˆç‡åˆ†æ
        for arch_name, arch_result in results.items():
            if arch_result.get("status") == "success":
                analysis["resource_efficiency"][arch_name] = {
                    "memory_efficiency": arch_result.get("memory_delta_mb", 0),
                    "cpu_efficiency": arch_result.get("cpu_overhead", 0),
                    "gpu_efficiency": arch_result.get("gpu_memory_delta_mb", 0)
                }

        # èƒ½åŠ›åˆ†æ
        analysis["capability_analysis"] = {
            "core_machine_uses_advanced_features": results.get("core_machine", {}).get("uses_quaternion_mapping", False),
            "general_architecture_simple": not results.get("general_transformer", {}).get("uses_quaternion_mapping", True),
            "deepseek_hosted_remotely": results.get("deepseek", {}).get("model_hosted_remotely", False)
        }

        # æ¨è
        if analysis["capability_analysis"]["core_machine_uses_advanced_features"]:
            analysis["recommendations"].append("æ ¸å¿ƒæœºæ¶æ„æä¾›äº†å…ˆè¿›çš„æ•°å­¦å»ºæ¨¡èƒ½åŠ›ï¼Œé€‚åˆéœ€è¦å¤æ‚æ¦‚å¿µç†è§£çš„ä»»åŠ¡")

        if analysis["summary"]["success_rate"] < 1.0:
            analysis["recommendations"].append("æŸäº›æ¶æ„å¯èƒ½éœ€è¦é¢å¤–çš„è®¾ç½®æˆ–ä¾èµ–")

        return analysis

    def _save_results(self, results: Dict[str, Any], analysis: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœ"""
        output = {
            "timestamp": time.time(),
            "system_info": self.get_system_metrics(),
            "results": results,
            "analysis": analysis
        }

        output_path = "/Users/imymm/H2Q-Evo/architecture_comparison_results.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    analyzer = ArchitectureComparisonSystem()
    results = analyzer.run_comparison_analysis()

    # æ‰“å°æ€»ç»“
    print("\nğŸ“Š åˆ†ææ€»ç»“")
    print("=" * 80)

    analysis = results["analysis"]

    print(f"æˆåŠŸæ¶æ„æ•°é‡: {len(analysis['summary']['successful_architectures'])}/{len(results['results'])}")
    print(f"æˆåŠŸç‡: {analysis['summary']['success_rate']:.1%}")

    print("\nğŸ† æ¶æ„èƒ½åŠ›å¯¹æ¯”:")
    for arch_name, arch_result in results["results"].items():
        status = arch_result.get("status", "unknown")
        print(f"  {arch_name}: {status}")
        if status == "success":
            if "avg_encoding_time" in arch_result:
                print(f"    ğŸ“ å¹³å‡ç¼–ç æ—¶é—´: {arch_result['avg_encoding_time']:.4f}s")
            elif "avg_generation_time" in arch_result:
                print(f"    ğŸ“ å¹³å‡ç”Ÿæˆæ—¶é—´: {arch_result['avg_generation_time']:.4f}s")
            if arch_result.get("uses_quaternion_mapping"):
                print("    âœ… ä½¿ç”¨å››å…ƒæ•°çƒé¢æ˜ å°„")
            if arch_result.get("uses_wordnet"):
                print("    âœ… ä½¿ç”¨WordNetè¯­ä¹‰ç½‘ç»œ")
            if arch_result.get("uses_fractal_structure"):
                print("    âœ… ä½¿ç”¨åˆ†å½¢ç»“æ„")

    print("\nğŸ’¡ å…³é”®å‘ç°:")
    for rec in analysis.get("recommendations", []):
        print(f"  â€¢ {rec}")

    print("\nâœ… æ¶æ„å¯¹æ¯”åˆ†æå®Œæˆ!")


if __name__ == "__main__":
    main()