#!/usr/bin/env python3
"""
H2Q-Evo æœ¬åœ°è®­ç»ƒä¸è¿›åŒ–ç³»ç»Ÿ
===================================

å®‰å…¨çš„æœ¬åœ°æ¨¡å‹è®­ç»ƒå’Œè¿›åŒ–
- å®Œå…¨ç¦»çº¿ï¼Œæ— è”ç½‘
- ä½¿ç”¨æœ¬åœ°æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
- è‡ªæˆ‘è¿›åŒ–ç®—æ³•
- æ€§èƒ½ç›‘æ§å’Œå®‰å…¨çº¦æŸ
"""

import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import time
import os
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
H2Q_PROJECT = PROJECT_ROOT / "h2q_project"
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(H2Q_PROJECT))

# å¯¼å…¥ç°æœ‰ç»„ä»¶
try:
    from local_long_text_generator import LocalLongTextGenerator
    from local_memory_index import OfflineMemoryIndex
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    learning_rate: float = 1e-4
    batch_size: int = 8
    max_epochs: int = 10
    sequence_length: int = 512
    save_interval: int = 5
    eval_interval: int = 2
    max_grad_norm: float = 1.0
    warmup_steps: int = 100


@dataclass
class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡"""
    epoch: int = 0
    step: int = 0
    loss: float = 0.0
    perplexity: float = 0.0
    learning_rate: float = 0.0
    grad_norm: float = 0.0
    tokens_processed: int = 0
    training_time: float = 0.0


class LocalTextDataset(Dataset):
    """æœ¬åœ°æ–‡æœ¬æ•°æ®é›†"""

    def __init__(self, data_dir: Path, sequence_length: int = 512):
        self.sequence_length = sequence_length
        self.data = []

        # åŠ è½½æœ¬åœ°æ•°æ®
        self._load_local_data(data_dir)

        # ç®€å•å­—ç¬¦çº§ç¼–ç 
        self.vocab_size = 256  # ASCIIå­—ç¬¦
        self.pad_token = 0

    def _load_local_data(self, data_dir: Path):
        """åŠ è½½æœ¬åœ°æ•°æ®"""
        print(f"ğŸ“š åŠ è½½æœ¬åœ°è®­ç»ƒæ•°æ®: {data_dir}")

        if not data_dir.exists():
            # åˆ›å»ºç¤ºä¾‹æ•°æ®
            data_dir.mkdir(parents=True, exist_ok=True)
            self._create_sample_data(data_dir)

        total_files = 0
        total_chars = 0

        # é€’å½’åŠ è½½æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
        for txt_file in data_dir.rglob("*.txt"):
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if len(content) > 100:  # åªä½¿ç”¨è¾ƒé•¿çš„æ–‡ä»¶
                        self.data.append(content)
                        total_chars += len(content)
                        total_files += 1
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {txt_file}: {e}")

        print(f"âœ“ åŠ è½½äº† {total_files} ä¸ªæ–‡ä»¶ï¼Œå…± {total_chars:,} ä¸ªå­—ç¬¦")

        if not self.data:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®")
            self._create_sample_data(data_dir)
            self.data = ["è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹è®­ç»ƒæ–‡æœ¬ï¼Œç”¨äºæµ‹è¯•æœ¬åœ°æ¨¡å‹è®­ç»ƒåŠŸèƒ½ã€‚"] * 10

    def _create_sample_data(self, data_dir: Path):
        """åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®"""
        sample_texts = [
            "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›é€ èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æœºå™¨ã€‚",
            "é‡å­è®¡ç®—åˆ©ç”¨é‡å­åŠ›å­¦çš„åŸç†ï¼Œå¦‚å åŠ å’Œçº ç¼ ï¼Œæ¥è¿›è¡Œè®¡ç®—ã€‚",
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œé€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹æ¥åšå‡ºé¢„æµ‹ã€‚",
            "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥è§£å†³å¤æ‚çš„æ¨¡å¼è¯†åˆ«é—®é¢˜ã€‚",
            "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯è®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€çš„æŠ€æœ¯ã€‚",
            "è®¡ç®—æœºè§†è§‰æ˜¯è®©æœºå™¨èƒ½å¤Ÿç†è§£å’Œè§£é‡Šè§†è§‰ä¿¡æ¯çš„æŠ€æœ¯ã€‚",
            "å¼ºåŒ–å­¦ä¹ é€šè¿‡è¯•é”™æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚",
            "ç¥ç»ç½‘ç»œæ˜¯å—ç”Ÿç‰©ç¥ç»ç³»ç»Ÿå¯å‘çš„è®¡ç®—æ¨¡å‹ã€‚",
            "å¤§æ•°æ®æ˜¯æŒ‡è§„æ¨¡å·¨å¤§ã€ç±»å‹å¤šæ ·ã€å¤„ç†é€Ÿåº¦å¿«çš„æµ·é‡æ•°æ®ã€‚",
            "ç®—æ³•æ˜¯è§£å†³ç‰¹å®šé—®é¢˜çš„ä¸€ç³»åˆ—æ˜ç¡®æŒ‡ä»¤ã€‚"
        ]

        train_dir = data_dir / "training"
        train_dir.mkdir(parents=True, exist_ok=True)

        for i, text in enumerate(sample_texts):
            with open(train_dir / f"sample_{i}.txt", 'w', encoding='utf-8') as f:
                # é‡å¤æ–‡æœ¬ä»¥å¢åŠ æ•°æ®é‡
                f.write((text + "\n") * 50)

    def __len__(self):
        return len(self.data) * 10  # æ¯ä¸ªæ–‡æœ¬ç”Ÿæˆ10ä¸ªåºåˆ—

    def __getitem__(self, idx):
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ–‡æœ¬
        text = self.data[idx % len(self.data)]

        # éšæœºé€‰æ‹©èµ·å§‹ä½ç½®
        start_pos = np.random.randint(0, max(1, len(text) - self.sequence_length - 1))
        chunk = text[start_pos:start_pos + self.sequence_length + 1]

        # å­—ç¬¦çº§ç¼–ç 
        tokens = [ord(c) % self.vocab_size for c in chunk]

        # å¡«å……æˆ–æˆªæ–­
        if len(tokens) < self.sequence_length + 1:
            tokens.extend([self.pad_token] * (self.sequence_length + 1 - len(tokens)))

        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)
        target_ids = torch.tensor(tokens[1:], dtype=torch.long)

        return input_ids, target_ids


class LocalModelTrainer:
    """æœ¬åœ°æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.dataset = None
        self.dataloader = None

        # è®­ç»ƒçŠ¶æ€
        self.metrics = TrainingMetrics()
        self.best_loss = float('inf')
        self.training_log = []

        print(f"ğŸ‹ï¸ æœ¬åœ°è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ | è®¾å¤‡: {self.device}")

    def setup_training(self, data_dir: Path):
        """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
        print("ğŸ”§ è®¾ç½®è®­ç»ƒç¯å¢ƒ...")

        # åˆ›å»ºæ•°æ®é›†
        self.dataset = LocalTextDataset(data_dir, self.config.sequence_length)
        self.dataloader = DataLoader(
            self.dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=0  # æœ¬åœ°è®­ç»ƒä½¿ç”¨å•çº¿ç¨‹
        )

        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨ç®€å•çš„Transformerï¼‰
        self._init_model()

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=0.01
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.config.max_epochs * len(self.dataloader)
        )

        print(f"âœ“ æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"âœ“ è®­ç»ƒæ•°æ®: {len(self.dataset)} ä¸ªåºåˆ—")
        print(f"âœ“ æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")

    def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        vocab_size = 256
        embed_dim = 256
        n_heads = 8
        n_layers = 6

        self.model = nn.Sequential(
            nn.Embedding(vocab_size, embed_dim),
            nn.TransformerEncoder(
                nn.TransformerEncoderLayer(
                    embed_dim, n_heads, batch_first=True, dropout=0.1
                ),
                num_layers=n_layers
            ),
            nn.Linear(embed_dim, vocab_size)
        ).to(self.device)

    def train_epoch(self) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0

        for batch_idx, (input_ids, target_ids) in enumerate(self.dataloader):
            input_ids = input_ids.to(self.device)
            target_ids = target_ids.to(self.device)

            # å‰å‘ä¼ æ’­
            outputs = self.model(input_ids)
            loss = nn.functional.cross_entropy(
                outputs.view(-1, outputs.size(-1)),
                target_ids.view(-1),
                ignore_index=0  # å¿½ç•¥å¡«å……token
            )

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)

            self.optimizer.step()
            self.scheduler.step()

            # æ›´æ–°æŒ‡æ ‡
            total_loss += loss.item()
            num_batches += 1
            self.metrics.step += 1
            self.metrics.tokens_processed += input_ids.numel()

            # å®šæœŸæŠ¥å‘Š
            if batch_idx % 10 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                perplexity = torch.exp(loss).item()
                print(f"  æ‰¹æ¬¡ {batch_idx:3d} | æŸå¤±: {loss.item():.4f} | å›°æƒ‘åº¦: {perplexity:.2f} | LR: {current_lr:.6f}")

        avg_loss = total_loss / num_batches
        return avg_loss

    def evaluate(self) -> Tuple[float, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for input_ids, target_ids in self.dataloader:
                input_ids = input_ids.to(self.device)
                target_ids = target_ids.to(self.device)

                outputs = self.model(input_ids)
                loss = nn.functional.cross_entropy(
                    outputs.view(-1, outputs.size(-1)),
                    target_ids.view(-1),
                    ignore_index=0
                )

                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / num_batches
        perplexity = torch.exp(torch.tensor(avg_loss)).item()

        return avg_loss, perplexity

    def save_checkpoint(self, epoch: int, loss: float):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = PROJECT_ROOT / "training_checkpoints"
        checkpoint_dir.mkdir(exist_ok=True)

        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'config': asdict(self.config),
            'metrics': asdict(self.metrics)
        }

        checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch}.pt"
        torch.save(checkpoint, checkpoint_path)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if loss < self.best_loss:
            self.best_loss = loss
            best_model_path = checkpoint_dir / "best_model.pt"
            torch.save(self.model.state_dict(), best_model_path)
            print(f"ğŸ† æœ€ä½³æ¨¡å‹å·²æ›´æ–°: {best_model_path}")

    def load_checkpoint(self, checkpoint_path: Path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not checkpoint_path.exists():
            print(f"âš ï¸ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
            return

        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        self.metrics.epoch = checkpoint.get('epoch', 0)
        self.best_loss = checkpoint.get('loss', float('inf'))

        print(f"ğŸ“‚ æ£€æŸ¥ç‚¹å·²åŠ è½½: {checkpoint_path}")

    def train(self, data_dir: Path, resume: bool = False):
        """å¼€å§‹è®­ç»ƒ"""
        print("\n" + "="*60)
        print("ğŸš€ H2Q-Evo æœ¬åœ°æ¨¡å‹è®­ç»ƒå¼€å§‹")
        print("="*60)
        print("ğŸ›¡ï¸ å®‰å…¨ä¿è¯ï¼šå®Œå…¨ç¦»çº¿ï¼Œæ— è”ç½‘")
        print("ğŸ“Š è®­ç»ƒé…ç½®ï¼š")
        print(f"  - å­¦ä¹ ç‡: {self.config.learning_rate}")
        print(f"  - æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        print(f"  - åºåˆ—é•¿åº¦: {self.config.sequence_length}")
        print(f"  - æœ€å¤§è½®æ•°: {self.config.max_epochs}")
        print("="*60 + "\n")

        # è®¾ç½®è®­ç»ƒç¯å¢ƒ
        self.setup_training(data_dir)

        # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
        if resume:
            checkpoint_dir = PROJECT_ROOT / "training_checkpoints"
            latest_checkpoint = max(checkpoint_dir.glob("checkpoint_epoch_*.pt"),
                                  key=lambda x: int(x.stem.split('_')[-1]), default=None)
            if latest_checkpoint:
                self.load_checkpoint(latest_checkpoint)

        start_time = time.time()

        for epoch in range(self.metrics.epoch, self.config.max_epochs):
            print(f"\nğŸ“… Epoch {epoch + 1}/{self.config.max_epochs}")
            print("-" * 40)

            # è®­ç»ƒ
            epoch_start = time.time()
            train_loss = self.train_epoch()
            epoch_time = time.time() - epoch_start

            # è¯„ä¼°
            if (epoch + 1) % self.config.eval_interval == 0:
                eval_loss, perplexity = self.evaluate()
                print(f"ğŸ“Š è¯„ä¼°æŸå¤±: {eval_loss:.4f} | å›°æƒ‘åº¦: {perplexity:.2f}")
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config.save_interval == 0:
                self.save_checkpoint(epoch + 1, train_loss)

            # æ›´æ–°æŒ‡æ ‡
            self.metrics.epoch = epoch + 1
            self.metrics.loss = train_loss
            self.metrics.perplexity = torch.exp(torch.tensor(train_loss)).item()
            self.metrics.training_time = time.time() - start_time

            # è®°å½•è®­ç»ƒæ—¥å¿—
            log_entry = {
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'eval_loss': eval_loss if (epoch + 1) % self.config.eval_interval == 0 else None,
                'perplexity': perplexity if (epoch + 1) % self.config.eval_interval == 0 else None,
                'epoch_time': epoch_time,
                'total_time': self.metrics.training_time
            }
            self.training_log.append(log_entry)

        total_time = time.time() - start_time
        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f} ç§’")
        print(f"ğŸ“‰ æœ€ç»ˆæŸå¤±: {self.metrics.loss:.4f}")
        print(f"ğŸ¯ æœ€ç»ˆå›°æƒ‘åº¦: {self.metrics.perplexity:.2f}")
        print(f"ğŸ“Š å¤„ç†tokenæ•°: {self.metrics.tokens_processed:,}")
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = PROJECT_ROOT / "h2q_project" / "h2q_trained_model.pt"
        torch.save(self.model.state_dict(), final_model_path)
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")

        # ä¿å­˜è®­ç»ƒæ—¥å¿—
        log_path = PROJECT_ROOT / "training_log.json"
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(self.training_log, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {log_path}")


class SelfEvolutionEngine:
    """è‡ªæˆ‘è¿›åŒ–å¼•æ“"""

    def __init__(self):
        self.trainer = LocalModelTrainer(TrainingConfig())
        self.memory_index = None
        self.generation_stats = {
            'total_evolutions': 0,
            'successful_evolutions': 0,
            'failed_evolutions': 0,
            'average_improvement': 0.0
        }

        print("ğŸ§¬ è‡ªæˆ‘è¿›åŒ–å¼•æ“å·²åˆå§‹åŒ–")

    def initialize_knowledge_base(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        data_dir = PROJECT_ROOT / "data" / "training_data"
        self.memory_index = OfflineMemoryIndex(data_dir)
        self.memory_index.build(max_files=100)

        print(f"ğŸ§  çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ | ç´¢å¼•æ–‡ä»¶: {len(self.memory_index.index)}")

    def evolutionary_training_cycle(self):
        """è¿›åŒ–è®­ç»ƒå‘¨æœŸ"""
        print("\nğŸ”„ å¼€å§‹è¿›åŒ–è®­ç»ƒå‘¨æœŸ...")

        # 1. è¯„ä¼°å½“å‰èƒ½åŠ›
        baseline_metrics = self._evaluate_current_capabilities()

        # 2. ç”Ÿæˆè®­ç»ƒæ•°æ®
        training_data = self._generate_training_data()

        # 3. æ‰§è¡Œè®­ç»ƒ
        self.trainer.train(training_data, resume=True)

        # 4. è¯„ä¼°æ”¹è¿›
        improved_metrics = self._evaluate_current_capabilities()
        improvement = self._calculate_improvement(baseline_metrics, improved_metrics)

        # 5. æ›´æ–°è¿›åŒ–ç»Ÿè®¡
        self.generation_stats['total_evolutions'] += 1
        if improvement > 0:
            self.generation_stats['successful_evolutions'] += 1
        else:
            self.generation_stats['failed_evolutions'] += 1

        self.generation_stats['average_improvement'] = (
            (self.generation_stats['average_improvement'] * (self.generation_stats['total_evolutions'] - 1)) +
            improvement
        ) / self.generation_stats['total_evolutions']

        print("\nğŸ“Š è¿›åŒ–ç»“æœ:")
        print(f"  æ”¹è¿›ç¨‹åº¦: {improvement:.4f}")
        print(f"  æˆåŠŸè¿›åŒ–: {'æ˜¯' if improvement > 0 else 'å¦'}")
        return improvement > 0

    def _evaluate_current_capabilities(self) -> Dict[str, float]:
        """è¯„ä¼°å½“å‰èƒ½åŠ›"""
        # ä½¿ç”¨ç®€å•çš„æŒ‡æ ‡è¯„ä¼°
        text_generator = LocalLongTextGenerator()

        # ç”Ÿæˆæµ‹è¯•æ–‡æœ¬
        test_prompts = [
            "è§£é‡Šäººå·¥æ™ºèƒ½çš„åŸºæœ¬åŸç†",
            "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—",
            "æœºå™¨å­¦ä¹ çš„å·¥ä½œåŸç†"
        ]

        total_length = 0
        total_diversity = 0

        for prompt in test_prompts:
            generated = text_generator.generate_long_text(prompt, max_tokens=200)
            total_length += len(generated)
            # ç®€å•å¤šæ ·æ€§åº¦é‡
            unique_chars = len(set(generated))
            total_diversity += unique_chars / len(generated) if generated else 0

        return {
            'avg_length': total_length / len(test_prompts),
            'avg_diversity': total_diversity / len(test_prompts)
        }

    def _generate_training_data(self) -> Path:
        """ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        data_dir = PROJECT_ROOT / "data" / "training_data" / "evolution"
        data_dir.mkdir(parents=True, exist_ok=True)

        # ä»ç°æœ‰çŸ¥è¯†åº“ç”Ÿæˆè®­ç»ƒæ•°æ®
        if self.memory_index and self.memory_index.index:
            # é€‰æ‹©é«˜è´¨é‡çš„æ–‡æ¡£è¿›è¡Œè®­ç»ƒ
            selected_docs = sorted(
                self.memory_index.index,
                key=lambda x: len(x['content']),
                reverse=True
            )[:10]  # é€‰æ‹©æœ€é•¿çš„10ä¸ªæ–‡æ¡£

            for i, doc in enumerate(selected_docs):
                # ç”Ÿæˆå˜ä½“æ•°æ®ç”¨äºè®­ç»ƒ
                variants = self._create_training_variants(doc['content'])
                for j, variant in enumerate(variants):
                    with open(data_dir / f"evolution_{i}_{j}.txt", 'w', encoding='utf-8') as f:
                        f.write(variant)

        return data_dir.parent

    def _create_training_variants(self, text: str) -> List[str]:
        """åˆ›å»ºè®­ç»ƒæ•°æ®å˜ä½“"""
        variants = [text]  # åŸå§‹æ–‡æœ¬

        # åˆ›å»ºä¸€äº›ç®€å•çš„å˜ä½“
        words = text.split()
        if len(words) > 10:
            # é‡æ–°æ’åˆ—å¥å­
            mid = len(words) // 2
            variant1 = ' '.join(words[mid:] + words[:mid])
            variants.append(variant1)

            # æˆªå–å­ä¸²
            variant2 = ' '.join(words[:len(words)//2])
            variants.append(variant2)

        return variants

    def _calculate_improvement(self, baseline: Dict[str, float], current: Dict[str, float]) -> float:
        """è®¡ç®—æ”¹è¿›ç¨‹åº¦"""
        improvement = 0.0
        for key in baseline:
            if key in current:
                improvement += (current[key] - baseline[key]) / max(baseline[key], 1e-6)
        return improvement / len(baseline) if baseline else 0.0

    def run_evolution_cycles(self, num_cycles: int = 3):
        """è¿è¡Œå¤šä¸ªè¿›åŒ–å‘¨æœŸ"""
        print("\n" + "="*60)
        print("ğŸ§¬ H2Q-Evo è‡ªæˆ‘è¿›åŒ–ä¹‹æ—…å¼€å§‹")
        print("="*60)
        print(f"ğŸ¯ ç›®æ ‡ï¼šè¿è¡Œ {num_cycles} ä¸ªè¿›åŒ–å‘¨æœŸ")
        print("ğŸ›¡ï¸ å®‰å…¨ï¼šå®Œå…¨æœ¬åœ°ï¼Œæ— è”ç½‘")
        print("="*60 + "\n")

        self.initialize_knowledge_base()

        successful_cycles = 0

        for cycle in range(num_cycles):
            print(f"\nğŸ”„ è¿›åŒ–å‘¨æœŸ {cycle + 1}/{num_cycles}")
            print("-" * 40)

            try:
                success = self.evolutionary_training_cycle()
                if success:
                    successful_cycles += 1
                    print(f"âœ… å‘¨æœŸ {cycle + 1} è¿›åŒ–æˆåŠŸ")
                else:
                    print(f"âš ï¸ å‘¨æœŸ {cycle + 1} è¿›åŒ–æœªè§æ˜¾è‘—æ”¹è¿›")
            except Exception as e:
                print(f"âŒ å‘¨æœŸ {cycle + 1} è¿›åŒ–å¤±è´¥: {e}")

        print("\nğŸŠ è¿›åŒ–å‘¨æœŸå®Œæˆï¼")
        print(f"ğŸ“ˆ æˆåŠŸå‘¨æœŸ: {successful_cycles}/{num_cycles}")
        print(f"ğŸ“Š å¹³å‡æ”¹è¿›: {self.generation_stats['average_improvement']:.2f}")
        print(f"ğŸ”¬ æ€»è¿›åŒ–æ¬¡æ•°: {self.generation_stats['total_evolutions']}")
        print(f"âœ… æˆåŠŸè¿›åŒ–: {self.generation_stats['successful_evolutions']}")
        print(f"âŒ å¤±è´¥è¿›åŒ–: {self.generation_stats['failed_evolutions']}")
        # ä¿å­˜è¿›åŒ–ç»Ÿè®¡
        stats_path = PROJECT_ROOT / "evolution_stats.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(self.generation_stats, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ è¿›åŒ–ç»Ÿè®¡å·²ä¿å­˜: {stats_path}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="H2Q-Evo æœ¬åœ°è®­ç»ƒä¸è¿›åŒ–ç³»ç»Ÿ")
    parser.add_argument("--mode", choices=["train", "evolve"], default="train",
                       help="è¿è¡Œæ¨¡å¼ï¼štrain(è®­ç»ƒ) æˆ– evolve(è¿›åŒ–)")
    parser.add_argument("--data_dir", type=str,
                       help="è®­ç»ƒæ•°æ®ç›®å½•ï¼ˆé»˜è®¤ä¸ºè‡ªåŠ¨åˆ›å»ºï¼‰")
    parser.add_argument("--epochs", type=int, default=5,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--cycles", type=int, default=3,
                       help="è¿›åŒ–å‘¨æœŸæ•°")

    args = parser.parse_args()

    if args.mode == "train":
        # åŸºç¡€è®­ç»ƒæ¨¡å¼
        config = TrainingConfig(max_epochs=args.epochs)
        trainer = LocalModelTrainer(config)

        data_dir = Path(args.data_dir) if args.data_dir else PROJECT_ROOT / "data" / "training_data"
        trainer.train(data_dir)

    elif args.mode == "evolve":
        # è‡ªæˆ‘è¿›åŒ–æ¨¡å¼
        evolution_engine = SelfEvolutionEngine()
        evolution_engine.run_evolution_cycles(args.cycles)


if __name__ == "__main__":
    main()