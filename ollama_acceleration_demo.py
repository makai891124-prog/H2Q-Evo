#!/usr/bin/env python3
"""
H2Q-Evo OllamaåŠ é€Ÿæ¼”ç¤ºè„šæœ¬
"""

import sys
import time
import psutil
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/Users/imymm/H2Q-Evo')

from h2q_ollama_accelerator import get_h2q_accelerator


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ H2Q-Evo OllamaåŠ é€Ÿæ¼”ç¤º")
    print("=" * 50)

    try:
        # åˆå§‹åŒ–åŠ é€Ÿå™¨
        accelerator = get_h2q_accelerator(max_memory_gb=6.0)

        # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
        show_system_status()

        # åŠ é€Ÿå¯ç”¨æ¨¡å‹
        test_models = ["deepseek-coder:33b"]  # å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹

        for model_name in test_models:
            if accelerator._check_ollama_model(model_name):
                print(f"\nâš¡ åŠ é€Ÿæ¨¡å‹: {model_name}")

                # åº”ç”¨H2QåŠ é€Ÿ
                result = accelerator.accelerate_ollama_model(model_name)

                if result["success"]:
                    print("âœ… åŠ é€ŸæˆåŠŸ!")
                    print(f"   åŠ é€Ÿæ¨¡å‹: {result['accelerated_model']}")
                    print(f"   å‹ç¼©ç‡: {result['compression_ratio']:.1f}x")
                    print(f"   å†…å­˜èŠ‚çœ: {result['memory_reduction_mb']:.0f}MB")
                    print(f"   ååé‡æå‡: {result['throughput_improvement']:.1f}x")

                    # æµ‹è¯•æ¨ç†
                    test_inference(result['accelerated_model'])
                else:
                    print(f"âŒ åŠ é€Ÿå¤±è´¥: {result.get('error', 'Unknown error')}")
            else:
                print(f"âš ï¸  æ¨¡å‹ä¸å­˜åœ¨: {model_name}")

        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        show_final_stats(accelerator)

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def show_system_status():
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    print("\nğŸ“Š ç³»ç»ŸçŠ¶æ€:")
    print(f"   CPUæ ¸å¿ƒæ•°: {psutil.cpu_count()}")
    print(f"   æ€»å†…å­˜: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    print(f"   å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / (1024**3):.1f} GB")


def test_inference(model_name: str):
    """æµ‹è¯•æ¨ç†"""
    print(f"\nğŸ§ª æµ‹è¯•æ¨ç†: {model_name}")
    try:
        import subprocess

        test_prompt = "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
        print(f"   æç¤º: {test_prompt}")

        start_time = time.time()
        result = subprocess.run(
            ["ollama", "run", model_name, test_prompt],
            capture_output=True,
            text=True,
            input=test_prompt,
            timeout=30
        )
        end_time = time.time()

        if result.returncode == 0:
            response = result.stdout.strip()
            latency = end_time - start_time
            print(f"   âœ… æ¨ç†æˆåŠŸ (è€—æ—¶: {latency:.2f}s)")
            print(f"   å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        else:
            print(f"   âŒ æ¨ç†å¤±è´¥: {result.stderr}")

    except Exception as e:
        print(f"   âŒ æ¨ç†é”™è¯¯: {e}")


def show_final_stats(accelerator):
    """æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡"""
    print("\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")

    stats = accelerator.get_performance_stats()

    print(f"   æ´»è·ƒåŠ é€Ÿæ¨¡å‹: {stats['active_models']}")
    print(f"   å½“å‰å†…å­˜ä½¿ç”¨: {stats['memory_usage_mb']:.1f} MB")
    print(f"   æ€»åŠ é€Ÿæ¨¡å‹æ•°: {stats['total_accelerated_models']}")

    perf = stats.get('performance_metrics', {})
    if perf:
        print(f"   æ€»æ¨ç†æ¬¡æ•°: {perf.get('total_inferences', 0)}")
        print(f"   å¹³å‡å»¶è¿Ÿ: {perf.get('average_latency_seconds', 0):.2f}s")
        print(f"   å¹³å‡ååé‡: {perf.get('average_throughput_tokens_per_second', 0):.1f} tokens/s")


if __name__ == "__main__":
    main()